{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "import operator\n",
    "import pdb\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reusing old folder to fetch users and their corresponding month year with highest data\n",
    "\n",
    "usern_mnth_df = pd.DataFrame()\n",
    "usern_mnth_df['user'] = \"\"\n",
    "usern_mnth_df['year'] = \"\"\n",
    "usern_mnth_df['month'] = \"\"\n",
    "\n",
    "file_list = glob.glob(\"C:/Users/12sha/Documents/thesislocation/Data/User stay points(month-maxdata, 50m 30min)/*.png\")\n",
    "for i in range(0, len(file_list)):\n",
    "    file_name = file_list[i]\n",
    "    user = file_name[88:91]\n",
    "    year = file_name[102:106]\n",
    "    month = file_name[106:108]\n",
    "    month = int(month)\n",
    "    usern_mnth_df.loc[i, 'user'] = user\n",
    "    usern_mnth_df.loc[i, 'year'] = year\n",
    "    usern_mnth_df.loc[i, 'month'] = \"{0:0=2d}\".format(month)\n",
    "usern_mnth_df = usern_mnth_df.drop_duplicates()\n",
    "usern_mnth_df = usern_mnth_df.reset_index(drop=True)\n",
    "\n",
    "#usern_mnth_df.to_csv(\"C:/Users/12sha/Documents/thesislocation/Data/user_highestdata.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e525ebbe3fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[1;31m#---------------------------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1055\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         \u001b[0mmonth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e525ebbe3fe6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcurr_hr_staypts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m                      \u001b[1;31m#B.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                     \u001b[0mform_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m                     \u001b[1;31m#read_trained_model()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m                     \u001b[1;31m#if not trained_model_df.empty:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e525ebbe3fe6>\u001b[0m in \u001b[0;36mform_states\u001b[1;34m()\u001b[0m\n\u001b[0;32m    424\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0madd_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Yes\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                     \u001b[0mstaypts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstaypts_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'StateId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mcurr_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'StateId'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchk_cluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                     \u001b[0mstaypts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstaypts_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'StateId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mchk_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'StateMeanLat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_lat_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m                     \u001b[0mstaypts_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstaypts_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'StateId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mchk_cluster\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'StateMeanLon'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_lon_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             self.obj._data = self.obj._data.setitem(indexer=indexer,\n\u001b[1;32m--> 651\u001b[1;33m                                                     value=value)\n\u001b[0m\u001b[0;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36msetitem\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   3691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3692\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3693\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'setitem'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3695\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m   3579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mgr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3581\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36msetitem\u001b[1;34m(self, indexer, value, mgr)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;31m# coerce and try to infer the dtypes of the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_coerce_and_cast_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_try_coerce_and_cast_result\u001b[1;34m(self, result, dtype)\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_coerce_and_cast_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_coerce_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_cast_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_try_cast_result\u001b[1;34m(self, result, dtype)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[1;31m# may need to change the dtype here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_downcast_to_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_try_coerce_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mmaybe_downcast_to_dtype\u001b[1;34m(result, dtype)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0minferred_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ensure_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minferred_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bool'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#online process\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_usr_file():\n",
    "    global usr_trejec_df\n",
    "    \n",
    "    #Load file names for user\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    usr_trejec_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    usr_trejec_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Date\"].map(str) + \" \" + usr_trejec_df[\"Time\"]\n",
    "    \n",
    "    usr_trejec_df.Timestamp = pd.to_datetime(usr_trejec_df.Timestamp)\n",
    "    \n",
    "    usr_trejec_df.index = usr_trejec_df['Timestamp']\n",
    "    usr_trejec_df = usr_trejec_df.resample('1T').mean()\n",
    "    usr_trejec_df = usr_trejec_df.dropna()\n",
    "    \n",
    "     #add columns to user trajectory dataframe\n",
    "    #1. add timestamp as column\n",
    "    usr_trejec_df['Timestamp'] = pd.to_datetime(usr_trejec_df.index)\n",
    "    #restore date and time column\n",
    "    usr_trejec_df['Date'] = usr_trejec_df.Timestamp.dt.date\n",
    "    usr_trejec_df['Time'] = usr_trejec_df.Timestamp.dt.time\n",
    "    usr_trejec_df['Hour'] = usr_trejec_df.Timestamp.dt.hour\n",
    "    \n",
    "    #sort values based on timestamp\n",
    "    usr_trejec_df = usr_trejec_df.sort_values(['Timestamp'])\n",
    "    #reset index\n",
    "    usr_trejec_df = usr_trejec_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    usr_trejec_df['Weekday'] = usr_trejec_df['Timestamp'].dt.weekday.map(str) + usr_trejec_df['Timestamp'].dt.weekday_name\n",
    "\n",
    "    usr_trejec_df['StayPoint'] = -1 # 1 if it is a staypoint, else 0\n",
    "    usr_trejec_df['StayptId'] = -1\n",
    "    usr_trejec_df['StayMeanLat'] = -1.0\n",
    "    usr_trejec_df['StayMeanLon'] = -1.0\n",
    "    usr_trejec_df['State'] = -1     # 1 if it is a state, else 0\n",
    "    usr_trejec_df['StateId'] = -1\n",
    "    usr_trejec_df['StateMeanLat'] = -1.0\n",
    "    usr_trejec_df['StateMeanLon'] = -1.0\n",
    "    \n",
    "    #remove columns not used/required\n",
    "    usr_trejec_df = usr_trejec_df.drop(['0', 'Altitude', 'NumDays'], axis = 1)\n",
    "\n",
    "#-------------------------------\n",
    "def prepare_dfs():\n",
    "    global cluster_hourly_df \n",
    "\n",
    "    #create cluster_hourly_df columns\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['StateId'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "        \n",
    "#------------------------------------------------------------------------------------\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def cluster(newlat, newlon, row, count, orig_lat, orig_lon):\n",
    "    global curr_hr_df\n",
    "    \n",
    "    currcluster = curr_hr_df['StayptId'][row-1]\n",
    "    curr_hr_df['StayptId'][row] = -1\n",
    "    curr_hr_df['StayMeanLat'][row] = -1.0\n",
    "    curr_hr_df['StayMeanLon'][row] = -1.0\n",
    "    curr_hr_df['StayPoint'][row] = -1\n",
    "    clulat = curr_hr_df['StayMeanLat'][row-1]\n",
    "    clulon = curr_hr_df['StayMeanLon'][row-1]\n",
    "    \n",
    "    #if the new point and the old point time difference is greater than tracking threshold\n",
    "    # then add both the points as staypoints and leave\n",
    "    prevPointTime = curr_hr_df['Timestamp'][row-1]\n",
    "    currPointTime = curr_hr_df['Timestamp'][row]\n",
    "    timm_diff = (currPointTime - prevPointTime).seconds /60\n",
    "    dist_diff = meters(clulat, clulon, newlat, newlon)   \n",
    "    \n",
    "    if (timm_diff >= track_t_thrhld and dist_diff > staypts_d_thrhld ):\n",
    "        curr_hr_df.loc[row-1, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row-1, 'StayMeanLat'] = curr_hr_df.loc[row-1, 'Latitude']\n",
    "        curr_hr_df.loc[row-1, 'StayMeanLon'] = curr_hr_df.loc[row-1, 'Longitude']\n",
    "        curr_hr_df.loc[row, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row, 'StayMeanLat'] = curr_hr_df.loc[row, 'Latitude']\n",
    "        curr_hr_df.loc[row, 'StayMeanLon'] = curr_hr_df.loc[row, 'Longitude']\n",
    "        curr_hr_df.loc[row, 'StayptId'] = currcluster + 1\n",
    "    else:    \n",
    "        #if the new point and old point's distance is less than threshold, then add it to current cluster\n",
    "        if meters(clulat, clulon, newlat, newlon)<= staypts_d_thrhld:\n",
    "            curr_hr_df['StayptId'][row] = currcluster\n",
    "            #calculate new mean lat and lon for the cluster\n",
    "            array_lat = curr_hr_df['Latitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "            array_lon = curr_hr_df['Longitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "\n",
    "            #cal new means\n",
    "            new_lat_mean = np.mean(array_lat)\n",
    "            new_lon_mean = np.mean(array_lon)\n",
    "\n",
    "            curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayMeanLat'] = new_lat_mean\n",
    "            curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayMeanLon'] = new_lon_mean\n",
    "\n",
    "    #         curr_hr_df['StayMeanLat'] = curr_hr_df.groupby('StayptId')['Latitude'].transform(np.mean)\n",
    "    #         curr_hr_df['StayMeanLon'] = curr_hr_df.groupby('StayptId')['Longitude'].transform(np.mean)\n",
    "            count = count + 1\n",
    "\n",
    "        #if the new point and old point's distance is greater than threshold, it means the point moved away\n",
    "        #if the previous cluster has more than two points, check the duration of the previous cluster\n",
    "        #   if the duration of the previos cluster is greater than threshold, assign it as a staypoint\n",
    "\n",
    "        #if the row read is the last row for this hour\n",
    "        if (row == len(curr_hr_df)-1):\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row-count+1]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayPoint'] = 1      \n",
    "                #incase the cluster is not a staypoint and the first point is already a staypoint\n",
    "                #then retain the latitued and longitudes\n",
    "                else:\n",
    "                    if (row-count) == 0 and curr_hr_df['StayPoint'][row-count] == 1:\n",
    "                        curr_hr_df['StayMeanLat'][row-count] = orig_lat\n",
    "                        curr_hr_df['StayMeanLon'][row-count] = orig_lon\n",
    "\n",
    "        #if the new point is moving away from the cluster\n",
    "        if meters(clulat, clulon, newlat, newlon)> staypts_d_thrhld:\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row-count]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row-1]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayPoint'] = 1\n",
    "                #incase the cluster is not a staypoint and the first point is already a staypoint\n",
    "                #then retain the latitued and longitudes\n",
    "                else:\n",
    "                    if (row-count) == 0 and curr_hr_df['StayPoint'][row-count] == 1:\n",
    "                        curr_hr_df['StayMeanLat'][row-count] = orig_lat\n",
    "                        curr_hr_df['StayMeanLon'][row-count] = orig_lon\n",
    "            \n",
    "            count = 1\n",
    "            curr_hr_df['StayMeanLat'][row] = curr_hr_df['Latitude'][row]\n",
    "            curr_hr_df['StayMeanLon'][row] = curr_hr_df['Longitude'][row]\n",
    "            curr_hr_df['StayptId'][row] = currcluster + 1\n",
    "\n",
    "    return count\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_trained_model():\n",
    "    global trained_model_df\n",
    "    \n",
    "    if os.path.isfile(dest_file_final_markov_chain):\n",
    "        trained_model_df = pd.read_csv(dest_file_final_markov_chain, header = 0)\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "def create_last_hr_staypts():\n",
    "    global curr_hr_df          #holds current hour points\n",
    "    global staypts_df          #holds all staypoints\n",
    "    global curr_hr_staypts_df  #holds current hour staypoints only\n",
    "    global prev_hour_last_point #stores last hour last point\n",
    "    \n",
    "    #clear current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_staypts_df.iloc[0:0]\n",
    "    \n",
    "    #reset index of current hour points\n",
    "    curr_hr_df = curr_hr_df.reset_index(drop=True)\n",
    "    \n",
    "    #feching the last stayptid \n",
    "    if not staypts_df.empty:\n",
    "        stayid = staypts_df['StayptId'].max() + 1 #assign next possible staypt id\n",
    "    else:\n",
    "        stayid = 1 #if this is the start, start from 1 as staypointID\n",
    "    \n",
    "    adding_first_as_staypt = 'N'\n",
    "    adding_last_as_staypt = 'N'\n",
    "    orig_lat = 0\n",
    "    orig_lon = 0\n",
    "    if not prev_hour_last_point.empty:\n",
    "        #check the time difference of last hour last point and this hour first point is greater than track t threshold\n",
    "        #if yes than add both as staypoints\n",
    "        if (int(time.mktime(curr_hr_df.loc[0, 'Timestamp'].timetuple()) - \n",
    "                   time.mktime(prev_hour_last_point.loc[0, 'Timestamp'].timetuple()))/60 > track_t_thrhld):\n",
    "            if prev_hour_last_point.loc[0, 'StayPoint'] != 1:\n",
    "                adding_last_as_staypt = 'Y'\n",
    "                prev_hour_last_point.loc[0, 'StayptId'] = stayid\n",
    "                stayid = stayid + 1\n",
    "                prev_hour_last_point.loc[0, 'StayPoint'] = 1\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLat'] = prev_hour_last_point.loc[0, 'Latitude']\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLon'] = prev_hour_last_point.loc[0, 'Longitude']\n",
    "                staypts_df = staypts_df.append(prev_hour_last_point)\n",
    "\n",
    "            curr_hr_df.loc[0, 'StayptId'] = stayid\n",
    "            stayid = stayid + 1\n",
    "            curr_hr_df.loc[0, 'StayPoint'] = 1\n",
    "            orig_lat = curr_hr_df.loc[0, 'Latitude']\n",
    "            orig_lon = curr_hr_df.loc[0, 'Longitude']\n",
    "            adding_first_as_staypt = 'Y'\n",
    "        \n",
    "            \n",
    "    #Read the file in an online manner as the points come and assign the points to clusters\n",
    "    row =1\n",
    "    count = 1\n",
    "    \n",
    "    if adding_first_as_staypt == 'N': \n",
    "        curr_hr_df['StayptId'][row-1] = stayid\n",
    "        curr_hr_df['StayPoint'][row-1] = -1\n",
    "        \n",
    "    curr_hr_df['StayMeanLat'][row-1] = curr_hr_df['Latitude'][0]\n",
    "    curr_hr_df['StayMeanLon'][row-1] = curr_hr_df['Longitude'][0]\n",
    "    \n",
    "    \n",
    "    while row < len(curr_hr_df):\n",
    "        count = cluster(curr_hr_df['Latitude'][row], curr_hr_df['Longitude'][row], row, count, orig_lat, orig_lon)\n",
    "        row= row + 1\n",
    "    \n",
    "    #copy the staypoints to the current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_df.loc[curr_hr_df['StayPoint'] == 1]\n",
    "    #copy the stay points into another dataframe\n",
    "    staypts_df = staypts_df.append(curr_hr_df.loc[curr_hr_df['StayPoint'] == 1])\n",
    "    #reset staypoints index\n",
    "    curr_hr_staypts_df.index = curr_hr_staypts_df['Timestamp']\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "\n",
    "    #store the last hour last point\n",
    "    prev_hour_last_point = prev_hour_last_point.iloc[0:0]\n",
    "    prev_hour_last_point = curr_hr_df.iloc[[len(curr_hr_df)-1]]\n",
    "    prev_hour_last_point = prev_hour_last_point.reset_index(drop=True)\n",
    "    \n",
    "    #clear current hour dataframe content\n",
    "    curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def add_start_end_times():\n",
    "    global staypts_df\n",
    "    \n",
    "    if staypts_df.empty or len(staypts_df) == 1:\n",
    "        return\n",
    "    \n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "    \n",
    "     #state = -2 indicates it is already processed\n",
    "    idx = staypts_df.index[staypts_df['State'] == -1]\n",
    "    if idx.empty:\n",
    "         return\n",
    "\n",
    "    if min(idx) == 0:\n",
    "        start = min(idx)\n",
    "    else:\n",
    "        start = min(idx) - 1\n",
    "    \n",
    "    prev_id = staypts_df.loc[start, 'StayptId']\n",
    "    tobeadded_staypts = pd.DataFrame(columns=['Latitude', 'Longitude', 'Timestamp', 'Date', 'Time',\n",
    "                                              'Hour', 'Weekday', 'StayPoint', 'StayptId', 'StayMeanLat',\n",
    "                                              'StayMeanLon', 'State', 'StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    j = 0\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i in range(start, len(staypts_df)):\n",
    "        #update state as -2 indicating processed\n",
    "        staypts_df.loc[i, 'State'] = -2 \n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        if staypts_df.loc[i, 'StayptId'] != prev_id:\n",
    "            prev_id = staypts_df.loc[i, 'StayptId']\n",
    "            strt_indx = i\n",
    "            end_indx = i-1\n",
    "\n",
    "            #calclulate time to be added\n",
    "            end1_trj_time = staypts_df.loc[end_indx, 'Timestamp']\n",
    "            end1_trj_lat = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "            end1_trj_lon = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "            str2_trj_time = staypts_df.loc[strt_indx, 'Timestamp']\n",
    "            str2_trj_lat = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "            str2_trj_lon = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "\n",
    "            dist_btw = meters(end1_trj_lat, end1_trj_lon, str2_trj_lat, str2_trj_lon)\n",
    "            time_btw = (str2_trj_time - end1_trj_time).seconds / 60\n",
    "\n",
    "            if time_btw != 0:\n",
    "                avg_speed = dist_btw/time_btw\n",
    "\n",
    "                #if the disctance between two points is less than 2*state_d_thrhld, that mean there is an overlap\n",
    "                # in this case, we cannot consider state_d_thrhld as the staypoint region, as:\n",
    "                #             before user leave state_d_thrhld of this staypoint the user already enters the next staypoint\n",
    "                if avg_speed != 0:\n",
    "                    \n",
    "                    #if dist_btw >= 2*state_d_thrhld or dist_btw <= state_d_thrhld:\n",
    "                    if dist_btw >= 2*state_d_thrhld:\n",
    "                        delta_t = min(state_d_thrhld, dist_btw)/avg_speed\n",
    "                    else:\n",
    "                        delta_t = dist_btw/(2* avg_speed)\n",
    "                else:\n",
    "                    delta_t = time_btw/2\n",
    "\n",
    "                end1_trj_time = end1_trj_time + timedelta(minutes=delta_t)\n",
    "                str2_trj_time = str2_trj_time - timedelta(minutes=delta_t)\n",
    "\n",
    "                #think about this later\n",
    "        #         if dist_btw<= 200:\n",
    "        #             mean_lat = (off_staypts.loc[i, 'meanlat'] + off_staypts.loc[i+1, 'meanlat'])/2\n",
    "        #             mean_lon = (off_staypts.loc[i, 'meanlon'] + off_staypts.loc[i+1, 'meanlon'])/2\n",
    "        #             off_staypts.loc[i, 'meanlat'] =  mean_lat\n",
    "        #             off_staypts.loc[i, 'meanlon'] = mean_lon\n",
    "        #             off_staypts.loc[i+1, 'meanlat'] = mean_lat\n",
    "        #             off_staypts.loc[i+1, 'meanlon'] = mean_lon\n",
    "\n",
    "                #curr_ls_r = len(staypts_df)\n",
    "\n",
    "                #add end of prev point line\n",
    "                tobeadded_staypts.loc[j, 'Latitude'] = staypts_df.loc[end_indx, 'Latitude']\n",
    "                tobeadded_staypts.loc[j, 'Longitude'] = staypts_df.loc[end_indx, 'Longitude']\n",
    "                tobeadded_staypts.loc[j, 'Timestamp'] = end1_trj_time\n",
    "                tobeadded_staypts.loc[j, 'Date'] = end1_trj_time.date()\n",
    "                tobeadded_staypts.loc[j, 'Time'] = end1_trj_time.time()\n",
    "                tobeadded_staypts.loc[j, 'Hour'] = end1_trj_time.hour\n",
    "                tobeadded_staypts.loc[j, 'Weekday'] = str(end1_trj_time.weekday())+ end1_trj_time.weekday_name\n",
    "                tobeadded_staypts.loc[j, 'StayPoint'] = 1\n",
    "                tobeadded_staypts.loc[j, 'StayptId'] = staypts_df.loc[end_indx, 'StayptId']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLat'] = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLon'] = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "                tobeadded_staypts.loc[j, 'State'] = -2   #to indicate its po´rocessed\n",
    "                tobeadded_staypts.loc[j, 'StateId'] = -1\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLat'] = -1.0\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLon'] = -1.0\n",
    "                j = j + 1\n",
    "\n",
    "                #add start of current point line\n",
    "                tobeadded_staypts.loc[j, 'Latitude'] = staypts_df.loc[strt_indx, 'Latitude']\n",
    "                tobeadded_staypts.loc[j, 'Longitude'] = staypts_df.loc[strt_indx, 'Longitude']\n",
    "                tobeadded_staypts.loc[j, 'Timestamp'] = str2_trj_time\n",
    "                tobeadded_staypts.loc[j, 'Date'] = str2_trj_time.date()\n",
    "                tobeadded_staypts.loc[j, 'Time'] = str2_trj_time.time()\n",
    "                tobeadded_staypts.loc[j, 'Hour'] = str2_trj_time.hour\n",
    "                tobeadded_staypts.loc[j, 'Weekday'] = str(str2_trj_time.weekday()) + str2_trj_time.weekday_name\n",
    "                tobeadded_staypts.loc[j, 'StayPoint'] = 1\n",
    "                tobeadded_staypts.loc[j, 'StayptId'] = staypts_df.loc[strt_indx, 'StayptId']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLat'] = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLon'] = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "                tobeadded_staypts.loc[j, 'State'] = -2 #to indicate its po´rocessed\n",
    "                tobeadded_staypts.loc[j, 'StateId'] = -1\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLat'] = -1.0\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLon'] = -1.0\n",
    "                j = j + 1\n",
    "    \n",
    "    staypts_df = staypts_df.append(tobeadded_staypts, ignore_index=True)\n",
    "    staypts_df = staypts_df.sort_values(['StayptId', 'Timestamp'])\n",
    "    staypts_df = staypts_df.reset_index(drop=True)            \n",
    "    \n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "\n",
    "#-------------form states-----------------------------------------------------------------------\n",
    "def form_states():\n",
    "    global staypts_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    #update states in final staypoints\n",
    "    #copy staypoint data as state data\n",
    "    staypts_df['StateId'] = staypts_df['StayptId']\n",
    "    staypts_df['StateMeanLat'] = staypts_df['StayMeanLat']\n",
    "    staypts_df['StateMeanLon'] = staypts_df['StayMeanLon']\n",
    "    \n",
    "    #this fucntion groups the staypoints together to from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    staypts_df1 = staypts_df1.reset_index(drop=True)\n",
    "    \n",
    "    row = 1\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i in range(0, len(staypts_df1)-1):\n",
    "        for j in range(i+1, len(staypts_df1)):\n",
    "        \n",
    "            chk_cluster = staypts_df1['StateId'][i]\n",
    "            chk_clulat = staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['StateId'][j]\n",
    "            curr_clulat = staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['StateMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= state_d_thrhld:\n",
    "                #before adding this point to the ith state, \n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "                \n",
    "                add_state = \"Yes\"\n",
    "                #form the existing lat and lon array\n",
    "                array_lat = staypts_df['Latitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = staypts_df['Longitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                #add the new lat and lon values to the array\n",
    "                new_lats = staypts_df['Latitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = staypts_df['Longitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "                \n",
    "                array_lat= np.append(array_lat, new_lats)\n",
    "                array_lon= np.append(array_lon, new_lons)\n",
    "                #cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "                \n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "                        \n",
    "                if add_state == \"Yes\":    \n",
    "                    staypts_df.loc[ (staypts_df['StateId']==curr_cluster), 'StateId'] = chk_cluster\n",
    "                    staypts_df.loc[ (staypts_df['StateId']==chk_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    staypts_df.loc[ (staypts_df['StateId']==chk_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "                \n",
    "    #update states for last hour staypoints\n",
    "    #copy staypoint data as state data\n",
    "    #copy staypoint data as state data\n",
    "    curr_hr_staypts_df['StateId'] = curr_hr_staypts_df['StayptId']\n",
    "    curr_hr_staypts_df['StateMeanLat'] = curr_hr_staypts_df['StayMeanLat']\n",
    "    curr_hr_staypts_df['StateMeanLon'] = curr_hr_staypts_df['StayMeanLon']\n",
    "    \n",
    "    #this fucntion groups the staypoints together to from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.sort_values(['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.reset_index(drop=True)\n",
    "    \n",
    "    row = 1\n",
    "\n",
    "    for i in range(0, len(curr_hr_staypts_df1)):\n",
    "        for j in range(i+1, len(curr_hr_staypts_df1)):\n",
    "        \n",
    "            chk_cluster = curr_hr_staypts_df1['StateId'][i]\n",
    "            chk_clulat = curr_hr_staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = curr_hr_staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = curr_hr_staypts_df1['StateId'][j]\n",
    "            curr_clulat = curr_hr_staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = curr_hr_staypts_df1['StateMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= state_d_thrhld:\n",
    "                #before adding this point to the ith state, \n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "                \n",
    "                add_state = \"Yes\"\n",
    "                #form the existing lat and lon array\n",
    "                array_lat = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                #add the new lat and lon values to the array\n",
    "                new_lats = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "                \n",
    "                array_lat= np.append(array_lat, new_lats)\n",
    "                array_lon= np.append(array_lon, new_lons)\n",
    "                #cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "                \n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "                        \n",
    "                if add_state == \"Yes\":    \n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateId'] = chk_cluster\n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "def cal_hourly_state_weight():\n",
    "    global cluster_hourly_df  \n",
    "    global staypts_df\n",
    "\n",
    "    curr_hr_cluster_hourly_df = pd.DataFrame()       \n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(drop=True)\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['StateId'][0]\n",
    "    curr_count = 0\n",
    "    row = 0\n",
    "    currstate_timestamps = []\n",
    "    currstate_timestamps =  np.append(currstate_timestamps, staypts_df.loc[0, 'Timestamp'])\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        curr_hr_cluster_hourly_df['Date'] = 0\n",
    "        curr_hr_cluster_hourly_df['StateId'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLat'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLon'] = 0\n",
    "        curr_hr_cluster_hourly_df[i] = 0\n",
    "\n",
    "    for i in range(1, len(staypts_df)):\n",
    "\n",
    "        if (staypts_df['StateId'][i] != last_clusid):\n",
    "            start = min(currstate_timestamps)\n",
    "            end = max(currstate_timestamps)\n",
    "\n",
    "            #if the state is with one hour\n",
    "            if start.hour == end.hour:\n",
    "                k = end - start\n",
    "                mins = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                date_read = start.date()\n",
    "                cluster_id = staypts_df['StateId'][i-1]\n",
    "                cluster_mean_lat = staypts_df['StateMeanLat'][i-1]\n",
    "                cluster_mean_lon = staypts_df['StateMeanLon'][i-1]\n",
    "                col_name = start.hour\n",
    "\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'AvgLat'] = cluster_mean_lat\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'AvgLon'] = cluster_mean_lon\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'Date'] = date_read\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'StateId'] = cluster_id\n",
    "                curr_hr_cluster_hourly_df.loc[row, col_name] = round((mins)/60,4)\n",
    "                row = row + 1\n",
    "\n",
    "            #if the state is beyond one hour boundary\n",
    "            else:\n",
    "                end = end + pd.Timedelta(hours=1) - pd.Timedelta(minutes=end.minute)\n",
    "                j = start\n",
    "                while j < end:\n",
    "                    if j == start:\n",
    "                        k = ((j + pd.Timedelta(hours=1) - pd.Timedelta(minutes=j.minute)) - j)\n",
    "                        mins = int((k / np.timedelta64(1, 'm')))\n",
    "                    elif j.hour == end.hour - 1:\n",
    "                        end_time = max(currstate_timestamps)\n",
    "                        k = (end_time - (end_time - pd.Timedelta(minutes=end_time.minute)))\n",
    "                        mins = int((k / np.timedelta64(1, 'm')))\n",
    "                    else:\n",
    "                        mins = 60\n",
    "\n",
    "                    date_read = j.date()\n",
    "                    cluster_id = staypts_df['StateId'][i-1]\n",
    "                    cluster_mean_lat = staypts_df['StateMeanLat'][i-1]\n",
    "                    cluster_mean_lon = staypts_df['StateMeanLon'][i-1]\n",
    "                    col_name = j.hour\n",
    "\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'AvgLat'] = cluster_mean_lat\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'AvgLon'] = cluster_mean_lon\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'Date'] = date_read\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'StateId'] = cluster_id\n",
    "                    curr_hr_cluster_hourly_df.loc[row, col_name] = round((mins)/60,4)\n",
    "                    row = row + 1\n",
    "\n",
    "                    j = j + pd.Timedelta(hours=1)\n",
    "\n",
    "            currstate_timestamps = []\n",
    "            currstate_timestamps =  np.append(currstate_timestamps, staypts_df.loc[i, 'Timestamp'])\n",
    "            last_clusid = staypts_df['StateId'][i]\n",
    "        else:\n",
    "            currstate_timestamps =  np.append(currstate_timestamps, staypts_df.loc[i, 'Timestamp'])\n",
    "\n",
    "\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.fillna(0)\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.groupby(['Date', 'StateId', 'AvgLat', 'AvgLon']).sum()\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(level=[0,1,2,3])\n",
    "   \n",
    "    cluster_hourly_df = curr_hr_cluster_hourly_df\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(drop=True)\n",
    "    \n",
    "    #normalize the hourly weights\n",
    "    for i in range(0, 24):\n",
    "        col = \"Sum\" + str(i)\n",
    "        cluster_hourly_df[col] = cluster_hourly_df.groupby('Date')[i].transform(np.sum)\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        col = \"Sum\" + str(i)\n",
    "        cluster_hourly_df[i] = cluster_hourly_df[i]/cluster_hourly_df[col]\n",
    "        cluster_hourly_df = cluster_hourly_df.drop([col], axis=1)\n",
    "    cluster_hourly_df = cluster_hourly_df.fillna(0)\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------\n",
    "def del_staypts_less_dur(prev_date):\n",
    "    global staypts_df\n",
    "    \n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "    idx = staypts_df.index[staypts_df['Date'] == prev_date]\n",
    "    if idx.empty:\n",
    "         return\n",
    "    start = min(idx) + 1\n",
    "\n",
    "    prev_stateid = staypts_df.loc[start-1, 'StateId'] \n",
    "    state_cur_count = 1\n",
    "    for i in range(start, len(staypts_df)):\n",
    "        if (prev_stateid != staypts_df.loc[i, 'StateId'] ):\n",
    "\n",
    "            duration = staypts_df.loc[i-1, 'Timestamp'] - staypts_df.loc[i-state_cur_count, 'Timestamp']\n",
    "            seconds = duration.seconds\n",
    "            minutes = seconds / 60\n",
    "\n",
    "            if minutes < staypts_t_thrhld:\n",
    "                for j in range(i-state_cur_count, i):\n",
    "                    staypts_df.loc[j, 'State'] = -3\n",
    "\n",
    "            state_cur_count = 1\n",
    "            prev_stateid = staypts_df.loc[i, 'StateId']\n",
    "        else:\n",
    "            state_cur_count = state_cur_count + 1\n",
    "\n",
    "    staypts_df = staypts_df[staypts_df.State != -3]\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------\n",
    "def visualize_hourly_state_weight():\n",
    "    global cluster_hourly_df\n",
    "    \n",
    "    # create a color dictionary for each cluster for the plot\n",
    "    dicts = {}\n",
    "    pos_dicts = {}\n",
    "    clu_list = []\n",
    "    clu_list = cluster_hourly_df['StateId'].unique()\n",
    "    r = lambda: random.randint(0, 255)\n",
    "    \n",
    "    for i in range(0, len(clu_list)):\n",
    "        # icts[clu_list[i]] = (colors[i])\n",
    "        dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(), r(), r()))\n",
    "        \n",
    "     # create a new graph where we will later add rectangles for each hour:cluster\n",
    "    fig2 = plt.figure(figsize=(18, 18))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "    \n",
    "    # get all the dates for y axis\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    date_range = list(range(min(date_list).day, max(date_list).day + 1))\n",
    "    y = range(0, len(date_range))\n",
    "    def_yticks = date_range\n",
    "    plt.yticks(y, def_yticks)\n",
    "\n",
    "    # set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_range))\n",
    "    \n",
    "    date_counter = 0\n",
    "    last_date = cluster_hourly_df['Date'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "\n",
    "    # drawing verical lines for each hour\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x=i, linewidth=1, color='r')\n",
    "    # horizontal lines\n",
    "    for i in range(min(date_list).day, max(date_list).day):\n",
    "        ax1.axhline(y=i, linewidth=1, color='r')\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        pos_dicts[i] = i\n",
    "    \n",
    "    for i in range(0, len(cluster_hourly_df)):\n",
    "        \n",
    "        if cluster_hourly_df.loc[i, 'Date'] != last_date:\n",
    "            for k in range(0, 24):\n",
    "                pos_dicts[k] = k\n",
    "            add_days = (cluster_hourly_df['Date'][i] - last_date).days\n",
    "            date_counter = date_counter + add_days\n",
    "            last_date = cluster_hourly_df['Date'][i]\n",
    "        \n",
    "        for j in range(0, 24):\n",
    "            \n",
    "            a = float(pos_dicts.get(j))   \n",
    "            b = a + cluster_hourly_df.loc[i, j]\n",
    "            pos_dicts[j] = b\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            if width != 0:\n",
    "                col_id = dicts.get(cluster_hourly_df['StateId'][i])\n",
    "                ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id,\n",
    "                                            label=cluster_hourly_df['StateId'][i]))\n",
    "                ax1.annotate(int(cluster_hourly_df['StateId'][i]), (a + width / 2, height / 2 + date_counter),\n",
    "                         color='w', weight='bold', fontsize=10, ha='center', va='center')\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "    destpng = usr_directory + \"/online.png\"\n",
    "    plt.savefig(destpng)\n",
    "    plt.show()\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_staypts_csv():\n",
    "    staypts_df.to_csv(dest_file_staypoints, sep='\\t', encoding='utf-8')\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_hourly_weights_csv():\n",
    "    cluster_hourly_df.to_csv(dest_file_hourly_weights,  sep='\\t', encoding='utf-8')\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_seperate_trasition_matrices():\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    \n",
    "    #create a temp dataframe for each data, and calculate trasition matrices from hour t to t+1\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "\n",
    "        #create a temp dataframe for pervious date\n",
    "        temp_df = pd.DataFrame()\n",
    "        matrices_df = pd.DataFrame()\n",
    "        temp_df = cluster_hourly_df.loc[cluster_hourly_df['Date'] == date_list[p]]\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0, 24):\n",
    "            matrices_df['Date'] = 0\n",
    "            matrices_df['StateId'] = 0\n",
    "            for j in range(0, len(temp_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['StateId'][j])\n",
    "                matrices_df[colname] = 0\n",
    "\n",
    "        matrices_df['Date'] = temp_df['Date']\n",
    "        matrices_df['StateId'] = temp_df['StateId']\n",
    "\n",
    "        for i in range (0, 23):\n",
    "            for j in range (0, len(temp_df)):\n",
    "                for k in range (0, len(temp_df)):\n",
    "                    prob = temp_df[i][j] * temp_df[i+1][k]\n",
    "                    colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['StateId'][k])\n",
    "                    matrices_df[colname][j] = prob\n",
    "        file_name = dest_path_each_day_trsn_mat + str(date_list[p]) + \".csv\"\n",
    "        matrices_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_markov_chains():\n",
    "    global final_transition_df\n",
    "    global co_loc\n",
    "    \n",
    "    final_transition_df = pd.DataFrame()\n",
    "\n",
    "    #create an empty markov chain frame for each state, and transition for each hour of the day\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    cluster_list = cluster_hourly_df['StateId'].unique()\n",
    "    AvgLat_list = cluster_hourly_df['AvgLat'].unique()\n",
    "    AvgLon_list = cluster_hourly_df['AvgLon'].unique()\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        final_transition_df['Address'] = 0\n",
    "        final_transition_df['AvgLat'] = 0\n",
    "        final_transition_df['AvgLon'] = 0\n",
    "        final_transition_df['StateId'] = 0\n",
    "        for j in range(0, cluster_hourly_df['StateId'].nunique()):\n",
    "            colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(cluster_list[j])\n",
    "            final_transition_df[colname] = 0\n",
    "\n",
    "    final_transition_df['StateId'] = cluster_list\n",
    "    final_transition_df['AvgLat'] = AvgLat_list\n",
    "    final_transition_df['AvgLon'] = AvgLon_list\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df.index = final_transition_df.StateId\n",
    "\n",
    "    #read each day file and sum the matching rows:cols combinations\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    path_dir = dest_path_each_day_trsn_mat\n",
    "\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        temp_df = pd.DataFrame()\n",
    "        filename = path_dir + str(date_list[p]) + '.csv'\n",
    "        temp_df =  pd.read_csv(filename, header = 0, sep='\\t')\n",
    "\n",
    "        for i in range(0, len(temp_df)):\n",
    "            rowname = temp_df['StateId'][i]\n",
    "            for src_column in temp_df:\n",
    "                for dest_column in final_transition_df:\n",
    "                    if src_column == dest_column and src_column != 'StateId' :\n",
    "                        #import pdb; pdb.set_trace()\n",
    "                        final_transition_df[dest_column][rowname] = (final_transition_df[dest_column][rowname] +\n",
    "                                                                    temp_df[src_column][i])\n",
    "\n",
    "    #replace zero to a small value \n",
    "    final_transition_df = final_transition_df.fillna(0)                    \n",
    "    final_transition_df = final_transition_df.replace(0, 0.00001)\n",
    "\n",
    "    #calculate probability from cluster x to cluster y from time t to t+1\n",
    "    final_transition_df = final_transition_df.reset_index(drop=True)\n",
    "    for clus in range(0, len(final_transition_df)):\n",
    "        for i in range(0, 24):\n",
    "            temp_sum = 0\n",
    "            for j in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['StateId'][j])\n",
    "                temp_sum += (final_transition_df[colname][clus])\n",
    "            for k in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['StateId'][k])\n",
    "                if temp_sum != 0:\n",
    "                    final_transition_df[colname][clus] = final_transition_df[colname][clus]/temp_sum\n",
    "\n",
    "    #create dictionary for coordinate : address\n",
    "    points = tuple(zip(final_transition_df.AvgLat, final_transition_df.AvgLon))\n",
    "    geocoder = Nominatim(timeout=10)\n",
    "    coordinate_location = {}\n",
    "\n",
    "    for coordinate in points:\n",
    "        try:\n",
    "            location = geocoder.reverse(coordinate)\n",
    "        except:\n",
    "            location = 'unknown'\n",
    "        coordinate_location[coordinate] = location\n",
    "\n",
    "    co_loc = {k:v for k,v in coordinate_location.items()}\n",
    "\n",
    "    for i in range(0, len(final_transition_df)):\n",
    "        address = co_loc.get((final_transition_df['AvgLat'][i], final_transition_df['AvgLon'][i]))\n",
    "        if address == 'unknown':\n",
    "            final_transition_df['Address'][i] = 'unknown'\n",
    "        else:\n",
    "            final_transition_df['Address'][i] = address[0]\n",
    "\n",
    "    #save the file\n",
    "    final_transition_df.to_csv(dest_file_final_markov_chain)\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        final_transition_temp_df = pd.DataFrame()\n",
    "        k = cluster_hourly_df['StateId'].nunique()*i + 4\n",
    "        final_transition_temp_df = final_transition_df.iloc[:,k:k + cluster_hourly_df['StateId'].nunique()]\n",
    "        final_transition_temp_df.index = cluster_list\n",
    "        file_name = usr_markov_chains_directory + \"/\" + str(i) + \" hour.csv\"\n",
    "        final_transition_temp_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def predict(prev_hour):\n",
    "    global trained_model_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    tobepredicted_df = curr_hr_staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon', 'Timestamp']]\n",
    "    tobepredicted_df = tobepredicted_df.drop_duplicates()\n",
    "    tobepredicted_df = tobepredicted_df.reset_index(drop=True)\n",
    "\n",
    "    for j in range(0, len(tobepredicted_df)):\n",
    "\n",
    "        new_lat = tobepredicted_df['StateMeanLat'][j]\n",
    "        new_lon = tobepredicted_df['StateMeanLon'][j]\n",
    "        #file_name = \"PredTime- \" +  str(tobepredicted_df['Timestamp'][j]) + \".csv\"\n",
    "        file_name = \"Pred\" + \".csv\"\n",
    "        for i in range(0, len(trained_model_df)):\n",
    "\n",
    "            trn_lat = trained_model_df['AvgLat'][i]\n",
    "            trn_lon = trained_model_df['AvgLon'][i]\n",
    "            if meters(trn_lat, trn_lon, new_lat, new_lon) <= state_d_thrhld:\n",
    "\n",
    "                predic_df = pd.DataFrame()\n",
    "\n",
    "                cluster_id = trained_model_df['StateId'][i]\n",
    "                curr_lat = trained_model_df['AvgLat'][i]\n",
    "                curr_lon = trained_model_df['AvgLon'][i]\n",
    "                curr_add = trained_model_df['Address'][i]\n",
    "                pred_loc = {\"current\":(cluster_id, curr_lat, curr_lon, curr_add)}\n",
    "\n",
    "                from_col_no = trained_model_df['StateId'].nunique() * prev_hour + 5\n",
    "                to_col_no = from_col_no + trained_model_df['StateId'].nunique()\n",
    "                predic_df = trained_model_df.iloc[i:i+1,from_col_no:to_col_no]\n",
    "                predic_df = predic_df.T\n",
    "                predic_df['StateId'] = cluster_id\n",
    "                predic_df['PredState'] = predic_df.index\n",
    "                predic_df['PredState'] = predic_df['PredState'].map(lambda x: x.split('-', 2)[-1])\n",
    "                predic_df.columns = ['Probability', 'StateId', 'PredState']\n",
    "                predic_df = predic_df.sort_values('Probability', ascending=False).head(5)\n",
    "                predic_df['Address'] = 0\n",
    "                predic_df['Latitude'] = 0.0\n",
    "                predic_df['Longitude'] = 0.0\n",
    "                predic_df = predic_df.reset_index(drop=True)\n",
    "\n",
    "                for j in range (0, len(predic_df)):\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    clus_to_find = int(float(predic_df['PredState'][j]))\n",
    "                    add = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'Address'].values[0]\n",
    "                    lat = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'AvgLat'].values[0]\n",
    "                    lon = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'AvgLon'].values[0]\n",
    "\n",
    "                    predic_df.loc[j, 'Address'] = add\n",
    "                    predic_df.loc[j, 'Latitude'] = lat\n",
    "                    predic_df.loc[j, 'Longitude'] = lon\n",
    "                file = dest_predicted_dir + file_name\n",
    "                print(\"Prediction --\\n\")\n",
    "                print(\"Current hour - \" + str(prev_hour))\n",
    "                print(\"\\nPrediction\\n\")\n",
    "                print(predic_df)\n",
    "                predic_df.to_csv(file, sep='\\t', encoding='utf-8')\n",
    "                break\n",
    "            \n",
    "#------------------------------------------ S T A R T -----------------------------------------------\n",
    "def main():\n",
    "    global usr_trejec_df\n",
    "    global trained_model_df\n",
    "    global curr_hr_df\n",
    "    global curr_hr_staypts_df\n",
    "    global staypts_df\n",
    "    global cluster_hourly_df\n",
    "    global final_transition_df\n",
    "    \n",
    "    #read test user trajectory file. In real scenerio, this will be the GPS read data\n",
    "    read_usr_file()\n",
    "\n",
    "    #prepere dataframes\n",
    "    prepare_dfs()\n",
    "\n",
    "    #Save first date and time as prev date and time for the start\n",
    "    prev_date = usr_trejec_df['Date'][0]\n",
    "    prev_hour = usr_trejec_df['Hour'][0]\n",
    "\n",
    "    #I. Read the new locations in an online gps location input mode\n",
    "    #  1. Everytime the hour changes, \n",
    "    #                  A. Find staypoints for the last hour and assign staypointID\n",
    "    #                  B. Cluster staypoints based on distance for last hour, form states and assign stateID\n",
    "    #                  C. Calculate state hourly weights for last hour\n",
    "    #                  D. Predict based on trained data(if available)\n",
    "    #  2. Everytime the date changes,\n",
    "    #                  A. Add the days data into the training data\n",
    "    #  3. If the hour and the time has not been changed, add the data to current hour data\n",
    "\n",
    "    #I\n",
    "    for i in range(0, len(usr_trejec_df)):\n",
    "\n",
    "        #store the read hour and date as new hour and new date\n",
    "        new_hour = usr_trejec_df['Hour'][i]\n",
    "        new_date = usr_trejec_df['Date'][i]\n",
    "\n",
    "        #1. \n",
    "        #if the hour has changed\n",
    "        if (new_hour != prev_hour): \n",
    "            #process the last hour data if available\n",
    "            if not curr_hr_df.empty:\n",
    "                #A.\n",
    "          \n",
    "                create_last_hr_staypts() \n",
    "                add_start_end_times()\n",
    "                if not curr_hr_staypts_df.empty:\n",
    "                     #B.\n",
    "                    form_states()\n",
    "                    #read_trained_model()\n",
    "                    #if not trained_model_df.empty:\n",
    "                    #    predict(prev_hour)\n",
    "            \n",
    "            prev_hour = new_hour \n",
    "            curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])  \n",
    "         #2. \n",
    "         #if the date has changed\n",
    "            if (new_date != prev_date):\n",
    "                if not staypts_df.empty:\n",
    "                    #del_staypts_less_dur(prev_date)\n",
    "                    #visualize_hourly_state_weight()\n",
    "                    cal_hourly_state_weight()\n",
    "                    update_staypts_csv()\n",
    "                    update_hourly_weights_csv()\n",
    "                    create_save_seperate_trasition_matrices()\n",
    "                    create_save_markov_chains()\n",
    "                prev_date = new_date           \n",
    "\n",
    "        #3. \n",
    "        #if the date and the hour has not changed, just add it to current hour dataframe.\n",
    "        # this dataframe is used once the hour is changed.\n",
    "        else:\n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])   \n",
    "            \n",
    "    visualize_hourly_state_weight()\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE INPUTS HERE-------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "state_d_thrhld = 200\n",
    "staypts_d_thrhld = 200\n",
    "staypts_t_thrhld = 20\n",
    "track_t_thrhld = 30\n",
    "#destination paths\n",
    "base_path = r\"C:\\Users\\12sha\\Documents\\thesislocation\\code_\\stay points\\v0.6 results\"\n",
    "\n",
    "for i in range (22, 30):\n",
    "    user = usern_mnth_df.loc[i, 'user']\n",
    "    month = usern_mnth_df.loc[i, 'year'] + usern_mnth_df.loc[i, 'month'] + \"01\"\n",
    "    \n",
    "    date_s = usern_mnth_df.loc[i, 'year'] + usern_mnth_df.loc[i, 'month'] + \"01\"\n",
    "    date_d = datetime(year=int(month[0:4]), month=int(month[4:6]), day=int(date_s[6:8]))\n",
    "    next_mn = date_d + relativedelta(months=1)\n",
    "    next_mn = next_mn.strftime('%Y%m%d')\n",
    "    \n",
    "    month = month[:-2]\n",
    "    next_mn = next_mn[:-2]\n",
    "    \n",
    "    for j in range(0, 2):\n",
    "        #source paths\n",
    "        file_source_raw = \"C:/Users/12sha/Documents/Geolife Trajectories 1.3/Data/\" + user + \"/Trajectory/\" + month + \"*.plt\"\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------------\n",
    "        #--------------------------------------CHANGE INPUTS HERE-------------------------------------------------------------\n",
    "        #---------------------------------------------------------------------------------------------------------------------\n",
    "        usr_directory = base_path + \"/User \" + user + \"/\" + month\n",
    "        usr_hrly_wght_directory = base_path + \"/User \" + user  + \"/\" + month + \"/hourlyweights\"\n",
    "        usr_sty_pts_directory = base_path + \"/User \" + user + \"/\" + month + \"/staypoints\"\n",
    "        usr_markov_chains_directory = base_path + \"/User \" + user + \"/\" + month + \"/markovchains\"\n",
    "        dest_predicted_dir = base_path + \"/User \" + user + \"/\" + month + \"/predict/\"\n",
    "\n",
    "        if not os.path.exists(usr_directory):\n",
    "            os.makedirs(usr_directory)\n",
    "        if not os.path.exists(usr_hrly_wght_directory):\n",
    "            os.makedirs(usr_hrly_wght_directory)\n",
    "        if not os.path.exists(usr_sty_pts_directory):\n",
    "            os.makedirs(usr_sty_pts_directory)  \n",
    "        if not os.path.exists(usr_markov_chains_directory):\n",
    "            os.makedirs(usr_markov_chains_directory)  \n",
    "        if not os.path.exists(dest_predicted_dir):\n",
    "            os.makedirs(dest_predicted_dir)  \n",
    "\n",
    "        #destination file names\n",
    "        dest_file_staypoints = usr_sty_pts_directory + \"/staypoints.csv\"\n",
    "        dest_file_hourly_weights = usr_hrly_wght_directory + \"/hourlyweights.csv\"\n",
    "        dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "        dest_file_final_markov_chain = usr_markov_chains_directory + \"/final.csv\"\n",
    "\n",
    "        #remove if the file already exists\n",
    "\n",
    "        try:\n",
    "            os.remove(dest_file_staypoints)   \n",
    "        except OSError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(dest_file_hourly_weights)\n",
    "        except OSError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(dest_file_final_markov_chain)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        #global dataframes used\n",
    "        #user raw trajectory dataframe\n",
    "        usr_trejec_df = pd.DataFrame()\n",
    "        #user trained model\n",
    "        trained_model_df = pd.DataFrame()\n",
    "        #current hour points\n",
    "        curr_hr_df = pd.DataFrame()\n",
    "        #current hour staypoints\n",
    "        curr_hr_staypts_df = pd.DataFrame()\n",
    "        #last hour last point\n",
    "        prev_hour_last_point = pd.DataFrame()\n",
    "        #all staypoints\n",
    "        staypts_df = pd.DataFrame()\n",
    "        #hourly cluster\n",
    "        cluster_hourly_df = pd.DataFrame()\n",
    "        #final markov chains\n",
    "        final_transition_df = pd.DataFrame()\n",
    "\n",
    "        clus_dict = {}\n",
    "        co_loc = {}\n",
    "        pred_loc = {}\n",
    "        lat_array = []\n",
    "        lon_array = []\n",
    "        global_count = 0\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------\n",
    "        main()\n",
    "\n",
    "        month = next_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
