{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.pyplot import figure\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "import operator\n",
    "import pdb\n",
    "from geopy.geocoders import Nominatim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4ce68e0055bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[1;31m#---------------------------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-4ce68e0055bf>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[1;31m#read test user trajectory file. In real scenerio, this will be the GPS read data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m     \u001b[0mread_usr_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[1;31m#prepere dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4ce68e0055bf>\u001b[0m in \u001b[0;36mread_usr_file\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#put the data from list into one dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0musr_trejec_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_dfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0musr_trejec_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Latitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Longitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Altitude'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NumDays'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0musr_trejec_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Timestamp\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0musr_trejec_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0musr_trejec_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    223\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#online process\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_usr_file():\n",
    "    global usr_trejec_df\n",
    "    \n",
    "    #Load file names for user\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    usr_trejec_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    usr_trejec_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Date\"].map(str) + \" \" + usr_trejec_df[\"Time\"]\n",
    "    \n",
    "    usr_trejec_df.Timestamp = pd.to_datetime(usr_trejec_df.Timestamp)\n",
    "    \n",
    "    usr_trejec_df.index = usr_trejec_df['Timestamp']\n",
    "    usr_trejec_df = usr_trejec_df.resample('1T').mean()\n",
    "    usr_trejec_df = usr_trejec_df.dropna()\n",
    "    \n",
    "     #add columns to user trajectory dataframe\n",
    "    #1. add timestamp as column\n",
    "    usr_trejec_df['Timestamp'] = pd.to_datetime(usr_trejec_df.index)\n",
    "    #restore date and time column\n",
    "    usr_trejec_df['Date'] = usr_trejec_df.Timestamp.dt.date\n",
    "    usr_trejec_df['Time'] = usr_trejec_df.Timestamp.dt.time\n",
    "    usr_trejec_df['Hour'] = usr_trejec_df.Timestamp.dt.hour\n",
    "    \n",
    "    #sort values based on timestamp\n",
    "    usr_trejec_df = usr_trejec_df.sort_values(['Timestamp'])\n",
    "    #reset index\n",
    "    usr_trejec_df = usr_trejec_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    usr_trejec_df['Weekday'] = usr_trejec_df['Timestamp'].dt.weekday.map(str) + usr_trejec_df['Timestamp'].dt.weekday_name\n",
    "\n",
    "    usr_trejec_df['StayPoint'] = -1 # 1 if it is a staypoint, else 0\n",
    "    usr_trejec_df['StayptId'] = -1\n",
    "    usr_trejec_df['StayMeanLat'] = -1.0\n",
    "    usr_trejec_df['StayMeanLon'] = -1.0\n",
    "    usr_trejec_df['State'] = -1     # 1 if it is a state, else 0\n",
    "    usr_trejec_df['StateId'] = -1\n",
    "    usr_trejec_df['StateMeanLat'] = -1.0\n",
    "    usr_trejec_df['StateMeanLon'] = -1.0\n",
    "    \n",
    "    #remove columns not used/required\n",
    "    usr_trejec_df = usr_trejec_df.drop(['0', 'Altitude', 'NumDays'], axis = 1)\n",
    "\n",
    "#-------------------------------\n",
    "def prepare_dfs():\n",
    "    global cluster_hourly_df \n",
    "\n",
    "    #create cluster_hourly_df columns\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['StateId'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "        \n",
    "#------------------------------------------------------------------------------------\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def cluster(newlat, newlon, row, count, orig_lat, orig_lon):\n",
    "    global curr_hr_df\n",
    "    \n",
    "    currcluster = curr_hr_df['StayptId'][row-1]\n",
    "    curr_hr_df['StayptId'][row] = -1\n",
    "    curr_hr_df['StayMeanLat'][row] = -1.0\n",
    "    curr_hr_df['StayMeanLon'][row] = -1.0\n",
    "    curr_hr_df['StayPoint'][row] = -1\n",
    "    clulat = curr_hr_df['StayMeanLat'][row-1]\n",
    "    clulon = curr_hr_df['StayMeanLon'][row-1]\n",
    "    \n",
    "    #if the new point and the old point time difference is greater than tracking threshold\n",
    "    # then add both the points as staypoints and leave\n",
    "    prevPointTime = curr_hr_df['Timestamp'][row-1]\n",
    "    currPointTime = curr_hr_df['Timestamp'][row]\n",
    "    timm_diff = (currPointTime - prevPointTime).seconds /60\n",
    "    dist_diff = meters(clulat, clulon, newlat, newlon)   \n",
    "    \n",
    "    if (timm_diff >= track_t_thrhld and dist_diff > staypts_d_thrhld ):\n",
    "        curr_hr_df.loc[row-1, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row-1, 'StayMeanLat'] = curr_hr_df.loc[row-1, 'Latitude']\n",
    "        curr_hr_df.loc[row-1, 'StayMeanLon'] = curr_hr_df.loc[row-1, 'Longitude']\n",
    "        curr_hr_df.loc[row, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row, 'StayMeanLat'] = curr_hr_df.loc[row, 'Latitude']\n",
    "        curr_hr_df.loc[row, 'StayMeanLon'] = curr_hr_df.loc[row, 'Longitude']\n",
    "        curr_hr_df.loc[row, 'StayptId'] = currcluster + 1\n",
    "    else:    \n",
    "        #if the new point and old point's distance is less than threshold, then add it to current cluster\n",
    "        if meters(clulat, clulon, newlat, newlon)<= staypts_d_thrhld:\n",
    "            curr_hr_df['StayptId'][row] = currcluster\n",
    "            #calculate new mean lat and lon for the cluster\n",
    "            array_lat = curr_hr_df['Latitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "            array_lon = curr_hr_df['Longitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "\n",
    "            #cal new means\n",
    "            new_lat_mean = np.mean(array_lat)\n",
    "            new_lon_mean = np.mean(array_lon)\n",
    "\n",
    "            curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayMeanLat'] = new_lat_mean\n",
    "            curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayMeanLon'] = new_lon_mean\n",
    "\n",
    "    #         curr_hr_df['StayMeanLat'] = curr_hr_df.groupby('StayptId')['Latitude'].transform(np.mean)\n",
    "    #         curr_hr_df['StayMeanLon'] = curr_hr_df.groupby('StayptId')['Longitude'].transform(np.mean)\n",
    "            count = count + 1\n",
    "\n",
    "        #if the new point and old point's distance is greater than threshold, it means the point moved away\n",
    "        #if the previous cluster has more than two points, check the duration of the previous cluster\n",
    "        #   if the duration of the previos cluster is greater than threshold, assign it as a staypoint\n",
    "\n",
    "        #if the row read is the last row for this hour\n",
    "        if (row == len(curr_hr_df)-1):\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row-count+1]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayPoint'] = 1      \n",
    "                #incase the cluster is not a staypoint and the first point is already a staypoint\n",
    "                #then retain the latitued and longitudes\n",
    "                else:\n",
    "                    if (row-count) == 0 and curr_hr_df['StayPoint'][row-count] == 1:\n",
    "                        curr_hr_df['StayMeanLat'][row-count] = orig_lat\n",
    "                        curr_hr_df['StayMeanLon'][row-count] = orig_lon\n",
    "\n",
    "        #if the new point is moving away from the cluster\n",
    "        if meters(clulat, clulon, newlat, newlon)> staypts_d_thrhld:\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row-count]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row-1]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayPoint'] = 1\n",
    "                #incase the cluster is not a staypoint and the first point is already a staypoint\n",
    "                #then retain the latitued and longitudes\n",
    "                else:\n",
    "                    if (row-count) == 0 and curr_hr_df['StayPoint'][row-count] == 1:\n",
    "                        curr_hr_df['StayMeanLat'][row-count] = orig_lat\n",
    "                        curr_hr_df['StayMeanLon'][row-count] = orig_lon\n",
    "            \n",
    "            count = 1\n",
    "            curr_hr_df['StayMeanLat'][row] = curr_hr_df['Latitude'][row]\n",
    "            curr_hr_df['StayMeanLon'][row] = curr_hr_df['Longitude'][row]\n",
    "            curr_hr_df['StayptId'][row] = currcluster + 1\n",
    "\n",
    "    return count\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_trained_model():\n",
    "    global trained_model_df\n",
    "    \n",
    "    if os.path.isfile(dest_file_final_markov_chain):\n",
    "        trained_model_df = pd.read_csv(dest_file_final_markov_chain, header = 0)\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "def create_last_hr_staypts():\n",
    "    global curr_hr_df          #holds current hour points\n",
    "    global staypts_df          #holds all staypoints\n",
    "    global curr_hr_staypts_df  #holds current hour staypoints only\n",
    "    global prev_hour_last_point #stores last hour last point\n",
    "    \n",
    "    #clear current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_staypts_df.iloc[0:0]\n",
    "    \n",
    "    #reset index of current hour points\n",
    "    curr_hr_df = curr_hr_df.reset_index(drop=True)\n",
    "    \n",
    "    #feching the last stayptid \n",
    "    if not staypts_df.empty:\n",
    "        stayid = staypts_df['StayptId'].max() + 1 #assign next possible staypt id\n",
    "    else:\n",
    "        stayid = 1 #if this is the start, start from 1 as staypointID\n",
    "    \n",
    "    adding_first_as_staypt = 'N'\n",
    "    adding_last_as_staypt = 'N'\n",
    "    orig_lat = 0\n",
    "    orig_lon = 0\n",
    "    if not prev_hour_last_point.empty:\n",
    "        #check the time difference of last hour last point and this hour first point is greater than track t threshold\n",
    "        #if yes than add both as staypoints\n",
    "        if (int(time.mktime(curr_hr_df.loc[0, 'Timestamp'].timetuple()) - \n",
    "                   time.mktime(prev_hour_last_point.loc[0, 'Timestamp'].timetuple()))/60 > track_t_thrhld):\n",
    "            if prev_hour_last_point.loc[0, 'StayPoint'] != 1:\n",
    "                adding_last_as_staypt = 'Y'\n",
    "                prev_hour_last_point.loc[0, 'StayptId'] = stayid\n",
    "                stayid = stayid + 1\n",
    "                prev_hour_last_point.loc[0, 'StayPoint'] = 1\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLat'] = prev_hour_last_point.loc[0, 'Latitude']\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLon'] = prev_hour_last_point.loc[0, 'Longitude']\n",
    "                staypts_df = staypts_df.append(prev_hour_last_point)\n",
    "\n",
    "            curr_hr_df.loc[0, 'StayptId'] = stayid\n",
    "            stayid = stayid + 1\n",
    "            curr_hr_df.loc[0, 'StayPoint'] = 1\n",
    "            orig_lat = curr_hr_df.loc[0, 'Latitude']\n",
    "            orig_lon = curr_hr_df.loc[0, 'Longitude']\n",
    "            adding_first_as_staypt = 'Y'\n",
    "        \n",
    "            \n",
    "    #Read the file in an online manner as the points come and assign the points to clusters\n",
    "    row =1\n",
    "    count = 1\n",
    "    \n",
    "    if adding_first_as_staypt == 'N': \n",
    "        curr_hr_df['StayptId'][row-1] = stayid\n",
    "        curr_hr_df['StayPoint'][row-1] = -1\n",
    "        \n",
    "    curr_hr_df['StayMeanLat'][row-1] = curr_hr_df['Latitude'][0]\n",
    "    curr_hr_df['StayMeanLon'][row-1] = curr_hr_df['Longitude'][0]\n",
    "    \n",
    "    \n",
    "    while row < len(curr_hr_df):\n",
    "        count = cluster(curr_hr_df['Latitude'][row], curr_hr_df['Longitude'][row], row, count, orig_lat, orig_lon)\n",
    "        row= row + 1\n",
    "    \n",
    "    #copy the staypoints to the current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_df.loc[curr_hr_df['StayPoint'] == 1]\n",
    "    #copy the stay points into another dataframe\n",
    "    staypts_df = staypts_df.append(curr_hr_df.loc[curr_hr_df['StayPoint'] == 1])\n",
    "    #reset staypoints index\n",
    "    curr_hr_staypts_df.index = curr_hr_staypts_df['Timestamp']\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "\n",
    "    #store the last hour last point\n",
    "    prev_hour_last_point = prev_hour_last_point.iloc[0:0]\n",
    "    prev_hour_last_point = curr_hr_df.iloc[[len(curr_hr_df)-1]]\n",
    "    prev_hour_last_point = prev_hour_last_point.reset_index(drop=True)\n",
    "    \n",
    "    #clear current hour dataframe content\n",
    "    curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def add_start_end_times():\n",
    "    global staypts_df\n",
    "    \n",
    "    if staypts_df.empty or len(staypts_df) == 1:\n",
    "        return\n",
    "    \n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "    \n",
    "     #state = -2 indicates it is already processed\n",
    "    idx = staypts_df.index[staypts_df['State'] == -1]\n",
    "    if idx.empty:\n",
    "         return\n",
    "\n",
    "    if min(idx) == 0:\n",
    "        start = min(idx)\n",
    "    else:\n",
    "        start = min(idx) - 1\n",
    "    \n",
    "    prev_id = staypts_df.loc[start, 'StayptId']\n",
    "    tobeadded_staypts = pd.DataFrame(columns=['Latitude', 'Longitude', 'Timestamp', 'Date', 'Time',\n",
    "                                              'Hour', 'Weekday', 'StayPoint', 'StayptId', 'StayMeanLat',\n",
    "                                              'StayMeanLon', 'State', 'StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    j = 0\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i in range(start, len(staypts_df)):\n",
    "        #update state as -2 indicating processed\n",
    "        staypts_df.loc[i, 'State'] = -2 \n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        if staypts_df.loc[i, 'StayptId'] != prev_id:\n",
    "            prev_id = staypts_df.loc[i, 'StayptId']\n",
    "            strt_indx = i\n",
    "            end_indx = i-1\n",
    "\n",
    "            #calclulate time to be added\n",
    "            end1_trj_time = staypts_df.loc[end_indx, 'Timestamp']\n",
    "            end1_trj_lat = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "            end1_trj_lon = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "            str2_trj_time = staypts_df.loc[strt_indx, 'Timestamp']\n",
    "            str2_trj_lat = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "            str2_trj_lon = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "\n",
    "            dist_btw = meters(end1_trj_lat, end1_trj_lon, str2_trj_lat, str2_trj_lon)\n",
    "            time_btw = (str2_trj_time - end1_trj_time).seconds / 60\n",
    "\n",
    "            if time_btw != 0:\n",
    "                avg_speed = dist_btw/time_btw\n",
    "\n",
    "                #if the disctance between two points is less than 2*state_d_thrhld, that mean there is an overlap\n",
    "                # in this case, we cannot consider state_d_thrhld as the staypoint region, as:\n",
    "                #             before user leave state_d_thrhld of this staypoint the user already enters the next staypoint\n",
    "                if avg_speed != 0:\n",
    "                    \n",
    "                    #if dist_btw >= 2*state_d_thrhld or dist_btw <= state_d_thrhld:\n",
    "                    if dist_btw >= 2*state_d_thrhld:\n",
    "                        delta_t = min(state_d_thrhld, dist_btw)/avg_speed\n",
    "                    else:\n",
    "                        delta_t = dist_btw/(2* avg_speed)\n",
    "                else:\n",
    "                    delta_t = time_btw/2\n",
    "\n",
    "                end1_trj_time = end1_trj_time + timedelta(minutes=delta_t)\n",
    "                str2_trj_time = str2_trj_time - timedelta(minutes=delta_t)\n",
    "\n",
    "                #think about this later\n",
    "        #         if dist_btw<= 200:\n",
    "        #             mean_lat = (off_staypts.loc[i, 'meanlat'] + off_staypts.loc[i+1, 'meanlat'])/2\n",
    "        #             mean_lon = (off_staypts.loc[i, 'meanlon'] + off_staypts.loc[i+1, 'meanlon'])/2\n",
    "        #             off_staypts.loc[i, 'meanlat'] =  mean_lat\n",
    "        #             off_staypts.loc[i, 'meanlon'] = mean_lon\n",
    "        #             off_staypts.loc[i+1, 'meanlat'] = mean_lat\n",
    "        #             off_staypts.loc[i+1, 'meanlon'] = mean_lon\n",
    "\n",
    "                #curr_ls_r = len(staypts_df)\n",
    "\n",
    "                #add end of prev point line\n",
    "                tobeadded_staypts.loc[j, 'Latitude'] = staypts_df.loc[end_indx, 'Latitude']\n",
    "                tobeadded_staypts.loc[j, 'Longitude'] = staypts_df.loc[end_indx, 'Longitude']\n",
    "                tobeadded_staypts.loc[j, 'Timestamp'] = end1_trj_time\n",
    "                tobeadded_staypts.loc[j, 'Date'] = end1_trj_time.date()\n",
    "                tobeadded_staypts.loc[j, 'Time'] = end1_trj_time.time()\n",
    "                tobeadded_staypts.loc[j, 'Hour'] = end1_trj_time.hour\n",
    "                tobeadded_staypts.loc[j, 'Weekday'] = str(end1_trj_time.weekday())+ end1_trj_time.weekday_name\n",
    "                tobeadded_staypts.loc[j, 'StayPoint'] = 1\n",
    "                tobeadded_staypts.loc[j, 'StayptId'] = staypts_df.loc[end_indx, 'StayptId']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLat'] = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLon'] = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "                tobeadded_staypts.loc[j, 'State'] = -2   #to indicate its po´rocessed\n",
    "                tobeadded_staypts.loc[j, 'StateId'] = -1\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLat'] = -1.0\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLon'] = -1.0\n",
    "                j = j + 1\n",
    "\n",
    "                #add start of current point line\n",
    "                tobeadded_staypts.loc[j, 'Latitude'] = staypts_df.loc[strt_indx, 'Latitude']\n",
    "                tobeadded_staypts.loc[j, 'Longitude'] = staypts_df.loc[strt_indx, 'Longitude']\n",
    "                tobeadded_staypts.loc[j, 'Timestamp'] = str2_trj_time\n",
    "                tobeadded_staypts.loc[j, 'Date'] = str2_trj_time.date()\n",
    "                tobeadded_staypts.loc[j, 'Time'] = str2_trj_time.time()\n",
    "                tobeadded_staypts.loc[j, 'Hour'] = str2_trj_time.hour\n",
    "                tobeadded_staypts.loc[j, 'Weekday'] = str(str2_trj_time.weekday()) + str2_trj_time.weekday_name\n",
    "                tobeadded_staypts.loc[j, 'StayPoint'] = 1\n",
    "                tobeadded_staypts.loc[j, 'StayptId'] = staypts_df.loc[strt_indx, 'StayptId']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLat'] = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "                tobeadded_staypts.loc[j, 'StayMeanLon'] = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "                tobeadded_staypts.loc[j, 'State'] = -2 #to indicate its po´rocessed\n",
    "                tobeadded_staypts.loc[j, 'StateId'] = -1\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLat'] = -1.0\n",
    "                tobeadded_staypts.loc[j, 'StateMeanLon'] = -1.0\n",
    "                j = j + 1\n",
    "    \n",
    "    staypts_df = staypts_df.append(tobeadded_staypts, ignore_index=True)\n",
    "    staypts_df = staypts_df.sort_values(['StayptId', 'Timestamp'])\n",
    "    staypts_df = staypts_df.reset_index(drop=True)            \n",
    "    \n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "\n",
    "#-------------form states-----------------------------------------------------------------------\n",
    "def form_states():\n",
    "    global staypts_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    #update states in final staypoints\n",
    "    #copy staypoint data as state data\n",
    "    staypts_df['StateId'] = staypts_df['StayptId']\n",
    "    staypts_df['StateMeanLat'] = staypts_df['StayMeanLat']\n",
    "    staypts_df['StateMeanLon'] = staypts_df['StayMeanLon']\n",
    "    \n",
    "    #this fucntion groups the staypoints together to from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    staypts_df1 = staypts_df1.reset_index(drop=True)\n",
    "    \n",
    "    row = 1\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for i in range(0, len(staypts_df1)-1):\n",
    "        for j in range(i+1, len(staypts_df1)):\n",
    "        \n",
    "            chk_cluster = staypts_df1['StateId'][i]\n",
    "            chk_clulat = staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['StateId'][j]\n",
    "            curr_clulat = staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['StateMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= state_d_thrhld:\n",
    "                #before adding this point to the ith state, \n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "                \n",
    "                add_state = \"Yes\"\n",
    "                #form the existing lat and lon array\n",
    "                array_lat = staypts_df['Latitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = staypts_df['Longitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                #add the new lat and lon values to the array\n",
    "                new_lats = staypts_df['Latitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = staypts_df['Longitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "                \n",
    "                array_lat= np.append(array_lat, new_lats)\n",
    "                array_lon= np.append(array_lon, new_lons)\n",
    "                #cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "                \n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "                        \n",
    "                if add_state == \"Yes\":    \n",
    "                    staypts_df.loc[ (staypts_df['StateId']==curr_cluster), 'StateId'] = chk_cluster\n",
    "                    staypts_df.loc[ (staypts_df['StateId']==chk_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    staypts_df.loc[ (staypts_df['StateId']==chk_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "                \n",
    "    #update states for last hour staypoints\n",
    "    #copy staypoint data as state data\n",
    "    #copy staypoint data as state data\n",
    "    curr_hr_staypts_df['StateId'] = curr_hr_staypts_df['StayptId']\n",
    "    curr_hr_staypts_df['StateMeanLat'] = curr_hr_staypts_df['StayMeanLat']\n",
    "    curr_hr_staypts_df['StateMeanLon'] = curr_hr_staypts_df['StayMeanLon']\n",
    "    \n",
    "    #this fucntion groups the staypoints together to from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.sort_values(['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.reset_index(drop=True)\n",
    "    \n",
    "    row = 1\n",
    "\n",
    "    for i in range(0, len(curr_hr_staypts_df1)):\n",
    "        for j in range(i+1, len(curr_hr_staypts_df1)):\n",
    "        \n",
    "            chk_cluster = curr_hr_staypts_df1['StateId'][i]\n",
    "            chk_clulat = curr_hr_staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = curr_hr_staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = curr_hr_staypts_df1['StateId'][j]\n",
    "            curr_clulat = curr_hr_staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = curr_hr_staypts_df1['StateMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= state_d_thrhld:\n",
    "                #before adding this point to the ith state, \n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "                \n",
    "                add_state = \"Yes\"\n",
    "                #form the existing lat and lon array\n",
    "                array_lat = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                #add the new lat and lon values to the array\n",
    "                new_lats = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "                \n",
    "                array_lat= np.append(array_lat, new_lats)\n",
    "                array_lon= np.append(array_lon, new_lons)\n",
    "                #cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "                \n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "                        \n",
    "                if add_state == \"Yes\":    \n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateId'] = chk_cluster\n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "def cal_hourly_state_weight():\n",
    "    global cluster_hourly_df  \n",
    "    global staypts_df\n",
    "\n",
    "    curr_hr_cluster_hourly_df = pd.DataFrame()       \n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(drop=True)\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['StateId'][0]\n",
    "    curr_count = 0\n",
    "    row = 0\n",
    "    currstate_timestamps = []\n",
    "    currstate_timestamps =  np.append(currstate_timestamps, staypts_df.loc[0, 'Timestamp'])\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        curr_hr_cluster_hourly_df['Date'] = 0\n",
    "        curr_hr_cluster_hourly_df['StateId'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLat'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLon'] = 0\n",
    "        curr_hr_cluster_hourly_df[i] = 0\n",
    "\n",
    "    for i in range(1, len(staypts_df)):\n",
    "\n",
    "        if (staypts_df['StateId'][i] != last_clusid):\n",
    "            start = min(currstate_timestamps)\n",
    "            end = max(currstate_timestamps)\n",
    "\n",
    "            #if the state is with one hour\n",
    "            if start.hour == end.hour:\n",
    "                k = end - start\n",
    "                mins = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                date_read = start.date()\n",
    "                cluster_id = staypts_df['StateId'][i-1]\n",
    "                cluster_mean_lat = staypts_df['StateMeanLat'][i-1]\n",
    "                cluster_mean_lon = staypts_df['StateMeanLon'][i-1]\n",
    "                col_name = start.hour\n",
    "\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'AvgLat'] = cluster_mean_lat\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'AvgLon'] = cluster_mean_lon\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'Date'] = date_read\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'StateId'] = cluster_id\n",
    "                curr_hr_cluster_hourly_df.loc[row, col_name] = round((mins)/60,4)\n",
    "                row = row + 1\n",
    "\n",
    "            #if the state is beyond one hour boundary\n",
    "            else:\n",
    "                end = end + pd.Timedelta(hours=1) - pd.Timedelta(minutes=end.minute)\n",
    "                j = start\n",
    "                while j < end:\n",
    "                    if j == start:\n",
    "                        k = ((j + pd.Timedelta(hours=1) - pd.Timedelta(minutes=j.minute)) - j)\n",
    "                        mins = int((k / np.timedelta64(1, 'm')))\n",
    "                    elif j.hour == end.hour - 1:\n",
    "                        end_time = max(currstate_timestamps)\n",
    "                        k = (end_time - (end_time - pd.Timedelta(minutes=end_time.minute)))\n",
    "                        mins = int((k / np.timedelta64(1, 'm')))\n",
    "                    else:\n",
    "                        mins = 60\n",
    "\n",
    "                    date_read = j.date()\n",
    "                    cluster_id = staypts_df['StateId'][i-1]\n",
    "                    cluster_mean_lat = staypts_df['StateMeanLat'][i-1]\n",
    "                    cluster_mean_lon = staypts_df['StateMeanLon'][i-1]\n",
    "                    col_name = j.hour\n",
    "\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'AvgLat'] = cluster_mean_lat\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'AvgLon'] = cluster_mean_lon\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'Date'] = date_read\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'StateId'] = cluster_id\n",
    "                    curr_hr_cluster_hourly_df.loc[row, col_name] = round((mins)/60,4)\n",
    "                    row = row + 1\n",
    "\n",
    "                    j = j + pd.Timedelta(hours=1)\n",
    "\n",
    "            currstate_timestamps = []\n",
    "            currstate_timestamps =  np.append(currstate_timestamps, staypts_df.loc[i, 'Timestamp'])\n",
    "            last_clusid = staypts_df['StateId'][i]\n",
    "        else:\n",
    "            currstate_timestamps =  np.append(currstate_timestamps, staypts_df.loc[i, 'Timestamp'])\n",
    "\n",
    "\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.fillna(0)\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.groupby(['Date', 'StateId', 'AvgLat', 'AvgLon']).sum()\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(level=[0,1,2,3])\n",
    "   \n",
    "    cluster_hourly_df = curr_hr_cluster_hourly_df\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(drop=True)\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------\n",
    "def del_staypts_less_dur(prev_date):\n",
    "    global staypts_df\n",
    "    \n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "    idx = staypts_df.index[staypts_df['Date'] == prev_date]\n",
    "    if idx.empty:\n",
    "         return\n",
    "    start = min(idx) + 1\n",
    "\n",
    "    prev_stateid = staypts_df.loc[start-1, 'StateId'] \n",
    "    state_cur_count = 1\n",
    "    for i in range(start, len(staypts_df)):\n",
    "        if (prev_stateid != staypts_df.loc[i, 'StateId'] ):\n",
    "\n",
    "            duration = staypts_df.loc[i-1, 'Timestamp'] - staypts_df.loc[i-state_cur_count, 'Timestamp']\n",
    "            seconds = duration.seconds\n",
    "            minutes = seconds / 60\n",
    "\n",
    "            if minutes < staypts_t_thrhld:\n",
    "                for j in range(i-state_cur_count, i):\n",
    "                    staypts_df.loc[j, 'State'] = -3\n",
    "\n",
    "            state_cur_count = 1\n",
    "            prev_stateid = staypts_df.loc[i, 'StateId']\n",
    "        else:\n",
    "            state_cur_count = state_cur_count + 1\n",
    "\n",
    "    staypts_df = staypts_df[staypts_df.State != -3]\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------\n",
    "def visualize_hourly_state_weight():\n",
    "    global staypts_df\n",
    "    \n",
    "    #create a color dictionary for each cluster for the plot\n",
    "    dicts = {}\n",
    "    clu_list = []\n",
    "    clu_list = staypts_df['StateId'].unique()\n",
    "    r = lambda: random.randint(0,255)\n",
    "    #olors = sns.color_palette(\"Paired\", len(clu_list))\n",
    "\n",
    "    for i in range(0, len(clu_list)):\n",
    "        #icts[clu_list[i]] = (colors[i])\n",
    "        dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(),r(),r()))\n",
    "\n",
    "    #create a new graph where we will later add rectangles for each hour:cluster\n",
    "    fig2 = plt.figure(figsize=(15,15))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "\n",
    "    #get all the dates for y axis\n",
    "    date_list = staypts_df['Timestamp'].dt.date.unique()\n",
    "    date_range = list(range(min(date_list).day, max(date_list).day+ 1))\n",
    "    y = range(0, len(date_range))\n",
    "    def_yticks = date_range\n",
    "    plt.yticks(y, def_yticks)\n",
    "\n",
    "    #set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_range))\n",
    "\n",
    "    date_counter = 0\n",
    "    last_date = staypts_df['Timestamp'][0].date()\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['StateId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "\n",
    "    #drawing verical lines for each hour\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x= i, linewidth=1, color='r')\n",
    "    #horizontal lines\n",
    "    for i in range(min(date_list).day, max(date_list).day):\n",
    "        ax1.axhline(y= i, linewidth=1, color='r')\n",
    "\n",
    "    for i in range(0, len(staypts_df)):\n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        if (i == len(staypts_df)-1):\n",
    "            a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "            b = staypts_df['Timestamp'][i].hour + staypts_df['Timestamp'][i].minute/60\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(staypts_df['StateId'][i])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['StateId'][i]))\n",
    "\n",
    "        #plot a rectangle if stateid change\n",
    "        if (staypts_df['StateId'][i] != last_clusid):\n",
    "            if staypts_df['Timestamp'][i-curr_count].date()!= staypts_df['Timestamp'][i-1].date():\n",
    "                #first day\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "                b = 24\n",
    "                width = b - a\n",
    "                height = 1\n",
    "                col_id = dicts.get(staypts_df['StateId'][i-1])\n",
    "                ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['StateId'][i-1]))\n",
    "                ax1.annotate(staypts_df['StateId'][i-1], (a + width/2, height/2 + date_counter), \n",
    "                             color='w', weight='bold', fontsize=10, ha='center', va='center')\n",
    "\n",
    "                #second day\n",
    "                a = 0\n",
    "                b = staypts_df['Timestamp'][i-1].hour + staypts_df['Timestamp'][i-1].minute/60\n",
    "                width = b - a\n",
    "                height = 1\n",
    "                col_id = dicts.get(staypts_df['StateId'][i-1])\n",
    "                ax1.add_patch(patches.Rectangle((a, date_counter+1), width, height, color=col_id, label=staypts_df['StateId'][i-1]))\n",
    "                ax1.annotate(staypts_df['StateId'][i-1], (a + width/2, height/2 + date_counter+1), \n",
    "                             color='w', weight='bold', fontsize=10, ha='center', va='center')\n",
    "\n",
    "            else:    \n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "                b = staypts_df['Timestamp'][i-1].hour + staypts_df['Timestamp'][i-1].minute/60\n",
    "                width = b - a\n",
    "                height = 1\n",
    "                col_id = dicts.get(staypts_df['StateId'][i-1])\n",
    "                ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['StateId'][i-1]))\n",
    "                ax1.annotate(staypts_df['StateId'][i-1], (a + width/2, height/2 + date_counter), \n",
    "                             color='w', weight='bold', fontsize=10, ha='center', va='center')\n",
    "\n",
    "            curr_count = 1\n",
    "\n",
    "            last_clusid = staypts_df['StateId'][i]\n",
    "\n",
    "            if (last_date != staypts_df['Timestamp'][i].date()):\n",
    "                add_days = (staypts_df['Timestamp'][i].date() - last_date).days\n",
    "                date_counter = date_counter + add_days\n",
    "                last_date = staypts_df['Timestamp'][i].date()\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "    destpng = usr_directory + \"/online.png\"\n",
    "    plt.savefig(destpng)\n",
    "    plt.show()\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_staypts_csv():\n",
    "    staypts_df.to_csv(dest_file_staypoints, sep='\\t', encoding='utf-8')\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_hourly_weights_csv():\n",
    "    cluster_hourly_df.to_csv(dest_file_hourly_weights,  sep='\\t', encoding='utf-8')\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_seperate_trasition_matrices():\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    \n",
    "    #create a temp dataframe for each data, and calculate trasition matrices from hour t to t+1\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "\n",
    "        #create a temp dataframe for pervious date\n",
    "        temp_df = pd.DataFrame()\n",
    "        matrices_df = pd.DataFrame()\n",
    "        temp_df = cluster_hourly_df.loc[cluster_hourly_df['Date'] == date_list[p]]\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0, 24):\n",
    "            matrices_df['Date'] = 0\n",
    "            matrices_df['StateId'] = 0\n",
    "            for j in range(0, len(temp_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['StateId'][j])\n",
    "                matrices_df[colname] = 0\n",
    "\n",
    "        matrices_df['Date'] = temp_df['Date']\n",
    "        matrices_df['StateId'] = temp_df['StateId']\n",
    "\n",
    "        for i in range (0, 23):\n",
    "            for j in range (0, len(temp_df)):\n",
    "                for k in range (0, len(temp_df)):\n",
    "                    prob = temp_df[i][j] * temp_df[i+1][k]\n",
    "                    colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['StateId'][k])\n",
    "                    matrices_df[colname][j] = prob\n",
    "        file_name = dest_path_each_day_trsn_mat + str(date_list[p]) + \".csv\"\n",
    "        matrices_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_markov_chains():\n",
    "    global final_transition_df\n",
    "    global co_loc\n",
    "    \n",
    "    final_transition_df = pd.DataFrame()\n",
    "\n",
    "    #create an empty markov chain frame for each state, and transition for each hour of the day\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    cluster_list = cluster_hourly_df['StateId'].unique()\n",
    "    AvgLat_list = cluster_hourly_df['AvgLat'].unique()\n",
    "    AvgLon_list = cluster_hourly_df['AvgLon'].unique()\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        final_transition_df['Address'] = 0\n",
    "        final_transition_df['AvgLat'] = 0\n",
    "        final_transition_df['AvgLon'] = 0\n",
    "        final_transition_df['StateId'] = 0\n",
    "        for j in range(0, cluster_hourly_df['StateId'].nunique()):\n",
    "            colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(cluster_list[j])\n",
    "            final_transition_df[colname] = 0\n",
    "\n",
    "    final_transition_df['StateId'] = cluster_list\n",
    "    final_transition_df['AvgLat'] = AvgLat_list\n",
    "    final_transition_df['AvgLon'] = AvgLon_list\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df.index = final_transition_df.StateId\n",
    "\n",
    "    #read each day file and sum the matching rows:cols combinations\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    path_dir = dest_path_each_day_trsn_mat\n",
    "\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        temp_df = pd.DataFrame()\n",
    "        filename = path_dir + str(date_list[p]) + '.csv'\n",
    "        temp_df =  pd.read_csv(filename, header = 0, sep='\\t')\n",
    "\n",
    "        for i in range(0, len(temp_df)):\n",
    "            rowname = temp_df['StateId'][i]\n",
    "            for src_column in temp_df:\n",
    "                for dest_column in final_transition_df:\n",
    "                    if src_column == dest_column and src_column != 'StateId' :\n",
    "                        #import pdb; pdb.set_trace()\n",
    "                        final_transition_df[dest_column][rowname] = (final_transition_df[dest_column][rowname] +\n",
    "                                                                    temp_df[src_column][i])\n",
    "\n",
    "    #replace zero to a small value \n",
    "    final_transition_df = final_transition_df.fillna(0)                    \n",
    "    final_transition_df = final_transition_df.replace(0, 0.00001)\n",
    "\n",
    "    #calculate probability from cluster x to cluster y from time t to t+1\n",
    "    final_transition_df = final_transition_df.reset_index(drop=True)\n",
    "    for clus in range(0, len(final_transition_df)):\n",
    "        for i in range(0, 24):\n",
    "            temp_sum = 0\n",
    "            for j in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['StateId'][j])\n",
    "                temp_sum += (final_transition_df[colname][clus])\n",
    "            for k in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['StateId'][k])\n",
    "                if temp_sum != 0:\n",
    "                    final_transition_df[colname][clus] = final_transition_df[colname][clus]/temp_sum\n",
    "\n",
    "    #create dictionary for coordinate : address\n",
    "    points = tuple(zip(final_transition_df.AvgLat, final_transition_df.AvgLon))\n",
    "    geocoder = Nominatim(timeout=10)\n",
    "    coordinate_location = {}\n",
    "\n",
    "    for coordinate in points:\n",
    "        try:\n",
    "            location = geocoder.reverse(coordinate)\n",
    "        except:\n",
    "            location = 'unknown'\n",
    "        coordinate_location[coordinate] = location\n",
    "\n",
    "    co_loc = {k:v for k,v in coordinate_location.items()}\n",
    "\n",
    "    for i in range(0, len(final_transition_df)):\n",
    "        address = co_loc.get((final_transition_df['AvgLat'][i], final_transition_df['AvgLon'][i]))\n",
    "        if address == 'unknown':\n",
    "            final_transition_df['Address'][i] = 'unknown'\n",
    "        else:\n",
    "            final_transition_df['Address'][i] = address[0]\n",
    "\n",
    "    #save the file\n",
    "    final_transition_df.to_csv(dest_file_final_markov_chain)\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        final_transition_temp_df = pd.DataFrame()\n",
    "        k = cluster_hourly_df['StateId'].nunique()*i + 4\n",
    "        final_transition_temp_df = final_transition_df.iloc[:,k:k + cluster_hourly_df['StateId'].nunique()]\n",
    "        final_transition_temp_df.index = cluster_list\n",
    "        file_name = usr_markov_chains_directory + \"/\" + str(i) + \" hour.csv\"\n",
    "        final_transition_temp_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def predict(prev_hour):\n",
    "    global trained_model_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    tobepredicted_df = curr_hr_staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon', 'Timestamp']]\n",
    "    tobepredicted_df = tobepredicted_df.drop_duplicates()\n",
    "    tobepredicted_df = tobepredicted_df.reset_index(drop=True)\n",
    "\n",
    "    for j in range(0, len(tobepredicted_df)):\n",
    "\n",
    "        new_lat = tobepredicted_df['StateMeanLat'][j]\n",
    "        new_lon = tobepredicted_df['StateMeanLon'][j]\n",
    "        #file_name = \"PredTime- \" +  str(tobepredicted_df['Timestamp'][j]) + \".csv\"\n",
    "        file_name = \"Pred\" + \".csv\"\n",
    "        for i in range(0, len(trained_model_df)):\n",
    "\n",
    "            trn_lat = trained_model_df['AvgLat'][i]\n",
    "            trn_lon = trained_model_df['AvgLon'][i]\n",
    "            if meters(trn_lat, trn_lon, new_lat, new_lon) <= state_d_thrhld:\n",
    "\n",
    "                predic_df = pd.DataFrame()\n",
    "\n",
    "                cluster_id = trained_model_df['StateId'][i]\n",
    "                curr_lat = trained_model_df['AvgLat'][i]\n",
    "                curr_lon = trained_model_df['AvgLon'][i]\n",
    "                curr_add = trained_model_df['Address'][i]\n",
    "                pred_loc = {\"current\":(cluster_id, curr_lat, curr_lon, curr_add)}\n",
    "\n",
    "                from_col_no = trained_model_df['StateId'].nunique() * prev_hour + 5\n",
    "                to_col_no = from_col_no + trained_model_df['StateId'].nunique()\n",
    "                predic_df = trained_model_df.iloc[i:i+1,from_col_no:to_col_no]\n",
    "                predic_df = predic_df.T\n",
    "                predic_df['StateId'] = cluster_id\n",
    "                predic_df['PredState'] = predic_df.index\n",
    "                predic_df['PredState'] = predic_df['PredState'].map(lambda x: x.split('-', 2)[-1])\n",
    "                predic_df.columns = ['Probability', 'StateId', 'PredState']\n",
    "                predic_df = predic_df.sort_values('Probability', ascending=False).head(5)\n",
    "                predic_df['Address'] = 0\n",
    "                predic_df['Latitude'] = 0.0\n",
    "                predic_df['Longitude'] = 0.0\n",
    "                predic_df = predic_df.reset_index(drop=True)\n",
    "\n",
    "                for j in range (0, len(predic_df)):\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    clus_to_find = int(float(predic_df['PredState'][j]))\n",
    "                    add = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'Address'].values[0]\n",
    "                    lat = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'AvgLat'].values[0]\n",
    "                    lon = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'AvgLon'].values[0]\n",
    "\n",
    "                    predic_df.loc[j, 'Address'] = add\n",
    "                    predic_df.loc[j, 'Latitude'] = lat\n",
    "                    predic_df.loc[j, 'Longitude'] = lon\n",
    "                file = dest_predicted_dir + file_name\n",
    "                print(\"Prediction --\\n\")\n",
    "                print(\"Current hour - \" + str(prev_hour))\n",
    "                print(\"\\nPrediction\\n\")\n",
    "                print(predic_df)\n",
    "                predic_df.to_csv(file, sep='\\t', encoding='utf-8')\n",
    "                break\n",
    "            \n",
    "#------------------------------------------ S T A R T -----------------------------------------------\n",
    "def main():\n",
    "    global usr_trejec_df\n",
    "    global trained_model_df\n",
    "    global curr_hr_df\n",
    "    global curr_hr_staypts_df\n",
    "    global staypts_df\n",
    "    global cluster_hourly_df\n",
    "    global final_transition_df\n",
    "    \n",
    "    #read test user trajectory file. In real scenerio, this will be the GPS read data\n",
    "    read_usr_file()\n",
    "\n",
    "    #prepere dataframes\n",
    "    prepare_dfs()\n",
    "\n",
    "    #Save first date and time as prev date and time for the start\n",
    "    prev_date = usr_trejec_df['Date'][0]\n",
    "    prev_hour = usr_trejec_df['Hour'][0]\n",
    "\n",
    "    #I. Read the new locations in an online gps location input mode\n",
    "    #  1. Everytime the hour changes, \n",
    "    #                  A. Find staypoints for the last hour and assign staypointID\n",
    "    #                  B. Cluster staypoints based on distance for last hour, form states and assign stateID\n",
    "    #                  C. Calculate state hourly weights for last hour\n",
    "    #                  D. Predict based on trained data(if available)\n",
    "    #  2. Everytime the date changes,\n",
    "    #                  A. Add the days data into the training data\n",
    "    #  3. If the hour and the time has not been changed, add the data to current hour data\n",
    "\n",
    "    #I\n",
    "    for i in range(0, len(usr_trejec_df)):\n",
    "\n",
    "        #store the read hour and date as new hour and new date\n",
    "        new_hour = usr_trejec_df['Hour'][i]\n",
    "        new_date = usr_trejec_df['Date'][i]\n",
    "\n",
    "        #1. \n",
    "        #if the hour has changed\n",
    "        if (new_hour != prev_hour): \n",
    "            #process the last hour data if available\n",
    "            if not curr_hr_df.empty:\n",
    "                #A.\n",
    "          \n",
    "                create_last_hr_staypts() \n",
    "                add_start_end_times()\n",
    "                if not curr_hr_staypts_df.empty:\n",
    "                     #B.\n",
    "                    form_states()\n",
    "                    #read_trained_model()\n",
    "                    #if not trained_model_df.empty:\n",
    "                    #    predict(prev_hour)\n",
    "            \n",
    "            prev_hour = new_hour \n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])  \n",
    "         #2. \n",
    "         #if the date has changed\n",
    "            if (new_date != prev_date):\n",
    "                if not staypts_df.empty:\n",
    "                    #del_staypts_less_dur(prev_date)\n",
    "                    #visualize_hourly_state_weight()\n",
    "                    cal_hourly_state_weight()\n",
    "                    update_staypts_csv()\n",
    "                    update_hourly_weights_csv()\n",
    "                    create_save_seperate_trasition_matrices()\n",
    "                    create_save_markov_chains()\n",
    "                prev_date = new_date           \n",
    "\n",
    "        #3. \n",
    "        #if the date and the hour has not changed, just add it to current hour dataframe.\n",
    "        # this dataframe is used once the hour is changed.\n",
    "        else:\n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])   \n",
    "            \n",
    "    visualize_hourly_state_weight()\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE INPUTS HERE-------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Edit user name, and path locations for source and destination files\n",
    "user = \"022\"\n",
    "month = \"200908\"\n",
    "state_d_thrhld = 200\n",
    "staypts_d_thrhld = 200\n",
    "staypts_t_thrhld = 20\n",
    "track_t_thrhld = 30\n",
    "\n",
    "#source paths\n",
    "file_src = \"C:/Users/12sha/Documents/Geolife Trajectories 1.3/Data/\" \n",
    "#destination paths\n",
    "base_path = r\"C:\\Users\\12sha\\Documents\\thesislocation\\code_\\stay points\\v0.6 results\"\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE INPUTS HERE-------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "file_source_raw = file_src + user + \"/Trajectory/\" + month + \"*.plt\"\n",
    "usr_directory = base_path + \"/User \" + user + \"/\" + month\n",
    "usr_hrly_wght_directory = base_path + \"/User \" + user  + \"/\" + month + \"/hourlyweights\"\n",
    "usr_sty_pts_directory = base_path + \"/User \" + user + \"/\" + month + \"/staypoints\"\n",
    "usr_markov_chains_directory = base_path + \"/User \" + user + \"/\" + month + \"/markovchains\"\n",
    "dest_predicted_dir = base_path + \"/User \" + user + \"/\" + month + \"/predict/\"\n",
    "\n",
    "if not os.path.exists(usr_directory):\n",
    "    os.makedirs(usr_directory)\n",
    "if not os.path.exists(usr_hrly_wght_directory):\n",
    "    os.makedirs(usr_hrly_wght_directory)\n",
    "if not os.path.exists(usr_sty_pts_directory):\n",
    "    os.makedirs(usr_sty_pts_directory)  \n",
    "if not os.path.exists(usr_markov_chains_directory):\n",
    "    os.makedirs(usr_markov_chains_directory)  \n",
    "if not os.path.exists(dest_predicted_dir):\n",
    "    os.makedirs(dest_predicted_dir)  \n",
    "\n",
    "#destination file names\n",
    "dest_file_staypoints = usr_sty_pts_directory + \"/staypoints.csv\"\n",
    "dest_file_hourly_weights = usr_hrly_wght_directory + \"/hourlyweights.csv\"\n",
    "dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "dest_file_final_markov_chain = usr_markov_chains_directory + \"/final.csv\"\n",
    "\n",
    "#remove if the file already exists\n",
    "\n",
    "try:\n",
    "    os.remove(dest_file_staypoints)   \n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(dest_file_hourly_weights)\n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(dest_file_final_markov_chain)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "#global dataframes used\n",
    "#user raw trajectory dataframe\n",
    "usr_trejec_df = pd.DataFrame()\n",
    "#user trained model\n",
    "trained_model_df = pd.DataFrame()\n",
    "#current hour points\n",
    "curr_hr_df = pd.DataFrame()\n",
    "#current hour staypoints\n",
    "curr_hr_staypts_df = pd.DataFrame()\n",
    "#last hour last point\n",
    "prev_hour_last_point = pd.DataFrame()\n",
    "#all staypoints\n",
    "staypts_df = pd.DataFrame()\n",
    "#hourly cluster\n",
    "cluster_hourly_df = pd.DataFrame()\n",
    "#final markov chains\n",
    "final_transition_df = pd.DataFrame()\n",
    "\n",
    "clus_dict = {}\n",
    "co_loc = {}\n",
    "pred_loc = {}\n",
    "lat_array = []\n",
    "lon_array = []\n",
    "global_count = 0\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
