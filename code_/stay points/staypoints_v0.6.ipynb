{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize staypoints offline\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "import operator\n",
    "import pdb\n",
    "from geopy.geocoders import Nominatim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "def directory_setup():\n",
    "    #create directories for output\n",
    "    usr_directory = dest_path + \"/User \" + user\n",
    "\n",
    "    if not os.path.exists(usr_directory):\n",
    "        os.makedirs(usr_directory)\n",
    "\n",
    "    #remove if the file already exists\n",
    "    try:\n",
    "        os.remove(usr_offline_staypoints_org)\n",
    "        os.remove(usr_offline_staypoints_at)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_usr_file():\n",
    "    global usr_trejec_df\n",
    "    \n",
    "    #Load file names for user\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    usr_trejec_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    usr_trejec_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Date\"].map(str) + \" \" + usr_trejec_df[\"Time\"]\n",
    "    \n",
    "    usr_trejec_df.Timestamp = pd.to_datetime(usr_trejec_df.Timestamp)\n",
    "    \n",
    "    usr_trejec_df.index = usr_trejec_df['Timestamp']\n",
    "    usr_trejec_df = usr_trejec_df.resample('1T').mean()\n",
    "    usr_trejec_df = usr_trejec_df.dropna()\n",
    "    \n",
    "     #add columns to user trajectory dataframe\n",
    "    #1. add timestamp as column\n",
    "    usr_trejec_df['Timestamp'] = pd.to_datetime(usr_trejec_df.index)\n",
    "    #restore date and time column\n",
    "    usr_trejec_df['Date'] = usr_trejec_df.Timestamp.dt.date\n",
    "    usr_trejec_df['Time'] = usr_trejec_df.Timestamp.dt.time\n",
    "\n",
    "    #sort values based on timestamp\n",
    "    usr_trejec_df = usr_trejec_df.sort_values(['Timestamp'])\n",
    "    #reset index\n",
    "    usr_trejec_df = usr_trejec_df.reset_index(drop=True)\n",
    "    \n",
    "    #some test columns\n",
    "    #distance between consicutive points to for further checks\n",
    "    usr_trejec_df['Distance(Km)'] = 0\n",
    "    for i in range(0, len(usr_trejec_df)-1):\n",
    "        usr_trejec_df.loc[i+1, 'Distance(Km)'] = (meters(usr_trejec_df.loc[i, 'Latitude'],\n",
    "                                                    usr_trejec_df.loc[i, 'Longitude'],\n",
    "                                                   usr_trejec_df.loc[i+1, 'Latitude'],\n",
    "                                                   usr_trejec_df.loc[i+1, 'Longitude'])) / 1000\n",
    "    #time difference between two points\n",
    "    usr_trejec_df['Time(Hr)'] = 0\n",
    "    for i in range(0, len(usr_trejec_df)-1):\n",
    "        usr_trejec_df.loc[i+1, 'Time(Hr)'] = (usr_trejec_df.loc[i+1, 'Timestamp'] -\n",
    "                                              usr_trejec_df.loc[i, 'Timestamp']).seconds/3600\n",
    "    #speed                                    \n",
    "    usr_trejec_df['Speed(Km/Hr)'] = usr_trejec_df['Distance(Km)'] / usr_trejec_df['Time(Hr)']\n",
    "    \n",
    "    usr_trejec_df['Hour'] = usr_trejec_df.Timestamp.dt.hour\n",
    "    \n",
    "    #distance clusters\n",
    "    usr_trejec_df['DistClus'] = 0\n",
    "    usr_trejec_df['ClusDur'] = 0\n",
    "    last_hr =  usr_trejec_df['Hour'][0]\n",
    "    clusid = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(usr_trejec_df)-1:\n",
    "        clusid+=1\n",
    "        \n",
    "        #update the cluster duration\n",
    "        if i != 0:\n",
    "            usr_trejec_df.loc[i-1, 'ClusDur']= (usr_trejec_df.loc[i-1, 'Timestamp'] - \n",
    "                                            usr_trejec_df.loc[i-curr_clus_count, 'Timestamp']).seconds/60\n",
    "        curr_clus_count = 1\n",
    "        \n",
    "        #add current cluster to mean values\n",
    "        array_lat = usr_trejec_df.loc[i,'Latitude']\n",
    "        array_lon = usr_trejec_df.loc[i,'Longitude']\n",
    "        new_lat_mean = np.mean(array_lat)\n",
    "        new_lon_mean = np.mean(array_lon)\n",
    "        \n",
    "        usr_trejec_df.loc[i, 'DistClus'] = clusid\n",
    "        for j in range(i+1, len(usr_trejec_df)):\n",
    "            #if the hour changes, stop j loop\n",
    "            if usr_trejec_df.loc[j, 'Hour'] != last_hr:\n",
    "                last_hr =  usr_trejec_df.loc[j, 'Hour']\n",
    "                i=j\n",
    "                break\n",
    "            else:\n",
    "                if meters(new_lat_mean, \n",
    "                           new_lon_mean, \n",
    "                           usr_trejec_df.loc[j,'Latitude'], \n",
    "                           usr_trejec_df.loc[j,'Longitude'])<= staypts_d_thrhld:\n",
    "                        curr_clus_count+=1\n",
    "                        \n",
    "                        array_lat= np.append(array_lat, usr_trejec_df.loc[j,'Latitude'])\n",
    "                        array_lon= np.append(array_lon, usr_trejec_df.loc[j,'Longitude'])\n",
    "                        new_lat_mean = np.mean(array_lat)\n",
    "                        new_lon_mean = np.mean(array_lon)\n",
    "        \n",
    "                        usr_trejec_df.loc[j, 'DistClus'] = usr_trejec_df.loc[i, 'DistClus']\n",
    "                        \n",
    "                else:\n",
    "                    i=j\n",
    "                    break\n",
    "        i=j\n",
    "        \n",
    "\n",
    "    usr_trejec_df['Weekday'] = usr_trejec_df['Timestamp'].dt.weekday.map(str) + usr_trejec_df['Timestamp'].dt.weekday_name\n",
    "\n",
    "    usr_trejec_df['StayPoint'] = -1 # 1 if it is a staypoint, else 0\n",
    "    usr_trejec_df['StayptId'] = -1\n",
    "    usr_trejec_df['StayMeanLat'] = -1.0\n",
    "    usr_trejec_df['StayMeanLon'] = -1.0\n",
    "    usr_trejec_df['State'] = -1     # 1 if it is a state, else 0\n",
    "    usr_trejec_df['StateId'] = -1\n",
    "    usr_trejec_df['StateMeanLat'] = -1.0\n",
    "    usr_trejec_df['StateMeanLon'] = -1.0\n",
    "    \n",
    "    #remove columns not used/required\n",
    "    usr_trejec_df = usr_trejec_df.drop(['0', 'Altitude', 'NumDays'], axis = 1)\n",
    "    \n",
    "    file_name = dest_path + \"/User \" + user + \"/input_trj_data.csv\"\n",
    "    usr_trejec_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "def offline_staypoints():\n",
    "    global off_staypts\n",
    "    \n",
    "    curr_cluster = usr_trejec_df.loc[0, 'DistClus']\n",
    "    count_clus = 1\n",
    "    lat_arr = []\n",
    "    lon_arr = []\n",
    "    row_dest = 0\n",
    "    \n",
    "    for i in range(1, len(usr_trejec_df)):\n",
    "        \n",
    "        if usr_trejec_df.loc[i, 'DistClus'] == curr_cluster:\n",
    "            count_clus = count_clus + 1\n",
    "        else:\n",
    "            if usr_trejec_df.loc[i-1, 'ClusDur'] >= 20:\n",
    "                \n",
    "                end = usr_trejec_df.loc[i-1, 'Timestamp'] \n",
    "                start = usr_trejec_df.loc[i-count_clus, 'Timestamp']\n",
    "                id_n = usr_trejec_df.loc[i-1, 'DistClus'] \n",
    "                for j in range(i-count_clus-1, i-1):\n",
    "                    lat_arr = np.append(lat_arr, usr_trejec_df.loc[j, 'Latitude'])\n",
    "                    lon_arr = np.append(lon_arr, usr_trejec_df.loc[j, 'Longitude'])\n",
    "                \n",
    "                lat_mean = np.mean(lat_arr)\n",
    "                lon_mean = np.mean(lon_arr)\n",
    "               \n",
    "                off_staypts.loc[row_dest, 'id'] = id_n\n",
    "                off_staypts['groupid'] = 0\n",
    "                off_staypts.loc[row_dest, 'start'] = start\n",
    "                off_staypts.loc[row_dest, 'end'] = end\n",
    "                off_staypts.loc[row_dest, 'meanlat'] = lat_mean\n",
    "                off_staypts.loc[row_dest, 'meanlon'] = lon_mean\n",
    "                off_staypts.loc[row_dest, 'StartEndStayptFlag'] = 'N'\n",
    "                row_dest = row_dest + 1\n",
    "                \n",
    "            curr_cluster = usr_trejec_df.loc[i, 'DistClus']\n",
    "            count_clus = 1\n",
    "                \n",
    "        if int(time.mktime(usr_trejec_df.loc[i, 'Timestamp'].timetuple()) - \n",
    "               time.mktime(usr_trejec_df.loc[i-1, 'Timestamp'].timetuple()))/60 >= 30:\n",
    "            if usr_trejec_df.loc[i-1, 'ClusDur'] < 20:\n",
    "                off_staypts.loc[row_dest, 'id'] =  usr_trejec_df.loc[i-1, 'DistClus'] \n",
    "                off_staypts['groupid'] = 0\n",
    "                off_staypts.loc[row_dest, 'start'] =  usr_trejec_df.loc[i-1, 'Timestamp'] \n",
    "                off_staypts.loc[row_dest, 'end'] =  usr_trejec_df.loc[i-1, 'Timestamp'] \n",
    "                off_staypts.loc[row_dest, 'meanlat'] =  usr_trejec_df.loc[i-1, 'Latitude'] \n",
    "                off_staypts.loc[row_dest, 'meanlon'] =  usr_trejec_df.loc[i-1, 'Longitude']\n",
    "                off_staypts.loc[row_dest, 'StartEndStayptFlag'] = 'Y'\n",
    "                row_dest = row_dest + 1\n",
    "\n",
    "            off_staypts.loc[row_dest, 'id'] =  usr_trejec_df.loc[i, 'DistClus'] \n",
    "            off_staypts['groupid'] = 0\n",
    "            off_staypts.loc[row_dest, 'start'] =  usr_trejec_df.loc[i, 'Timestamp'] \n",
    "            off_staypts.loc[row_dest, 'end'] =  usr_trejec_df.loc[i, 'Timestamp'] \n",
    "            off_staypts.loc[row_dest, 'meanlat'] =  usr_trejec_df.loc[i, 'Latitude'] \n",
    "            off_staypts.loc[row_dest, 'meanlon'] =  usr_trejec_df.loc[i, 'Longitude'] \n",
    "            off_staypts.loc[row_dest, 'StartEndStayptFlag'] = 'Y'\n",
    "            row_dest = row_dest + 1\n",
    "    \n",
    "    off_staypts.to_csv(usr_offline_staypoints_org, sep='\\t', encoding='utf-8')\n",
    "    \n",
    "    #remove staypoints added from start end trajectories which are also part of staypoints within trajectories\n",
    "    i = 0\n",
    "    length_df = len(off_staypts)\n",
    "    for i in range(0, len(off_staypts)-1):\n",
    "        \n",
    "        if off_staypts.loc[i, 'id'] == off_staypts.loc[i+1, 'id']:\n",
    "            if off_staypts.loc[i, 'StartEndStayptFlag'] == 'Y':\n",
    "                index_n = i\n",
    "            else:\n",
    "                index_n = i+1\n",
    "    \n",
    "            off_staypts.loc[index_n, 'StartEndStayptFlag'] = 'D'\n",
    "    \n",
    "    off_staypts = off_staypts[off_staypts.StartEndStayptFlag != 'D']        \n",
    "    off_staypts = off_staypts.reset_index(drop=True)\n",
    "    \n",
    "    #adjust start and end of staypoints\n",
    "    for i in range(0, len(off_staypts)-1):\n",
    "        end1_trj_time = off_staypts.loc[i, 'end']\n",
    "        end1_trj_lat = off_staypts.loc[i, 'meanlat']\n",
    "        end1_trj_lon = off_staypts.loc[i, 'meanlon']\n",
    "        str2_trj_time = off_staypts.loc[i+1, 'start']\n",
    "        str2_trj_lat = off_staypts.loc[i+1, 'meanlat']\n",
    "        str2_trj_lon = off_staypts.loc[i+1, 'meanlon']\n",
    "        \n",
    "        dist_btw = meters(end1_trj_lat, end1_trj_lon, str2_trj_lat, str2_trj_lon)\n",
    "        time_btw = (str2_trj_time - end1_trj_time).seconds / 60\n",
    "        \n",
    "        \n",
    "        if time_btw != 0:\n",
    "            avg_speed = dist_btw/time_btw\n",
    "            \n",
    "            #if the disctance between two points is less than 2*state_d_thrhld, that mean there is an overlap\n",
    "            # in this case, we cannot consider state_d_thrhld as the staypoint region, as:\n",
    "            #             before you leave state_d_thrhld of this staypoint the user already enters the next staypoint\n",
    "            if dist_btw< 2*state_d_thrhld:\n",
    "                delta_t = dist_btw/(2* avg_speed)\n",
    "            else:\n",
    "                delta_t = min(state_d_thrhld, dist_btw)/avg_speed\n",
    "        \n",
    "            end1_trj_time = end1_trj_time + timedelta(minutes=delta_t)\n",
    "            str2_trj_time = str2_trj_time - timedelta(minutes=delta_t)\n",
    "            \n",
    "            if dist_btw<= 200:\n",
    "                mean_lat = (off_staypts.loc[i, 'meanlat'] + off_staypts.loc[i+1, 'meanlat'])/2\n",
    "                mean_lon = (off_staypts.loc[i, 'meanlon'] + off_staypts.loc[i+1, 'meanlon'])/2\n",
    "                off_staypts.loc[i, 'meanlat'] =  mean_lat\n",
    "                off_staypts.loc[i, 'meanlon'] = mean_lon\n",
    "                off_staypts.loc[i+1, 'meanlat'] = mean_lat\n",
    "                off_staypts.loc[i+1, 'meanlon'] = mean_lon\n",
    "             \n",
    "            off_staypts.loc[i, 'end'] = end1_trj_time\n",
    "            off_staypts.loc[i+1, 'start'] = str2_trj_time\n",
    "    \n",
    "    #delete all staypoints with duration less than 20 minutes\n",
    "    off_staypts.start = pd.to_datetime(off_staypts.start)\n",
    "    off_staypts.end = pd.to_datetime(off_staypts.end)\n",
    "    off_staypts['duration'] = 0.0\n",
    "    for i in range(0, len(off_staypts)):\n",
    "        duration = off_staypts.loc[i, 'end']-off_staypts.loc[i, 'start']\n",
    "        seconds = duration.seconds\n",
    "        minutes = seconds / 60\n",
    "        off_staypts.loc[i, 'duration'] = minutes\n",
    "        \n",
    "    off_staypts = off_staypts[off_staypts.duration >= 20] \n",
    "    off_staypts = off_staypts.reset_index(drop=True)\n",
    "    \n",
    "    #group staypoints\n",
    "    off_staypts1 = off_staypts[['id', 'meanlat', 'meanlon']].copy()\n",
    "    off_staypts1['groupid'] = 0\n",
    "    count_start = 0\n",
    "    id_num = 1\n",
    "    while (count_start< len(off_staypts1)):\n",
    "        off_staypts1['groupid'][count_start] = id_num\n",
    "        i = count_start\n",
    "        for j in range(i+1, len(off_staypts1)):\n",
    "            \n",
    "            chk_clulat = off_staypts1['meanlat'][i]\n",
    "            chk_clulon = off_staypts1['meanlon'][i]\n",
    "            curr_clulat = off_staypts1['meanlat'][j]\n",
    "            curr_clulon = off_staypts1['meanlon'][j]\n",
    "            \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= state_d_thrhld:\n",
    "                off_staypts1['groupid'][j] = id_num\n",
    "                count_start = count_start + 1\n",
    "        \n",
    "        off_staypts1 = off_staypts1.sort_values(by = ['groupid'], ascending=False)\n",
    "        off_staypts1 = off_staypts1.reset_index(drop=True)\n",
    "        count_start = count_start + 1\n",
    "        id_num = id_num + 1\n",
    "            \n",
    "    #create group id dict and update in  main dataframe\n",
    "    grp_dict = pd.Series(off_staypts1.groupid.values,index=off_staypts1.id).to_dict()\n",
    "    \n",
    "    for i in range(0, len(off_staypts)):\n",
    "        groupid = grp_dict.get(off_staypts.loc[i, 'id'])\n",
    "        off_staypts.loc[i, 'groupid'] = groupid\n",
    "      \n",
    "    #plot the offline staypoints\n",
    "    #create a new graph where we will later add rectangles for each hour:cluster\n",
    "    #create a color dictionary for each cluster for the plot\n",
    "    \n",
    "    \n",
    "    dicts = {}\n",
    "    clu_list = []\n",
    "    clu_list = off_staypts['groupid'].unique()\n",
    "    r = lambda: random.randint(0,255)\n",
    "    for i in range(0, len(clu_list)):\n",
    "        dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(),r(),r()))\n",
    "    \n",
    "        \n",
    "        \n",
    "    fig2 = plt.figure(figsize=(15,15))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "    \n",
    "    #get all the dates for y axis\n",
    "    date_list = off_staypts['start'].dt.date.unique()\n",
    "    \n",
    "    y = range(0, len(date_list))\n",
    "    def_yticks = date_list\n",
    "    plt.yticks(y, def_yticks)\n",
    "    \n",
    "     #drawing verical lines for each hour\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x= i, linewidth=1, color='r')\n",
    "    \n",
    "     #set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_list))\n",
    "\n",
    "    date_counter = 0\n",
    "    last_date = off_staypts['start'][0].date()\n",
    "    \n",
    "    for i in range(0, len(off_staypts)):\n",
    "       \n",
    "        if (last_date != off_staypts['start'][i].date()):\n",
    "            date_counter = date_counter + 1\n",
    "            last_date = off_staypts['start'][i].date()\n",
    "            ax1.axhline(y= date_counter, linewidth=1, color='r')\n",
    "            \n",
    "        #if the trajectory is going to next day\n",
    "        if (off_staypts['start'][i].date() !=\n",
    "           off_staypts['end'][i].date()):\n",
    "            #day 1 rectangle\n",
    "            a = (off_staypts['start'][i].hour + \n",
    "             off_staypts['start'][i].minute/60)\n",
    "            b = 24\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(off_staypts['groupid'][i])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=off_staypts['groupid'][i]))\n",
    "            ax1.annotate(off_staypts['groupid'][i], (a + width/2, height/2 + date_counter), \n",
    "                     color='w', weight='bold', fontsize=10, ha='center', va='center')\n",
    "            \n",
    "            #day 2 rectangle\n",
    "            a = 0\n",
    "            b = (off_staypts['end'][i].hour + \n",
    "             off_staypts['end'][i].minute/60)\n",
    "            width = b - a\n",
    "            height = 1\n",
    "\n",
    "            col_id = dicts.get(off_staypts['groupid'][i])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter+1), width, height, color=col_id, label=off_staypts['groupid'][i]))\n",
    "            ax1.annotate(off_staypts['groupid'][i], (a + width/2, height/2 + date_counter+1), \n",
    "                     color='w', weight='bold', fontsize=10, ha='center', va='center')\n",
    "            \n",
    "        if (off_staypts['start'][i].date() ==\n",
    "           off_staypts['end'][i].date()):    \n",
    "            a = (off_staypts['start'][i].hour + \n",
    "                 off_staypts['start'][i].minute/60)\n",
    "            b = (off_staypts['end'][i].hour +\n",
    "                off_staypts['end'][i].minute/60)\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            \n",
    "            col_id = dicts.get(off_staypts['groupid'][i])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=off_staypts['groupid'][i]))\n",
    "            ax1.annotate(off_staypts['groupid'][i], (a + width/2, height/2 + date_counter), \n",
    "                     color='w', weight='bold', fontsize=10, ha='center', va='center')\n",
    "\n",
    "\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "    plt.show()\n",
    "    \n",
    "    off_staypts.to_csv(usr_offline_staypoints_at, sep='\\t', encoding='utf-8')\n",
    "#-------------------------------\n",
    "def prepare_dfs():\n",
    "    global cluster_hourly_df    \n",
    "    \n",
    "    #create off_staypts\n",
    "    off_staypts['id'] = 0\n",
    "    off_staypts['groupid'] = 0\n",
    "    off_staypts['start'] = 0\n",
    "    off_staypts['end'] = 0\n",
    "    off_staypts['meanlat'] = 0\n",
    "    off_staypts['meanlon'] = 0\n",
    "    off_staypts['StartEndStayptFlag'] = 'N'\n",
    "\n",
    "#--------------------------------MAIN--------------------------------------------------------------\n",
    "def main():\n",
    "    global usr_trejec_df\n",
    "    \n",
    "    #create directories\n",
    "    directory_setup()\n",
    "    \n",
    "    #prepere dataframes\n",
    "    prepare_dfs()\n",
    "    \n",
    "    #read test user trajectory file. In real scenerio, this will be the GPS read data\n",
    "    read_usr_file()\n",
    "    \n",
    "    #find offline staypoints\n",
    "    offline_staypoints()\n",
    "\n",
    "#-----------------------------CHANGE HERE --------------------------------------------------------\n",
    "\n",
    "state_d_thrhld = 200    #state distance threshold\n",
    "staypts_d_thrhld = 200  #staypoint distance threshold\n",
    "staypts_t_thrhld = 20   #staypoint time threshold\n",
    "state_d_thrhld = 200    #state distance threshold\n",
    "\n",
    "#Edit user name, and path locations for source and destination files\n",
    "user = \"085\"\n",
    "file_source_raw = \"C:/Users/12sha/Documents/Geolife Trajectories 1.3/Data/\" + user + \"/Trajectory/200811*.plt\"\n",
    "dest_path = \"C:/Users/12sha/Documents/thesislocation/code_/stay points/v0.6 results\"\n",
    "\n",
    "#destination files\n",
    "usr_offline_staypoints_org = dest_path + \"/User \" + user + \"/staypoints(original).csv\"\n",
    "usr_offline_staypoints_at = dest_path + \"/User \" + user + \"/staypoints(with added times).csv\"\n",
    "#------------------------------DATAFRAME DECLARATIONS---------------------------------------------\n",
    "\n",
    "#global dataframes used\n",
    "\n",
    "usr_trejec_df = pd.DataFrame() #user raw trajectory dataframe\n",
    "\n",
    "off_staypts = pd.DataFrame() #offline staypoints\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#online process\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math \n",
    "import os\n",
    "import errno\n",
    "import matplotlib.patches as patches\n",
    "from copy import deepcopy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "import operator\n",
    "import pdb\n",
    "from geopy.geocoders import Nominatim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_usr_file():\n",
    "    global usr_trejec_df\n",
    "    \n",
    "    #Load file names for user\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    #Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header = None) for filename in filenames]\n",
    "\n",
    "    #put the data from list into one dataframe\n",
    "    usr_trejec_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    usr_trejec_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Date\"].map(str) + \" \" + usr_trejec_df[\"Time\"]\n",
    "    \n",
    "    usr_trejec_df.Timestamp = pd.to_datetime(usr_trejec_df.Timestamp)\n",
    "    \n",
    "    usr_trejec_df.index = usr_trejec_df['Timestamp']\n",
    "    usr_trejec_df = usr_trejec_df.resample('1T').mean()\n",
    "    usr_trejec_df = usr_trejec_df.dropna()\n",
    "    \n",
    "     #add columns to user trajectory dataframe\n",
    "    #1. add timestamp as column\n",
    "    usr_trejec_df['Timestamp'] = pd.to_datetime(usr_trejec_df.index)\n",
    "    #restore date and time column\n",
    "    usr_trejec_df['Date'] = usr_trejec_df.Timestamp.dt.date\n",
    "    usr_trejec_df['Time'] = usr_trejec_df.Timestamp.dt.time\n",
    "    usr_trejec_df['Hour'] = usr_trejec_df.Timestamp.dt.hour\n",
    "    \n",
    "    #sort values based on timestamp\n",
    "    usr_trejec_df = usr_trejec_df.sort_values(['Timestamp'])\n",
    "    #reset index\n",
    "    usr_trejec_df = usr_trejec_df.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    usr_trejec_df['Weekday'] = usr_trejec_df['Timestamp'].dt.weekday.map(str) + usr_trejec_df['Timestamp'].dt.weekday_name\n",
    "\n",
    "    usr_trejec_df['StayPoint'] = -1 # 1 if it is a staypoint, else 0\n",
    "    usr_trejec_df['StayptId'] = -1\n",
    "    usr_trejec_df['StayMeanLat'] = -1.0\n",
    "    usr_trejec_df['StayMeanLon'] = -1.0\n",
    "    usr_trejec_df['State'] = -1     # 1 if it is a state, else 0\n",
    "    usr_trejec_df['StateId'] = -1\n",
    "    usr_trejec_df['StateMeanLat'] = -1.0\n",
    "    usr_trejec_df['StateMeanLon'] = -1.0\n",
    "    \n",
    "    #remove columns not used/required\n",
    "    usr_trejec_df = usr_trejec_df.drop(['0', 'Altitude', 'NumDays'], axis = 1)\n",
    "\n",
    "#-------------------------------\n",
    "def prepare_dfs():\n",
    "    global cluster_hourly_df \n",
    "\n",
    "    #create cluster_hourly_df columns\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['StateId'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "        \n",
    "#------------------------------------------------------------------------------------\n",
    "#Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):  \n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "    d = R * c\n",
    "    return d * 1000 # meters\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def cluster(newlat, newlon, row, count, orig_lat, orig_lon):\n",
    "    global curr_hr_df\n",
    "    \n",
    "    currcluster = curr_hr_df['StayptId'][row-1]\n",
    "    curr_hr_df['StayptId'][row] = -1\n",
    "    curr_hr_df['StayMeanLat'][row] = -1.0\n",
    "    curr_hr_df['StayMeanLon'][row] = -1.0\n",
    "    curr_hr_df['StayPoint'][row] = -1\n",
    "    clulat = curr_hr_df['StayMeanLat'][row-1]\n",
    "    clulon = curr_hr_df['StayMeanLon'][row-1]\n",
    "    \n",
    "    #if the new point and the old point time difference is greater than tracking threshold\n",
    "    # then add both the points as staypoints and leave\n",
    "    prevPointTime = curr_hr_df['Timestamp'][row-1]\n",
    "    currPointTime = curr_hr_df['Timestamp'][row]\n",
    "    timm_diff = (currPointTime - prevPointTime).seconds /60\n",
    "    dist_diff = meters(clulat, clulon, newlat, newlon)   \n",
    "    \n",
    "    if (timm_diff >= track_t_thrhld and dist_diff > staypts_d_thrhld ):\n",
    "        curr_hr_df.loc[row-1, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row, 'StayptId'] = currcluster + 1\n",
    "    else:    \n",
    "        #if the new point and old point's distance is less than threshold, then add it to current cluster\n",
    "        if meters(clulat, clulon, newlat, newlon)<= staypts_d_thrhld:\n",
    "            curr_hr_df['StayptId'][row] = currcluster\n",
    "            #calculate new mean lat and lon for the cluster\n",
    "            array_lat = curr_hr_df['Latitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "            array_lon = curr_hr_df['Longitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "\n",
    "            #cal new means\n",
    "            new_lat_mean = np.mean(array_lat)\n",
    "            new_lon_mean = np.mean(array_lon)\n",
    "\n",
    "            curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayMeanLat'] = new_lat_mean\n",
    "            curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayMeanLon'] = new_lon_mean\n",
    "\n",
    "    #         curr_hr_df['StayMeanLat'] = curr_hr_df.groupby('StayptId')['Latitude'].transform(np.mean)\n",
    "    #         curr_hr_df['StayMeanLon'] = curr_hr_df.groupby('StayptId')['Longitude'].transform(np.mean)\n",
    "            count = count + 1\n",
    "\n",
    "        #if the new point and old point's distance is greater than threshold, it means the point moved away\n",
    "        #if the previous cluster has more than two points, check the duration of the previous cluster\n",
    "        #   if the duration of the previos cluster is greater than threshold, assign it as a staypoint\n",
    "\n",
    "        #if the row read is the last row for this hour\n",
    "        if (row == len(curr_hr_df)-1):\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row-count+1]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayPoint'] = 1        \n",
    "\n",
    "        #if the new point is moving away from the cluster\n",
    "        if meters(clulat, clulon, newlat, newlon)> staypts_d_thrhld:\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row-count]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row-1]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[ (curr_hr_df['StayptId']==currcluster), 'StayPoint'] = 1\n",
    "                #incase the cluster is not a staypoint and the first point is already a staypoint\n",
    "                #then retain the latitued and longitudes\n",
    "                else:\n",
    "                    if (row-count) == 0 and curr_hr_df['StayPoint'][row-count] == 1:\n",
    "                        curr_hr_df['StayMeanLat'][row-count] = orig_lat\n",
    "                        curr_hr_df['StayMeanLon'][row-count] = orig_lon\n",
    "            \n",
    "            count = 1\n",
    "            curr_hr_df['StayMeanLat'][row] = curr_hr_df['Latitude'][row]\n",
    "            curr_hr_df['StayMeanLon'][row] = curr_hr_df['Longitude'][row]\n",
    "            curr_hr_df['StayptId'][row] = currcluster + 1\n",
    "\n",
    "    return count\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def read_trained_model():\n",
    "    global trained_model_df\n",
    "    \n",
    "    if os.path.isfile(dest_file_final_markov_chain):\n",
    "        trained_model_df = pd.read_csv(dest_file_final_markov_chain, header = 0, sep=\"\\t\")\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "def create_last_hr_staypts():\n",
    "    global curr_hr_df          #holds current hour points\n",
    "    global staypts_df          #holds all staypoints\n",
    "    global curr_hr_staypts_df  #holds current hour staypoints only\n",
    "    global prev_hour_last_point #stores last hour last point\n",
    "    \n",
    "    #clear current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_staypts_df.iloc[0:0]\n",
    "    \n",
    "    #reset index of current hour points\n",
    "    curr_hr_df = curr_hr_df.reset_index(drop=True)\n",
    "    \n",
    "    #feching the last stayptid \n",
    "    if not staypts_df.empty:\n",
    "        stayid = staypts_df['StayptId'].max() + 1 #assign next possible staypt id\n",
    "    else:\n",
    "        stayid = 1 #if this is the start, start from 1 as staypointID\n",
    "    \n",
    "    adding_first_as_staypt = 'N'\n",
    "    adding_last_as_staypt = 'N'\n",
    "    orig_lat = 0\n",
    "    orig_lon = 0\n",
    "    if not prev_hour_last_point.empty:\n",
    "        #check the time difference of last hour last point and this hour first point is greater than track t threshold\n",
    "        #if yes than add both as staypoints\n",
    "        if (int(time.mktime(curr_hr_df.loc[0, 'Timestamp'].timetuple()) - \n",
    "                   time.mktime(prev_hour_last_point.loc[0, 'Timestamp'].timetuple()))/60 > track_t_thrhld):\n",
    "            if prev_hour_last_point.loc[0, 'StayPoint'] != 1:\n",
    "                adding_last_as_staypt = 'Y'\n",
    "                prev_hour_last_point.loc[0, 'StayptId'] = stayid\n",
    "                stayid = stayid + 1\n",
    "                prev_hour_last_point.loc[0, 'StayPoint'] = 1\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLat'] = prev_hour_last_point.loc[0, 'Latitude']\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLon'] = prev_hour_last_point.loc[0, 'Longitude']\n",
    "                staypts_df = staypts_df.append(prev_hour_last_point)\n",
    "\n",
    "            curr_hr_df.loc[0, 'StayptId'] = stayid\n",
    "            stayid = stayid + 1\n",
    "            curr_hr_df.loc[0, 'StayPoint'] = 1\n",
    "            orig_lat = curr_hr_df.loc[0, 'Latitude']\n",
    "            orig_lon = curr_hr_df.loc[0, 'Longitude']\n",
    "            adding_first_as_staypt = 'Y'\n",
    "        \n",
    "            \n",
    "    #Read the file in an online manner as the points come and assign the points to clusters\n",
    "    row =1\n",
    "    count = 1\n",
    "    \n",
    "    if adding_first_as_staypt == 'N': \n",
    "        curr_hr_df['StayptId'][row-1] = stayid\n",
    "        curr_hr_df['StayPoint'][row-1] = -1\n",
    "        \n",
    "    curr_hr_df['StayMeanLat'][row-1] = curr_hr_df['Latitude'][0]\n",
    "    curr_hr_df['StayMeanLon'][row-1] = curr_hr_df['Longitude'][0]\n",
    "    \n",
    "    \n",
    "    while row < len(curr_hr_df):\n",
    "        count = cluster(curr_hr_df['Latitude'][row], curr_hr_df['Longitude'][row], row, count, orig_lat, orig_lon)\n",
    "        row= row + 1\n",
    "    \n",
    "    #copy the staypoints to the current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_df.loc[curr_hr_df['StayPoint'] == 1]\n",
    "    #copy the stay points into another dataframe\n",
    "    staypts_df = staypts_df.append(curr_hr_df.loc[curr_hr_df['StayPoint'] == 1])\n",
    "    #reset staypoints index\n",
    "    curr_hr_staypts_df.index = curr_hr_staypts_df['Timestamp']\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "\n",
    "    #store the last hour last point\n",
    "    prev_hour_last_point = prev_hour_last_point.iloc[0:0]\n",
    "    prev_hour_last_point = curr_hr_df.iloc[[len(curr_hr_df)-1]]\n",
    "    prev_hour_last_point = prev_hour_last_point.reset_index(drop=True)\n",
    "    \n",
    "    #clear current hour dataframe content\n",
    "    curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "def add_start_end_times():\n",
    "    global staypts_df\n",
    "    \n",
    "    if staypts_df.empty or len(staypts_df) == 1:\n",
    "        return\n",
    "    \n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "    \n",
    "#     idx = staypts_df.index[staypts_df['Hour'] == prev_hour]\n",
    "#     if idx.empty:\n",
    "#         return\n",
    "    \n",
    "#     #fetch the index for which we should add the times\n",
    "#     strt_indx = min(idx)\n",
    "#     end_indx = strt_indx - 1\n",
    "    \n",
    "     #state = -2 indicates it is already processed\n",
    "    idx = staypts_df.index[staypts_df['State'] == -1]\n",
    "    if idx.empty:\n",
    "         return\n",
    "\n",
    "    if min(idx) == 0:\n",
    "        strt_indx = min(idx)\n",
    "    else:\n",
    "        strt_indx = min(idx) - 1\n",
    "    \n",
    "    prev_id = staypts_df.loc[strt_indx, 'StayptId']\n",
    "    for i in range(strt_indx, len(staypts_df)):  \n",
    "        #update state as -2 indicating processed\n",
    "        staypts_df.loc[i, 'State'] = -2 \n",
    "        \n",
    "        if staypts_df.loc[i, 'StayptId'] != prev_id and staypts_df.loc[i, 'StayptId']>prev_id:\n",
    "            prev_id = staypts_df.loc[i, 'StayptId']\n",
    "            strt_indx = i\n",
    "            end_indx = i-1\n",
    "            \n",
    "            #calclulate time to be added\n",
    "            end1_trj_time = staypts_df.loc[end_indx, 'Timestamp']\n",
    "            end1_trj_lat = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "            end1_trj_lon = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "            str2_trj_time = staypts_df.loc[strt_indx, 'Timestamp']\n",
    "            str2_trj_lat = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "            str2_trj_lon = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "\n",
    "            dist_btw = meters(end1_trj_lat, end1_trj_lon, str2_trj_lat, str2_trj_lon)\n",
    "            time_btw = (str2_trj_time - end1_trj_time).seconds / 60\n",
    "            \n",
    "            if time_btw != 0:\n",
    "                avg_speed = dist_btw/time_btw\n",
    "\n",
    "                #if the disctance between two points is less than 2*state_d_thrhld, that mean there is an overlap\n",
    "                # in this case, we cannot consider state_d_thrhld as the staypoint region, as:\n",
    "                #             before user leave state_d_thrhld of this staypoint the user already enters the next staypoint\n",
    "                if dist_btw< 2*state_d_thrhld:\n",
    "                    delta_t = dist_btw/(2* avg_speed)\n",
    "                else:\n",
    "                    delta_t = min(state_d_thrhld, dist_btw)/avg_speed\n",
    "\n",
    "                end1_trj_time = end1_trj_time + timedelta(minutes=delta_t)\n",
    "                str2_trj_time = str2_trj_time - timedelta(minutes=delta_t)\n",
    "\n",
    "                #think about this later\n",
    "        #         if dist_btw<= 200:\n",
    "        #             mean_lat = (off_staypts.loc[i, 'meanlat'] + off_staypts.loc[i+1, 'meanlat'])/2\n",
    "        #             mean_lon = (off_staypts.loc[i, 'meanlon'] + off_staypts.loc[i+1, 'meanlon'])/2\n",
    "        #             off_staypts.loc[i, 'meanlat'] =  mean_lat\n",
    "        #             off_staypts.loc[i, 'meanlon'] = mean_lon\n",
    "        #             off_staypts.loc[i+1, 'meanlat'] = mean_lat\n",
    "        #             off_staypts.loc[i+1, 'meanlon'] = mean_lon\n",
    "\n",
    "                curr_ls_r = len(staypts_df)\n",
    "\n",
    "                #add end of prev point line\n",
    "                staypts_df.loc[curr_ls_r+1, 'Latitude'] = staypts_df.loc[end_indx, 'Latitude']\n",
    "                staypts_df.loc[curr_ls_r+1, 'Longitude'] = staypts_df.loc[end_indx, 'Longitude']\n",
    "                staypts_df.loc[curr_ls_r+1, 'Timestamp'] = end1_trj_time\n",
    "                staypts_df.loc[curr_ls_r+1, 'Date'] = end1_trj_time.date()\n",
    "                staypts_df.loc[curr_ls_r+1, 'Time'] = end1_trj_time.time()\n",
    "                staypts_df.loc[curr_ls_r+1, 'Hour'] = end1_trj_time.hour\n",
    "                staypts_df.loc[curr_ls_r+1, 'Weekday'] = str(end1_trj_time.weekday())+ end1_trj_time.weekday_name\n",
    "                staypts_df.loc[curr_ls_r+1, 'StayPoint'] = 1\n",
    "                staypts_df.loc[curr_ls_r+1, 'StayptId'] = staypts_df.loc[end_indx, 'StayptId']\n",
    "                staypts_df.loc[curr_ls_r+1, 'StayMeanLat'] = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "                staypts_df.loc[curr_ls_r+1, 'StayMeanLon'] = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "                staypts_df.loc[curr_ls_r+1, 'State'] = -2   #to indicate its po´rocessed\n",
    "                staypts_df.loc[curr_ls_r+1, 'StateId'] = -1\n",
    "                staypts_df.loc[curr_ls_r+1, 'StateMeanLat'] = -1\n",
    "                staypts_df.loc[curr_ls_r+1, 'StateMeanLon'] = -1\n",
    "\n",
    "                #add start of current point line\n",
    "                staypts_df.loc[curr_ls_r+2, 'Latitude'] = staypts_df.loc[strt_indx, 'Latitude']\n",
    "                staypts_df.loc[curr_ls_r+2, 'Longitude'] = staypts_df.loc[strt_indx, 'Longitude']\n",
    "                staypts_df.loc[curr_ls_r+2, 'Timestamp'] = str2_trj_time\n",
    "                staypts_df.loc[curr_ls_r+2, 'Date'] = str2_trj_time.date()\n",
    "                staypts_df.loc[curr_ls_r+2, 'Time'] = str2_trj_time.time()\n",
    "                staypts_df.loc[curr_ls_r+2, 'Hour'] = str2_trj_time.hour\n",
    "                staypts_df.loc[curr_ls_r+2, 'Weekday'] = str(str2_trj_time.weekday()) + str2_trj_time.weekday_name\n",
    "                staypts_df.loc[curr_ls_r+2, 'StayPoint'] = 1\n",
    "                staypts_df.loc[curr_ls_r+2, 'StayptId'] = staypts_df.loc[strt_indx, 'StayptId']\n",
    "                staypts_df.loc[curr_ls_r+2, 'StayMeanLat'] = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "                staypts_df.loc[curr_ls_r+2, 'StayMeanLon'] = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "                staypts_df.loc[curr_ls_r+2, 'State'] = -2 #to indicate its po´rocessed\n",
    "                staypts_df.loc[curr_ls_r+2, 'StateId'] = -1\n",
    "                staypts_df.loc[curr_ls_r+2, 'StateMeanLat'] = -1\n",
    "                staypts_df.loc[curr_ls_r+2, 'StateMeanLon'] = -1\n",
    "                \n",
    "                #sort values again with timestamp and reset index\n",
    "                #staypts_df = staypts_df.sort_values(['Timestamp'])\n",
    "                #staypts_df = staypts_df.reset_index(drop=True)\n",
    "    \n",
    "    staypts_df = staypts_df.sort_values(['Timestamp', 'StayptId'])\n",
    "    staypts_df = staypts_df.reset_index(drop=True)            \n",
    "    \n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "def cal_hourly_state_weight():\n",
    "    global curr_hr_staypts_df\n",
    "    global cluster_hourly_df   \n",
    "    \n",
    "    curr_hr_cluster_hourly_df = pd.DataFrame()       \n",
    "    \n",
    "    last_hour = curr_hr_staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = curr_hr_staypts_df['StateId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        curr_hr_cluster_hourly_df['Date'] = 0\n",
    "        curr_hr_cluster_hourly_df['StateId'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLat'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLon'] = 0\n",
    "        curr_hr_cluster_hourly_df[i] = 0\n",
    "    \n",
    "    for i in range(0, len(curr_hr_staypts_df)):\n",
    "\n",
    "        if (i == len(curr_hr_staypts_df)-1):\n",
    "            \n",
    "            k = curr_hr_staypts_df['Timestamp'][i] - curr_hr_staypts_df['Timestamp'][i-curr_count]\n",
    "            l = int((k / np.timedelta64(1, 'm')))\n",
    "            \n",
    "            date_read = curr_hr_staypts_df.index[i].date()\n",
    "            cluster_id = curr_hr_staypts_df['StateId'][i]\n",
    "            ClusterMeanLat = curr_hr_staypts_df['StateMeanLat'][i]\n",
    "            ClusterMeanLon = curr_hr_staypts_df['StateMeanLon'][i]\n",
    "            col_name = curr_hr_staypts_df.index[i].hour\n",
    "\n",
    "            curr_hr_cluster_hourly_df.loc[j,'AvgLat'] = ClusterMeanLat\n",
    "            curr_hr_cluster_hourly_df.loc[j,'AvgLon'] = ClusterMeanLon\n",
    "            curr_hr_cluster_hourly_df.loc[j,'Date'] = date_read\n",
    "            curr_hr_cluster_hourly_df.loc[j,'StateId'] = cluster_id\n",
    "            curr_hr_cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            \n",
    "        if (curr_hr_staypts_df['Timestamp'][i].hour != last_hour) | (curr_hr_staypts_df['StateId'][i] != last_clusid):\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "            if (curr_count == 1) & (curr_hr_staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                k = ((curr_hr_staypts_df['Timestamp'][i-1] + pd.Timedelta(hours=1) - \n",
    "                      pd.Timedelta(minutes=curr_hr_staypts_df['Timestamp'][i-1].minute)) - \n",
    "                     curr_hr_staypts_df['Timestamp'][i-1])\n",
    "            else:\n",
    "                k = curr_hr_staypts_df['Timestamp'][i-1] - curr_hr_staypts_df['Timestamp'][i-curr_count]\n",
    "\n",
    "            l = int((k / np.timedelta64(1, 'm')))\n",
    "            date_read = curr_hr_staypts_df.index[i-1].date()\n",
    "            cluster_id = curr_hr_staypts_df['StateId'][i-1]\n",
    "            ClusterMeanLat = curr_hr_staypts_df['StateMeanLat'][i-1]\n",
    "            ClusterMeanLon = curr_hr_staypts_df['StateMeanLon'][i-1]\n",
    "            col_name = curr_hr_staypts_df.index[i-1].hour\n",
    "\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'AvgLat'] = ClusterMeanLat\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'AvgLon'] = ClusterMeanLon\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'Date'] = date_read\n",
    "            curr_hr_cluster_hourly_df.loc[j, 'StateId'] = cluster_id\n",
    "            curr_hr_cluster_hourly_df.loc[j, col_name] = round((l)/60,4)\n",
    "            j = j + 1\n",
    "            curr_count = 1\n",
    "\n",
    "            if (curr_hr_staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = curr_hr_staypts_df['Timestamp'][i].hour\n",
    "            if (curr_hr_staypts_df['StateId'][i] != last_clusid):\n",
    "                last_clusid = curr_hr_staypts_df['StateId'][i]\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.fillna(0)\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.groupby(['Date', 'StateId', 'AvgLat', 'AvgLon']).sum()\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(level=[0,1,2,3])\n",
    "   \n",
    "    cluster_hourly_df = cluster_hourly_df.append(curr_hr_cluster_hourly_df, ignore_index=True)\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(drop=True)\n",
    "    \n",
    "#-------------form states-----------------------------------------------------------------------\n",
    "def form_states():\n",
    "    global staypts_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    #update states in final staypoints\n",
    "    #copy staypoint data as state data\n",
    "    staypts_df['StateId'] = staypts_df['StayptId']\n",
    "    staypts_df['StateMeanLat'] = staypts_df['StayMeanLat']\n",
    "    staypts_df['StateMeanLon'] = staypts_df['StayMeanLon']\n",
    "    \n",
    "    #this fucntion groups the staypoints together to from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['StateMeanLat', 'StateMeanLon'])\n",
    "    staypts_df1 = staypts_df1.reset_index(drop=True)\n",
    "    \n",
    "    row = 1\n",
    "\n",
    "    for i in range(0, len(staypts_df1)-1):\n",
    "        for j in range(i+1, len(staypts_df1)):\n",
    "        \n",
    "            chk_cluster = staypts_df1['StateId'][i]\n",
    "            chk_clulat = staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['StateId'][j]\n",
    "            curr_clulat = staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['StateMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= state_d_thrhld:\n",
    "                #before adding this point to the ith state, \n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "                \n",
    "                add_state = \"Yes\"\n",
    "                #form the existing lat and lon array\n",
    "                array_lat = staypts_df['Latitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = staypts_df['Longitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                #add the new lat and lon values to the array\n",
    "                new_lats = staypts_df['Latitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = staypts_df['Longitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "                \n",
    "                array_lat= np.append(array_lat, new_lats)\n",
    "                array_lon= np.append(array_lon, new_lons)\n",
    "                #cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "                \n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "                        \n",
    "                if add_state == \"Yes\":    \n",
    "                    staypts_df.loc[ (staypts_df['StateId']==curr_cluster), 'StateId'] = chk_cluster\n",
    "                    staypts_df.loc[ (staypts_df['StateId']==chk_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    staypts_df.loc[ (staypts_df['StateId']==chk_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    #update states for last hour staypoints\n",
    "    #copy staypoint data as state data\n",
    "     #copy staypoint data as state data\n",
    "    curr_hr_staypts_df['StateId'] = curr_hr_staypts_df['StayptId']\n",
    "    curr_hr_staypts_df['StateMeanLat'] = curr_hr_staypts_df['StayMeanLat']\n",
    "    curr_hr_staypts_df['StateMeanLon'] = curr_hr_staypts_df['StayMeanLon']\n",
    "    \n",
    "    #this fucntion groups the staypoints together to from different days \n",
    "    #Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.sort_values(['StateMeanLat', 'StateMeanLon'])\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.reset_index(drop=True)\n",
    "    \n",
    "    row = 1\n",
    "\n",
    "    for i in range(0, len(curr_hr_staypts_df1)):\n",
    "        for j in range(i+1, len(curr_hr_staypts_df1)):\n",
    "        \n",
    "            chk_cluster = curr_hr_staypts_df1['StateId'][i]\n",
    "            chk_clulat = curr_hr_staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = curr_hr_staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = curr_hr_staypts_df1['StateId'][j]\n",
    "            curr_clulat = curr_hr_staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = curr_hr_staypts_df1['StateMeanLon'][j]\n",
    "        \n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon)<= state_d_thrhld:\n",
    "                #before adding this point to the ith state, \n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "                \n",
    "                add_state = \"Yes\"\n",
    "                #form the existing lat and lon array\n",
    "                array_lat = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                #add the new lat and lon values to the array\n",
    "                new_lats = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "                \n",
    "                array_lat= np.append(array_lat, new_lats)\n",
    "                array_lon= np.append(array_lon, new_lons)\n",
    "                #cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "                \n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "                        \n",
    "                if add_state == \"Yes\":    \n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateId'] = chk_cluster\n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    curr_hr_staypts_df.loc[ (curr_hr_staypts_df['StateId']==curr_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "#------------------------------------------------------------------------------------------------\n",
    "def visualize_hourly_state_weight():\n",
    "    global staypts_df\n",
    "    \n",
    "    #create a color dictionary for each cluster for the plot\n",
    "    dicts = {}\n",
    "    clu_list = []\n",
    "    clu_list = staypts_df['StateId'].unique()\n",
    "    r = lambda: random.randint(0,255)\n",
    "    #olors = sns.color_palette(\"Paired\", len(clu_list))\n",
    "    \n",
    "    for i in range(0, len(clu_list)):\n",
    "        #icts[clu_list[i]] = (colors[i])\n",
    "        dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(),r(),r()))\n",
    "        \n",
    "    #create a new graph where we will later add rectangles for each hour:cluster\n",
    "    fig2 = plt.figure(figsize=(15,15))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "\n",
    "    #get all the dates for y axis\n",
    "    date_list = staypts_df['Timestamp'].dt.date.unique()\n",
    "    y = range(0, len(date_list))\n",
    "    def_yticks = date_list\n",
    "    plt.yticks(y, def_yticks)\n",
    "    \n",
    "    #set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_list))\n",
    "\n",
    "    date_counter = 0\n",
    "    last_date = staypts_df['Timestamp'][0].date()\n",
    "    last_hour = staypts_df['Timestamp'][0].hour\n",
    "    last_clusid = staypts_df['StateId'][0]\n",
    "    curr_count = 0\n",
    "    j = 0\n",
    "    \n",
    "    #drawing verical lines for each hour\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x= i, linewidth=1, color='r')\n",
    "\n",
    "    for i in range(0, len(staypts_df)):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        \n",
    "        if (i == len(staypts_df)-1):\n",
    "            a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "            b = staypts_df['Timestamp'][i].hour + staypts_df['Timestamp'][i].minute/60\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(staypts_df['StateId'][i])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['StateId'][i]))\n",
    "            \n",
    "        #plot a rectangle if the hour or stateid or date has changed\n",
    "        if ((staypts_df['Timestamp'][i].hour != last_hour) | (staypts_df['StateId'][i] != last_clusid)\n",
    "           | (last_date != staypts_df['Timestamp'][i].date())):\n",
    "\n",
    "            if (curr_count == 1) & (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + 1\n",
    "            else:\n",
    "                a = staypts_df['Timestamp'][i-curr_count].hour + staypts_df['Timestamp'][i-curr_count].minute/60\n",
    "\n",
    "            b = staypts_df['Timestamp'][i-1].hour + staypts_df['Timestamp'][i-1].minute/60\n",
    "\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            col_id = dicts.get(staypts_df['StateId'][i-1])\n",
    "            ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id, label=staypts_df['StateId'][i-1]))\n",
    "\n",
    "            curr_count = 1\n",
    "\n",
    "            if (staypts_df['Timestamp'][i].hour != last_hour):\n",
    "                last_hour = staypts_df['Timestamp'][i].hour\n",
    "            if (staypts_df['StateId'][i] != last_clusid):\n",
    "                last_clusid = staypts_df['StateId'][i]\n",
    "            if (last_date != staypts_df['Timestamp'][i].date()):\n",
    "                date_counter = date_counter + 1\n",
    "                last_date = staypts_df['Timestamp'][i].date()\n",
    "                ax1.axhline(y= date_counter, linewidth=1, color='r')\n",
    "\n",
    "        else:\n",
    "            curr_count = curr_count + 1\n",
    "            \n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    handle_list, label_list = [], []\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label not in label_list:\n",
    "            handle_list.append(handle)\n",
    "            label_list.append(label)\n",
    "    plt.legend(handle_list, label_list)\n",
    "\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "    plt.show()\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_staypts_csv():\n",
    "     with open(dest_file_staypoints, 'a') as f:\n",
    "             (staypts_df).to_csv(f,  sep='\\t', encoding='utf-8')\n",
    "#-----------------------------------------------------------------------------------\n",
    "def update_hourly_weights_csv():\n",
    "    with open(dest_file_hourly_weights, 'a') as f:\n",
    "             (cluster_hourly_df).to_csv(f,  sep='\\t', encoding='utf-8')\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_seperate_trasition_matrices():\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    \n",
    "    #create a temp dataframe for each data, and calculate trasition matrices from hour t to t+1\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "\n",
    "        #create a temp dataframe for pervious date\n",
    "        temp_df = pd.DataFrame()\n",
    "        matrices_df = pd.DataFrame()\n",
    "        temp_df = cluster_hourly_df.loc[cluster_hourly_df['Date'] == date_list[p]]\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0, 24):\n",
    "            matrices_df['Date'] = 0\n",
    "            matrices_df['StateId'] = 0\n",
    "            for j in range(0, len(temp_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['StateId'][j])\n",
    "                matrices_df[colname] = 0\n",
    "\n",
    "        matrices_df['Date'] = temp_df['Date']\n",
    "        matrices_df['StateId'] = temp_df['StateId']\n",
    "\n",
    "        for i in range (0, 23):\n",
    "            for j in range (0, len(temp_df)):\n",
    "                for k in range (0, len(temp_df)):\n",
    "                    prob = temp_df[i][j] * temp_df[i+1][k]\n",
    "                    colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(temp_df['StateId'][k])\n",
    "                    matrices_df[colname][j] = prob\n",
    "        file_name = dest_path_each_day_trsn_mat + str(date_list[p]) + \".csv\"\n",
    "        matrices_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def create_save_markov_chains():\n",
    "    global final_transition_df\n",
    "    global co_loc\n",
    "    \n",
    "    final_transition_df = pd.DataFrame()\n",
    "    \n",
    "    #create an empty markov chain frame for each state, and transition for each hour of the day\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    cluster_list = cluster_hourly_df['StateId'].unique()\n",
    "    AvgLat_list = cluster_hourly_df['AvgLat'].unique()\n",
    "    AvgLon_list = cluster_hourly_df['AvgLon'].unique()\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        final_transition_df['Address'] = 0\n",
    "        final_transition_df['AvgLat'] = 0\n",
    "        final_transition_df['AvgLon'] = 0\n",
    "        final_transition_df['StateId'] = 0\n",
    "        for j in range(0, cluster_hourly_df['StateId'].nunique()):\n",
    "            colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(cluster_list[j])\n",
    "            final_transition_df[colname] = 0\n",
    "          \n",
    "    final_transition_df['StateId'] = cluster_list\n",
    "    final_transition_df['AvgLat'] = AvgLat_list\n",
    "    final_transition_df['AvgLon'] = AvgLon_list\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df.index = final_transition_df.StateId\n",
    "\n",
    "    #read each day file and sum the matching rows:cols combinations\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    path_dir = dest_path_each_day_trsn_mat\n",
    "\n",
    "    \n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        temp_df = pd.DataFrame()\n",
    "        filename = path_dir + str(date_list[p]) + '.csv'\n",
    "        temp_df =  pd.read_csv(filename, header = 0, sep='\\t')\n",
    "\n",
    "        for i in range(0, len(temp_df)):\n",
    "            rowname = temp_df['StateId'][i]\n",
    "            for src_column in temp_df:\n",
    "                for dest_column in final_transition_df:\n",
    "                    if src_column == dest_column and src_column != 'StateId' :\n",
    "                        #import pdb; pdb.set_trace()\n",
    "                        final_transition_df[dest_column][rowname] = (final_transition_df[dest_column][rowname] +\n",
    "                                                                    temp_df[src_column][i])\n",
    "\n",
    "    #calculate probability from cluster x to cluster y from time t to t+1\n",
    "    final_transition_df = final_transition_df.reset_index(drop=True)\n",
    "    for clus in range(0, len(final_transition_df)):\n",
    "        for i in range(0, 24):\n",
    "            temp_sum = 0\n",
    "            for j in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['StateId'][j])\n",
    "                temp_sum += (final_transition_df[colname][clus])\n",
    "            for k in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i+1) + ')-' + str(final_transition_df['StateId'][k])\n",
    "                if temp_sum != 0:\n",
    "                    final_transition_df[colname][clus] = final_transition_df[colname][clus]/temp_sum\n",
    "    \n",
    "    #create dictionary for coordinate : address\n",
    "    points = tuple(zip(final_transition_df.AvgLat, final_transition_df.AvgLon))\n",
    "    geocoder = Nominatim(timeout=10)\n",
    "    coordinate_location = {}\n",
    "    \n",
    "    for coordinate in points:\n",
    "        try:\n",
    "            location = geocoder.reverse(coordinate)\n",
    "        except:\n",
    "            location = 'unknown'\n",
    "        coordinate_location[coordinate] = location\n",
    "    \n",
    "    co_loc = {k:v for k,v in coordinate_location.items()}\n",
    "\n",
    "    for i in range(0, len(final_transition_df)):\n",
    "        address = co_loc.get((final_transition_df['AvgLat'][i], final_transition_df['AvgLon'][i]))\n",
    "        if address == 'unknown':\n",
    "            final_transition_df['Address'][i] = 'unknown'\n",
    "        else:\n",
    "            final_transition_df['Address'][i] = address[0]\n",
    "    \n",
    "    #replace zero probabilities to a small value and save the file\n",
    "    final_transition_df = final_transition_df.fillna(0)                    \n",
    "    final_transition_df = final_transition_df.replace(0, 0.00001)\n",
    "    final_transition_df.to_csv(dest_file_final_markov_chain, sep='\\t')\n",
    "    \n",
    "    for i in range(0, 24):\n",
    "        final_transition_temp_df = pd.DataFrame()\n",
    "        k = cluster_hourly_df['StateId'].nunique()*i + 4\n",
    "        final_transition_temp_df = final_transition_df.iloc[:,k:k + cluster_hourly_df['StateId'].nunique()]\n",
    "        final_transition_temp_df.index = cluster_list\n",
    "        file_name = usr_markov_chains_directory + \"/\" + str(i) + \" hour.csv\"\n",
    "        final_transition_temp_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "        \n",
    "        \n",
    "#------------------------------------------------------------------------------------\n",
    "def predict(new_hour):\n",
    "    global trained_model_df\n",
    "    global curr_hr_staypts_df\n",
    "    \n",
    "    tobepredicted_df = curr_hr_staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon', 'Timestamp']]\n",
    "    tobepredicted_df = tobepredicted_df.drop_duplicates()\n",
    "    tobepredicted_df = tobepredicted_df.reset_index(drop=True)\n",
    "\n",
    "    for j in range(0, len(tobepredicted_df)):\n",
    "        new_lat = tobepredicted_df['StateMeanLat'][j]\n",
    "        new_lon = tobepredicted_df['StateMeanLon'][j]\n",
    "        file_name = \"Predicted Timestamp - \" +  str(tobepredicted_df['Timestamp'][j]) + \".csv\"\n",
    "        for i in range(0, len(trained_model_df)):\n",
    "\n",
    "            trn_lat = trained_model_df['AvgLat'][i]\n",
    "            trn_lon = trained_model_df['AvgLon'][i]\n",
    "            if meters(trn_lat, trn_lon, new_lat, new_lon) <= 10000:\n",
    "\n",
    "                predic_df = pd.DataFrame()\n",
    "\n",
    "                cluster_id = trained_model_df['StateId'][i]\n",
    "                curr_lat = trained_model_df['AvgLat'][i]\n",
    "                curr_lon = trained_model_df['AvgLon'][i]\n",
    "                curr_add = trained_model_df['Address'][i]\n",
    "                pred_loc = {\"current\":(cluster_id, curr_lat, curr_lon, curr_add)}\n",
    "\n",
    "                from_col_no = trained_model_df['StateId'].nunique() * new_hour + 5\n",
    "                to_col_no = from_col_no + trained_model_df['StateId'].nunique()\n",
    "                predic_df = trained_model_df.iloc[i:i+1,from_col_no:to_col_no]\n",
    "                predic_df = predic_df.T\n",
    "                predic_df['StateId'] = cluster_id\n",
    "                predic_df['SelectedState'] = predic_df.index\n",
    "                predic_df['SelectedState'] = predic_df['SelectedState'].map(lambda x: x.split('-', 2)[-1])\n",
    "                predic_df.columns = ['Probability', 'StateId', 'SelectedState']\n",
    "                predic_df = predic_df.sort_values('Probability', ascending=False).head(10)\n",
    "                predic_df['Address'] = 0\n",
    "                predic_df['Latitude'] = 0.0\n",
    "                predic_df['Longitude'] = 0.0\n",
    "                predic_df = predic_df.reset_index(drop=True)\n",
    "\n",
    "                for j in range (0, len(predic_df)):\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    clus_to_find = int(float(predic_df['SelectedState'][j]))\n",
    "                    add = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'Address'].values[0]\n",
    "                    lat = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'AvgLat'].values[0]\n",
    "                    lon = trained_model_df.loc[ (trained_model_df['StateId'] == clus_to_find), 'AvgLon'].values[0]\n",
    "\n",
    "                    predic_df.loc[j, 'Address'] = add\n",
    "                    predic_df.loc[j, 'Latitude'] = lat\n",
    "                    predic_df.loc[j, 'Longitude'] = lon\n",
    "                file = dest_predicted_dir + file_name\n",
    "                predic_df.to_csv(file, sep='\\t', encoding='utf-8')\n",
    "                break\n",
    "            \n",
    "#------------------------------------------ S T A R T -----------------------------------------------\n",
    "#global dataframes used\n",
    "#user raw trajectory dataframe\n",
    "usr_trejec_df = pd.DataFrame()\n",
    "#user trained model\n",
    "trained_model_df = pd.DataFrame()\n",
    "#current hour points\n",
    "curr_hr_df = pd.DataFrame()\n",
    "#current hour staypoints\n",
    "curr_hr_staypts_df = pd.DataFrame()\n",
    "#last hour last point\n",
    "prev_hour_last_point = pd.DataFrame()\n",
    "#all staypoints\n",
    "staypts_df = pd.DataFrame()\n",
    "#hourly cluster\n",
    "cluster_hourly_df = pd.DataFrame()\n",
    "#final markov chains\n",
    "final_transition_df = pd.DataFrame()\n",
    "\n",
    "clus_dict = {}\n",
    "co_loc = {}\n",
    "pred_loc = {}\n",
    "lat_array = []\n",
    "lon_array = []\n",
    "global_count = 0\n",
    "\n",
    "def main():\n",
    "    global usr_trejec_df\n",
    "    global trained_model_df\n",
    "    global curr_hr_df\n",
    "    global curr_hr_staypts_df\n",
    "    global staypts_df\n",
    "    global cluster_hourly_df\n",
    "    global final_transition_df\n",
    "    \n",
    "    #read test user trajectory file. In real scenerio, this will be the GPS read data\n",
    "    read_usr_file()\n",
    "\n",
    "    #prepere dataframes\n",
    "    prepare_dfs()\n",
    "\n",
    "    #Save first date and time as prev date and time for the start\n",
    "    prev_date = usr_trejec_df['Date'][0]\n",
    "    prev_hour = usr_trejec_df['Hour'][0]\n",
    "\n",
    "    #I. Read the new locations in an online gps location input mode\n",
    "    #  1. Everytime the hour changes, \n",
    "    #                  A. Find staypoints for the last hour and assign staypointID\n",
    "    #                  B. Cluster staypoints based on distance for last hour, form states and assign stateID\n",
    "    #                  C. Calculate state hourly weights for last hour\n",
    "    #                  D. Predict based on trained data(if available)\n",
    "    #  2. Everytime the date changes,\n",
    "    #                  A. Add the days data into the training data\n",
    "    #  3. If the hour and the time has not been changed, add the data to current hour data\n",
    "\n",
    "    #I\n",
    "    for i in range(0, len(usr_trejec_df)):\n",
    "\n",
    "        #store the read hour and date as new hour and new date\n",
    "        new_hour = usr_trejec_df['Hour'][i]\n",
    "        new_date = usr_trejec_df['Date'][i]\n",
    "\n",
    "        #1. \n",
    "        #if the hour has changed\n",
    "        if (new_hour != prev_hour): \n",
    "            #process the last hour data if available\n",
    "            if not curr_hr_df.empty:\n",
    "                #A.\n",
    "          \n",
    "                create_last_hr_staypts() \n",
    "                add_start_end_times()\n",
    "#                if not curr_hr_staypts_df.empty:\n",
    "#                     #B.\n",
    "#                     form_states()\n",
    "#                     #C.\n",
    "#                     cal_hourly_state_weight()\n",
    "#                     #D.\n",
    "#     #                 read_trained_model()\n",
    "#     #                 if not trained_model_df.empty:\n",
    "#     #                     predict(new_hour)\n",
    "            \n",
    "            prev_hour = new_hour \n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])  \n",
    "#         #2. \n",
    "#         #if the date has changed\n",
    "#         if (new_date != prev_date):\n",
    "\n",
    "#             if not staypts_df.empty:\n",
    "#                 visualize_hourly_state_weight()\n",
    "#                 update_staypts_csv()\n",
    "#                 update_hourly_weights_csv()\n",
    "#                 #A.\n",
    "#                 create_save_seperate_trasition_matrices()\n",
    "#                 create_save_markov_chains()\n",
    "\n",
    "#             prev_date = new_date\n",
    "\n",
    "        #3. \n",
    "        #if the date and the hour has not changed, just add it to current hour dataframe.\n",
    "        # this dataframe is used once the hour is changed.\n",
    "        else:\n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])   \n",
    "            \n",
    "            \n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE INPUTS HERE-------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Edit user name, and path locations for source and destination files\n",
    "user = \"085\"\n",
    "state_d_thrhld = 200\n",
    "staypts_d_thrhld = 200\n",
    "staypts_t_thrhld = 20\n",
    "track_t_thrhld = 30\n",
    "\n",
    "#source paths\n",
    "file_source_raw = \"C:/Users/12sha/Documents/Geolife Trajectories 1.3/Data/\" + user + \"/Trajectory/20081105*.plt\"\n",
    "\n",
    "#destination paths\n",
    "base_path = r\"C:\\Users\\12sha\\Documents\\thesislocation\\code_\\stay points\\v0.6 results\"\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "#--------------------------------------CHANGE INPUTS HERE-------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "usr_directory = base_path + \"/User \" + user\n",
    "usr_hrly_wght_directory = base_path + \"/User \" + user + \"/hourlyweights\"\n",
    "usr_sty_pts_directory = base_path + \"/User \" + user + \"/staypoints\"\n",
    "usr_markov_chains_directory = base_path + \"/User \" + user + \"/markovchains\"\n",
    "dest_predicted_dir = base_path + \"/User \" + user + \"/predict/\"\n",
    "\n",
    "if not os.path.exists(usr_directory):\n",
    "    os.makedirs(usr_directory)\n",
    "if not os.path.exists(usr_hrly_wght_directory):\n",
    "    os.makedirs(usr_hrly_wght_directory)\n",
    "if not os.path.exists(usr_sty_pts_directory):\n",
    "    os.makedirs(usr_sty_pts_directory)  \n",
    "if not os.path.exists(usr_markov_chains_directory):\n",
    "    os.makedirs(usr_markov_chains_directory)  \n",
    "if not os.path.exists(dest_predicted_dir):\n",
    "    os.makedirs(dest_predicted_dir)  \n",
    "\n",
    "#destination file names\n",
    "dest_file_staypoints = usr_sty_pts_directory + \"/staypoints.csv\"\n",
    "dest_file_hourly_weights = usr_hrly_wght_directory + \"/hourlyweights.csv\"\n",
    "dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "dest_file_final_markov_chain = usr_markov_chains_directory + \"/final.csv\"\n",
    "\n",
    "#remove if the file already exists\n",
    "try:\n",
    "    os.remove(dest_file_staypoints)\n",
    "    os.remove(dest_file_hourly_weights)\n",
    "    os.remove(dest_file_final_markov_chain)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "staypts_df.to_csv(\"C:/Users/12sha/Documents/thesislocation/code_/stay points/v0.6 results/User 085/teststaynew.csv\", sep='\\t', encoding='utf-8')\n",
    "#usr_trejec_df.to_csv(\"C:/Users/12sha/Documents/thesislocation/code_/stay points/v0.6 results/User 085/testtrj.csv\", sep='\\t', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
