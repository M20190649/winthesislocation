{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math \n",
    "import os\n",
    "import matplotlib.patches as patches\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# ONLINE PROCESS\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "def read_usr_file():\n",
    "    global usr_trejec_df\n",
    "\n",
    "    # Load file names for user\n",
    "    filenames = glob.glob(file_source_raw)\n",
    "\n",
    "    # Read the files\n",
    "    list_of_dfs = [pd.read_csv(filename, skiprows=6, header=None) for filename in filenames]\n",
    "    \n",
    "    #if nothing is read from file return\n",
    "    if not list_of_dfs:\n",
    "        return\n",
    "\n",
    "    # put the data from list into one dataframe\n",
    "    usr_trejec_df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    usr_trejec_df.columns = ['Latitude', 'Longitude', '0', 'Altitude', 'NumDays', 'Date', 'Time']\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Date\"].map(str) + \" \" + usr_trejec_df[\"Time\"]\n",
    "\n",
    "    usr_trejec_df.Timestamp = pd.to_datetime(usr_trejec_df.Timestamp)\n",
    "\n",
    "    # convert to China's local time GMT + 8 (Note few users are not from China, they will be affected)\n",
    "    usr_trejec_df[\"Timestamp\"] = usr_trejec_df[\"Timestamp\"] + pd.to_timedelta(8, unit='h')\n",
    "\n",
    "    usr_trejec_df.index = usr_trejec_df['Timestamp']\n",
    "    usr_trejec_df = usr_trejec_df.resample('1T').mean()\n",
    "    usr_trejec_df = usr_trejec_df.dropna()\n",
    "\n",
    "    # add columns to user trajectory dataframe\n",
    "    # 1. add timestamp as column\n",
    "    usr_trejec_df['Timestamp'] = pd.to_datetime(usr_trejec_df.index)\n",
    "    # restore date and time column\n",
    "    usr_trejec_df['Date'] = usr_trejec_df.Timestamp.dt.date\n",
    "    usr_trejec_df['Time'] = usr_trejec_df.Timestamp.dt.time\n",
    "    usr_trejec_df['Hour'] = usr_trejec_df.Timestamp.dt.hour\n",
    "\n",
    "    # sort values based on timestamp\n",
    "    usr_trejec_df = usr_trejec_df.sort_values(['Timestamp'])\n",
    "    # reset index\n",
    "    usr_trejec_df = usr_trejec_df.reset_index(drop=True)\n",
    "\n",
    "    usr_trejec_df['Weekday'] = usr_trejec_df['Timestamp'].dt.weekday.map(str) + usr_trejec_df[\n",
    "        'Timestamp'].dt.weekday_name\n",
    "\n",
    "    # set other columns with default value\n",
    "    usr_trejec_df['StayPoint'] = -1    # 1 if it is a staypoint, else 0\n",
    "    usr_trejec_df['StayptId'] = -1\n",
    "    usr_trejec_df['StayMeanLat'] = -1.0\n",
    "    usr_trejec_df['StayMeanLon'] = -1.0\n",
    "    usr_trejec_df['State'] = -1        # 1 if it is a state, else 0\n",
    "    usr_trejec_df['StateId'] = -1\n",
    "    usr_trejec_df['StateMeanLat'] = -1.0\n",
    "    usr_trejec_df['StateMeanLon'] = -1.0\n",
    "\n",
    "    # remove columns not used/required\n",
    "    usr_trejec_df = usr_trejec_df.drop(['0', 'Altitude', 'NumDays'], axis=1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "def prepare_dfs():\n",
    "    global cluster_hourly_df\n",
    "\n",
    "    # create cluster_hourly_df columns\n",
    "    for i in range(0, 24):\n",
    "        cluster_hourly_df['Date'] = 0\n",
    "        cluster_hourly_df['StateId'] = 0\n",
    "        cluster_hourly_df['AvgLat'] = 0\n",
    "        cluster_hourly_df['AvgLon'] = 0\n",
    "        cluster_hourly_df[i] = 0\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Find distance between two lan:lon points in meters\n",
    "def meters(lat1, lon1, lat2, lon2):\n",
    "    R = 6378.137  # Radius of earth in KM\n",
    "    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180\n",
    "    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180\n",
    "    a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(lat1 * math.pi / 180) * math.cos(\n",
    "        lat2 * math.pi / 180) * math.sin(dLon / 2) * math.sin(dLon / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    d = R * c\n",
    "    return d * 1000  # meters\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "def cluster(newlat, newlon, row, count, orig_lat, orig_lon):\n",
    "    global curr_hr_df\n",
    "\n",
    "    # set the new point as not staypoint\n",
    "    currcluster = curr_hr_df['StayptId'][row - 1]\n",
    "    curr_hr_df['StayptId'][row] = -1\n",
    "    curr_hr_df['StayMeanLat'][row] = -1.0\n",
    "    curr_hr_df['StayMeanLon'][row] = -1.0\n",
    "    curr_hr_df['StayPoint'][row] = -1\n",
    "    clulat = curr_hr_df['StayMeanLat'][row - 1]\n",
    "    clulon = curr_hr_df['StayMeanLon'][row - 1]\n",
    "\n",
    "    # if the new point and the old point time difference is greater than tracking threshold\n",
    "    # then add both the points as staypoints and leave\n",
    "    prevPointTime = curr_hr_df['Timestamp'][row - 1]\n",
    "    currPointTime = curr_hr_df['Timestamp'][row]\n",
    "    timm_diff = (currPointTime - prevPointTime).seconds / 60\n",
    "    dist_diff = meters(clulat, clulon, newlat, newlon)\n",
    "\n",
    "    if (timm_diff >= track_t_thrhld) and (dist_diff > staypts_d_thrhld):\n",
    "        curr_hr_df.loc[row - 1, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row - 1, 'StayMeanLat'] = curr_hr_df.loc[row - 1, 'Latitude']\n",
    "        curr_hr_df.loc[row - 1, 'StayMeanLon'] = curr_hr_df.loc[row - 1, 'Longitude']\n",
    "        curr_hr_df.loc[row, 'StayPoint'] = 1\n",
    "        curr_hr_df.loc[row, 'StayMeanLat'] = curr_hr_df.loc[row, 'Latitude']\n",
    "        curr_hr_df.loc[row, 'StayMeanLon'] = curr_hr_df.loc[row, 'Longitude']\n",
    "        curr_hr_df.loc[row, 'StayptId'] = currcluster + 1\n",
    "    else:\n",
    "        # if the new point and old point's distance is less than threshold, then add it to current cluster\n",
    "        if meters(clulat, clulon, newlat, newlon) <= staypts_d_thrhld:\n",
    "            curr_hr_df['StayptId'][row] = currcluster\n",
    "            # calculate new mean lat and lon for the cluster\n",
    "            array_lat = curr_hr_df['Latitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "            array_lon = curr_hr_df['Longitude'].loc[curr_hr_df['StayptId'] == currcluster].values\n",
    "\n",
    "            # cal new means\n",
    "            new_lat_mean = np.mean(array_lat)\n",
    "            new_lon_mean = np.mean(array_lon)\n",
    "\n",
    "            curr_hr_df.loc[(curr_hr_df['StayptId'] == currcluster), 'StayMeanLat'] = new_lat_mean\n",
    "            curr_hr_df.loc[(curr_hr_df['StayptId'] == currcluster), 'StayMeanLon'] = new_lon_mean\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        # if the new point and old point's distance is greater than threshold, it means the point moved away\n",
    "        # if the previous cluster has more than two points, check the duration of the previous cluster\n",
    "        #   if the duration of the previous cluster is greater than threshold, assign it as a staypoint\n",
    "\n",
    "        # if the row read is the last row for this hour\n",
    "        if (row == len(curr_hr_df) - 1):\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row - count + 1]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[(curr_hr_df['StayptId'] == currcluster), 'StayPoint'] = 1\n",
    "                    # in case the cluster is not a staypoint and the first point is already a staypoint\n",
    "                    # then retain the latitudes and longitudes\n",
    "                else:\n",
    "                    if (row - count) == 0 and curr_hr_df['StayPoint'][row - count] == 1:\n",
    "                        curr_hr_df['StayMeanLat'][row - count] = orig_lat\n",
    "                        curr_hr_df['StayMeanLon'][row - count] = orig_lon\n",
    "\n",
    "        # if the new point is moving away from the cluster\n",
    "        if meters(clulat, clulon, newlat, newlon) > staypts_d_thrhld:\n",
    "            if count >= 2:\n",
    "                MinClusTime = curr_hr_df['Timestamp'][row - count]\n",
    "                MaxClusTime = curr_hr_df['Timestamp'][row - 1]\n",
    "                k = MaxClusTime - MinClusTime\n",
    "                l = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                if (l >= staypts_t_thrhld):\n",
    "                    curr_hr_df.loc[(curr_hr_df['StayptId'] == currcluster), 'StayPoint'] = 1\n",
    "                # incase the cluster is not a staypoint and the first point is already a staypoint\n",
    "                # then retain the latitudes and longitudes\n",
    "                else:\n",
    "                    if (row - count) == 0 and curr_hr_df['StayPoint'][row - count] == 1:\n",
    "                        curr_hr_df['StayMeanLat'][row - count] = orig_lat\n",
    "                        curr_hr_df['StayMeanLon'][row - count] = orig_lon\n",
    "\n",
    "            count = 1\n",
    "            curr_hr_df['StayMeanLat'][row] = curr_hr_df['Latitude'][row]\n",
    "            curr_hr_df['StayMeanLon'][row] = curr_hr_df['Longitude'][row]\n",
    "            curr_hr_df['StayptId'][row] = currcluster + 1\n",
    "\n",
    "    return count\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "def create_last_hr_staypts():\n",
    "    global curr_hr_df\n",
    "    global staypts_df\n",
    "    global curr_hr_staypts_df\n",
    "    global prev_hour_last_point\n",
    "\n",
    "    # clear current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_staypts_df.iloc[0:0]\n",
    "\n",
    "    # reset index of current hour points\n",
    "    curr_hr_df = curr_hr_df.reset_index(drop=True)\n",
    "\n",
    "    # fetching the last stayptid\n",
    "    if not staypts_df.empty:\n",
    "        stayid = staypts_df['StayptId'].max() + 1   # assign next possible staypt id\n",
    "    else:\n",
    "        stayid = 1                                  # if this is the start, start from 1 as staypt id\n",
    "\n",
    "    adding_first_as_staypt = 'N'\n",
    "    orig_lat = 0\n",
    "    orig_lon = 0\n",
    "    if not prev_hour_last_point.empty:\n",
    "        # check the time difference of last hour last point and this hour first point is greater than track t threshold\n",
    "        # if yes than add both as staypoints\n",
    "        if (int(time.mktime(curr_hr_df.loc[0, 'Timestamp'].timetuple()) -\n",
    "                time.mktime(prev_hour_last_point.loc[0, 'Timestamp'].timetuple())) / 60 > track_t_thrhld):\n",
    "            if prev_hour_last_point.loc[0, 'StayPoint'] != 1:\n",
    "                prev_hour_last_point.loc[0, 'StayptId'] = stayid\n",
    "                stayid = stayid + 1\n",
    "                prev_hour_last_point.loc[0, 'StayPoint'] = 1\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLat'] = prev_hour_last_point.loc[0, 'Latitude']\n",
    "                prev_hour_last_point.loc[0, 'StayMeanLon'] = prev_hour_last_point.loc[0, 'Longitude']\n",
    "                staypts_df = staypts_df.append(prev_hour_last_point)\n",
    "\n",
    "            curr_hr_df.loc[0, 'StayptId'] = stayid\n",
    "            stayid = stayid + 1\n",
    "            curr_hr_df.loc[0, 'StayPoint'] = 1\n",
    "            orig_lat = curr_hr_df.loc[0, 'Latitude']\n",
    "            orig_lon = curr_hr_df.loc[0, 'Longitude']\n",
    "            adding_first_as_staypt = 'Y'\n",
    "\n",
    "    row = 1\n",
    "    count = 1\n",
    "\n",
    "    if adding_first_as_staypt == 'N':\n",
    "        curr_hr_df['StayptId'][row - 1] = stayid\n",
    "        curr_hr_df['StayPoint'][row - 1] = -1\n",
    "\n",
    "    curr_hr_df['StayMeanLat'][row - 1] = curr_hr_df['Latitude'][0]\n",
    "    curr_hr_df['StayMeanLon'][row - 1] = curr_hr_df['Longitude'][0]\n",
    "\n",
    "    # Read the file in an online manner as the points come and assign the points to clusters\n",
    "    while row < len(curr_hr_df):\n",
    "        count = cluster(curr_hr_df['Latitude'][row], curr_hr_df['Longitude'][row], row, count, orig_lat, orig_lon)\n",
    "        row = row + 1\n",
    "\n",
    "    # copy the staypoints to the current hour staypoints dataframe\n",
    "    curr_hr_staypts_df = curr_hr_df.loc[curr_hr_df['StayPoint'] == 1]\n",
    "    # copy the stay points into another dataframe\n",
    "    staypts_df = staypts_df.append(curr_hr_df.loc[curr_hr_df['StayPoint'] == 1])\n",
    "    # reset staypoints index\n",
    "    curr_hr_staypts_df.index = curr_hr_staypts_df['Timestamp']\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "\n",
    "    # store the last hour last point\n",
    "    prev_hour_last_point = prev_hour_last_point.iloc[0:0]\n",
    "    prev_hour_last_point = curr_hr_df.iloc[[len(curr_hr_df) - 1]]\n",
    "    prev_hour_last_point = prev_hour_last_point.reset_index(drop=True)\n",
    "\n",
    "    # clear current hour dataframe content\n",
    "    curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "def add_start_end_times():\n",
    "    global staypts_df\n",
    "\n",
    "    # if no staypoints are found, then leave\n",
    "    if staypts_df.empty or len(staypts_df) == 1:\n",
    "        return\n",
    "\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "\n",
    "    # state = -2 indicates it is already processed\n",
    "    idx = staypts_df.index[staypts_df['State'] == -1]\n",
    "    if idx.empty:\n",
    "        return\n",
    "\n",
    "    # get the starting row where to add the time\n",
    "    if min(idx) == 0:\n",
    "        start = min(idx)\n",
    "    else:\n",
    "        start = min(idx) - 1\n",
    "\n",
    "    prev_id = staypts_df.loc[start, 'StayptId']\n",
    "    tobeadded_staypts = pd.DataFrame(columns=['Latitude', 'Longitude', 'Timestamp', 'Date', 'Time',\n",
    "                                              'Hour', 'Weekday', 'StayPoint', 'StayptId', 'StayMeanLat',\n",
    "                                              'StayMeanLon', 'State', 'StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    j = 0\n",
    "    for i in range(start, len(staypts_df)):\n",
    "        # update state as -2 indicating processed\n",
    "        staypts_df.loc[i, 'State'] = -2\n",
    "\n",
    "        if staypts_df.loc[i, 'StayptId'] != prev_id:\n",
    "            prev_id = staypts_df.loc[i, 'StayptId']\n",
    "            strt_indx = i\n",
    "            end_indx = i - 1\n",
    "\n",
    "            # calculate time to be added\n",
    "            end1_trj_time = staypts_df.loc[end_indx, 'Timestamp']\n",
    "            end1_trj_lat = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "            end1_trj_lon = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "            str2_trj_time = staypts_df.loc[strt_indx, 'Timestamp']\n",
    "            str2_trj_lat = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "            str2_trj_lon = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "\n",
    "            dist_btw = meters(end1_trj_lat, end1_trj_lon, str2_trj_lat, str2_trj_lon)\n",
    "            time_btw = (str2_trj_time - end1_trj_time).seconds / 60\n",
    "\n",
    "            if time_btw != 0:\n",
    "                avg_speed = dist_btw / time_btw\n",
    "\n",
    "                # if the distance between two points is less than 2*state_d_thrhld, that mean there is an overlap\n",
    "                # in this case, we cannot consider state_d_thrhld as the staypoint region, as:\n",
    "                # before user leave state_d_thrhld of this staypoint the user already enters the next staypoint\n",
    "                if avg_speed != 0:\n",
    "\n",
    "                    if dist_btw >= 2 * state_d_thrhld:\n",
    "                        delta_t = min(state_d_thrhld, dist_btw) / avg_speed\n",
    "                    else:\n",
    "                        delta_t = dist_btw / (2 * avg_speed)\n",
    "                else:\n",
    "                    delta_t = time_btw / 2\n",
    "\n",
    "                if delta_t <= 1440:\n",
    "                    end1_trj_time = end1_trj_time + timedelta(minutes=delta_t)\n",
    "                    str2_trj_time = str2_trj_time - timedelta(minutes=delta_t)\n",
    "\n",
    "                    # add end of prev point line\n",
    "                    tobeadded_staypts.loc[j, 'Latitude'] = staypts_df.loc[end_indx, 'Latitude']\n",
    "                    tobeadded_staypts.loc[j, 'Longitude'] = staypts_df.loc[end_indx, 'Longitude']\n",
    "                    tobeadded_staypts.loc[j, 'Timestamp'] = end1_trj_time\n",
    "                    tobeadded_staypts.loc[j, 'Date'] = end1_trj_time.date()\n",
    "                    tobeadded_staypts.loc[j, 'Time'] = end1_trj_time.time()\n",
    "                    tobeadded_staypts.loc[j, 'Hour'] = end1_trj_time.hour\n",
    "                    tobeadded_staypts.loc[j, 'Weekday'] = str(end1_trj_time.weekday()) + end1_trj_time.weekday_name\n",
    "                    tobeadded_staypts.loc[j, 'StayPoint'] = 1\n",
    "                    tobeadded_staypts.loc[j, 'StayptId'] = staypts_df.loc[end_indx, 'StayptId']\n",
    "                    tobeadded_staypts.loc[j, 'StayMeanLat'] = staypts_df.loc[end_indx, 'StayMeanLat']\n",
    "                    tobeadded_staypts.loc[j, 'StayMeanLon'] = staypts_df.loc[end_indx, 'StayMeanLon']\n",
    "                    tobeadded_staypts.loc[j, 'State'] = -2  # to indicate its po´rocessed\n",
    "                    tobeadded_staypts.loc[j, 'StateId'] = -1\n",
    "                    tobeadded_staypts.loc[j, 'StateMeanLat'] = -1.0\n",
    "                    tobeadded_staypts.loc[j, 'StateMeanLon'] = -1.0\n",
    "                    j = j + 1\n",
    "\n",
    "                    # add start of current point line\n",
    "                    tobeadded_staypts.loc[j, 'Latitude'] = staypts_df.loc[strt_indx, 'Latitude']\n",
    "                    tobeadded_staypts.loc[j, 'Longitude'] = staypts_df.loc[strt_indx, 'Longitude']\n",
    "                    tobeadded_staypts.loc[j, 'Timestamp'] = str2_trj_time\n",
    "                    tobeadded_staypts.loc[j, 'Date'] = str2_trj_time.date()\n",
    "                    tobeadded_staypts.loc[j, 'Time'] = str2_trj_time.time()\n",
    "                    tobeadded_staypts.loc[j, 'Hour'] = str2_trj_time.hour\n",
    "                    tobeadded_staypts.loc[j, 'Weekday'] = str(str2_trj_time.weekday()) + str2_trj_time.weekday_name\n",
    "                    tobeadded_staypts.loc[j, 'StayPoint'] = 1\n",
    "                    tobeadded_staypts.loc[j, 'StayptId'] = staypts_df.loc[strt_indx, 'StayptId']\n",
    "                    tobeadded_staypts.loc[j, 'StayMeanLat'] = staypts_df.loc[strt_indx, 'StayMeanLat']\n",
    "                    tobeadded_staypts.loc[j, 'StayMeanLon'] = staypts_df.loc[strt_indx, 'StayMeanLon']\n",
    "                    tobeadded_staypts.loc[j, 'State'] = -2  # to indicate its po´rocessed\n",
    "                    tobeadded_staypts.loc[j, 'StateId'] = -1\n",
    "                    tobeadded_staypts.loc[j, 'StateMeanLat'] = -1.0\n",
    "                    tobeadded_staypts.loc[j, 'StateMeanLon'] = -1.0\n",
    "                    j = j + 1\n",
    "\n",
    "    staypts_df = staypts_df.append(tobeadded_staypts, ignore_index=True)\n",
    "    staypts_df = staypts_df.sort_values(['StayptId', 'Timestamp'])\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "\n",
    "    staypts_df.index = staypts_df['Timestamp']\n",
    "\n",
    "\n",
    "# -------------form states-----------------------------------------------------------------------\n",
    "def form_states():\n",
    "    global staypts_df\n",
    "    global curr_hr_staypts_df\n",
    "\n",
    "    # update states in final staypoints\n",
    "    # copy staypoint data as state data\n",
    "    staypts_df['StateId'] = staypts_df['StayptId']\n",
    "    staypts_df['StateMeanLat'] = staypts_df['StayMeanLat']\n",
    "    staypts_df['StateMeanLon'] = staypts_df['StayMeanLon']\n",
    "\n",
    "    # this function groups the staypoints together to from different days\n",
    "    # Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    staypts_df1 = staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    staypts_df1 = staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    staypts_df1 = staypts_df1.sort_values(['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    staypts_df1 = staypts_df1.reset_index(drop=True)\n",
    "\n",
    "    for i in range(0, len(staypts_df1) - 1):\n",
    "        for j in range(i + 1, len(staypts_df1)):\n",
    "\n",
    "            chk_cluster = staypts_df1['StateId'][i]\n",
    "            chk_clulat = staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = staypts_df1['StateId'][j]\n",
    "            curr_clulat = staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = staypts_df1['StateMeanLon'][j]\n",
    "\n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon) <= state_d_thrhld:\n",
    "                # before adding this point to the ith state,\n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "\n",
    "                add_state = \"Yes\"\n",
    "                # form the existing lat and lon array\n",
    "                array_lat = staypts_df['Latitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = staypts_df['Longitude'].loc[staypts_df['StateId'] == chk_cluster].values\n",
    "                # add the new lat and lon values to the array\n",
    "                new_lats = staypts_df['Latitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = staypts_df['Longitude'].loc[staypts_df['StateId'] == curr_cluster].values\n",
    "\n",
    "                array_lat = np.append(array_lat, new_lats)\n",
    "                array_lon = np.append(array_lon, new_lons)\n",
    "                # cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "\n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "\n",
    "                if add_state == \"Yes\":\n",
    "                    staypts_df.loc[(staypts_df['StateId'] == curr_cluster), 'StateId'] = chk_cluster\n",
    "                    staypts_df.loc[(staypts_df['StateId'] == chk_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    staypts_df.loc[(staypts_df['StateId'] == chk_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "\n",
    "    # update states for last hour staypoints\n",
    "    # copy staypoint data as state data\n",
    "    curr_hr_staypts_df['StateId'] = curr_hr_staypts_df['StayptId']\n",
    "    curr_hr_staypts_df['StateMeanLat'] = curr_hr_staypts_df['StayMeanLat']\n",
    "    curr_hr_staypts_df['StateMeanLon'] = curr_hr_staypts_df['StayMeanLon']\n",
    "\n",
    "    # this function groups the staypoints together to from different days\n",
    "    # Copy the stay points dataframe into another dataframe and remove duplicates\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df[['StateId', 'StateMeanLat', 'StateMeanLon']].copy()\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.drop_duplicates(subset=['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.sort_values(['StateId', 'StateMeanLat', 'StateMeanLon'])\n",
    "    curr_hr_staypts_df1 = curr_hr_staypts_df1.reset_index(drop=True)\n",
    "\n",
    "    for i in range(0, len(curr_hr_staypts_df1)):\n",
    "        for j in range(i + 1, len(curr_hr_staypts_df1)):\n",
    "\n",
    "            chk_cluster = curr_hr_staypts_df1['StateId'][i]\n",
    "            chk_clulat = curr_hr_staypts_df1['StateMeanLat'][i]\n",
    "            chk_clulon = curr_hr_staypts_df1['StateMeanLon'][i]\n",
    "            curr_cluster = curr_hr_staypts_df1['StateId'][j]\n",
    "            curr_clulat = curr_hr_staypts_df1['StateMeanLat'][j]\n",
    "            curr_clulon = curr_hr_staypts_df1['StateMeanLon'][j]\n",
    "\n",
    "            if meters(chk_clulat, chk_clulon, curr_clulat, curr_clulon) <= state_d_thrhld:\n",
    "                # before adding this point to the ith state,\n",
    "                #   calculate new mean with jth point,\n",
    "                #   if the new mean is still keeping all the states with id(i) than add jth to the state\n",
    "                #   else not\n",
    "\n",
    "                add_state = \"Yes\"\n",
    "                # form the existing lat and lon array\n",
    "                array_lat = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                array_lon = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == chk_cluster].values\n",
    "                # add the new lat and lon values to the array\n",
    "                new_lats = curr_hr_staypts_df['Latitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "                new_lons = curr_hr_staypts_df['Longitude'].loc[curr_hr_staypts_df['StateId'] == curr_cluster].values\n",
    "\n",
    "                array_lat = np.append(array_lat, new_lats)\n",
    "                array_lon = np.append(array_lon, new_lons)\n",
    "                # cal new means\n",
    "                new_lat_mean = np.mean(array_lat)\n",
    "                new_lon_mean = np.mean(array_lon)\n",
    "\n",
    "                for k in range(0, len(array_lat)):\n",
    "                    if meters(array_lat[k], array_lon[k], new_lat_mean, new_lon_mean) > state_d_thrhld:\n",
    "                        add_state = \"No\"\n",
    "\n",
    "                if add_state == \"Yes\":\n",
    "                    curr_hr_staypts_df.loc[(curr_hr_staypts_df['StateId'] == curr_cluster), 'StateId'] = chk_cluster\n",
    "                    curr_hr_staypts_df.loc[\n",
    "                        (curr_hr_staypts_df['StateId'] == curr_cluster), 'StateMeanLat'] = new_lat_mean\n",
    "                    curr_hr_staypts_df.loc[\n",
    "                        (curr_hr_staypts_df['StateId'] == curr_cluster), 'StateMeanLon'] = new_lon_mean\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "def cal_hourly_state_weight():\n",
    "    global cluster_hourly_df\n",
    "    global staypts_df\n",
    "\n",
    "    curr_hr_cluster_hourly_df = pd.DataFrame()\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(drop=True)\n",
    "    staypts_df = staypts_df.reset_index(drop=True)\n",
    "\n",
    "    last_clusid = staypts_df['StateId'][0]\n",
    "    row = 0\n",
    "    currstate_timestamps = []\n",
    "    currstate_timestamps = np.append(currstate_timestamps, staypts_df.loc[0, 'Timestamp'])\n",
    "\n",
    "    # create an empty time slotted data or hourly state weights\n",
    "    for i in range(0, 24):\n",
    "        curr_hr_cluster_hourly_df['Date'] = 0\n",
    "        curr_hr_cluster_hourly_df['StateId'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLat'] = 0\n",
    "        curr_hr_cluster_hourly_df['AvgLon'] = 0\n",
    "        curr_hr_cluster_hourly_df[i] = 0\n",
    "\n",
    "    # populate time slotted data or hourly state weights\n",
    "    for i in range(1, len(staypts_df)):\n",
    "\n",
    "        if (staypts_df['StateId'][i] != last_clusid):\n",
    "            start = min(currstate_timestamps)\n",
    "            end = max(currstate_timestamps)\n",
    "\n",
    "            # if the state is with one hour\n",
    "            if start.hour == end.hour:\n",
    "                k = end - start\n",
    "                mins = int((k / np.timedelta64(1, 'm')))\n",
    "\n",
    "                date_read = start.date()\n",
    "                cluster_id = staypts_df['StateId'][i - 1]\n",
    "                cluster_mean_lat = staypts_df['StateMeanLat'][i - 1]\n",
    "                cluster_mean_lon = staypts_df['StateMeanLon'][i - 1]\n",
    "                col_name = start.hour\n",
    "\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'AvgLat'] = cluster_mean_lat\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'AvgLon'] = cluster_mean_lon\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'Date'] = date_read\n",
    "                curr_hr_cluster_hourly_df.loc[row, 'StateId'] = cluster_id\n",
    "                curr_hr_cluster_hourly_df.loc[row, col_name] = round((mins) / 60, 4)\n",
    "                row = row + 1\n",
    "\n",
    "            # if the state is beyond one hour boundary\n",
    "            else:\n",
    "                end = end + pd.Timedelta(hours=1) - pd.Timedelta(minutes=end.minute)\n",
    "                j = start\n",
    "                while j < end:\n",
    "                    if j == start:\n",
    "                        k = ((j + pd.Timedelta(hours=1) - pd.Timedelta(minutes=j.minute)) - j)\n",
    "                        mins = int((k / np.timedelta64(1, 'm')))\n",
    "                    elif j.hour == end.hour - 1:\n",
    "                        end_time = max(currstate_timestamps)\n",
    "                        k = (end_time - (end_time - pd.Timedelta(minutes=end_time.minute)))\n",
    "                        mins = int((k / np.timedelta64(1, 'm')))\n",
    "                    else:\n",
    "                        mins = 60\n",
    "\n",
    "                    date_read = j.date()\n",
    "                    cluster_id = staypts_df['StateId'][i - 1]\n",
    "                    cluster_mean_lat = staypts_df['StateMeanLat'][i - 1]\n",
    "                    cluster_mean_lon = staypts_df['StateMeanLon'][i - 1]\n",
    "                    col_name = j.hour\n",
    "\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'AvgLat'] = cluster_mean_lat\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'AvgLon'] = cluster_mean_lon\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'Date'] = date_read\n",
    "                    curr_hr_cluster_hourly_df.loc[row, 'StateId'] = cluster_id\n",
    "                    curr_hr_cluster_hourly_df.loc[row, col_name] = round((mins) / 60, 4)\n",
    "                    row = row + 1\n",
    "\n",
    "                    j = j + pd.Timedelta(hours=1)\n",
    "\n",
    "            currstate_timestamps = []\n",
    "            currstate_timestamps = np.append(currstate_timestamps, staypts_df.loc[i, 'Timestamp'])\n",
    "            last_clusid = staypts_df['StateId'][i]\n",
    "        else:\n",
    "            currstate_timestamps = np.append(currstate_timestamps, staypts_df.loc[i, 'Timestamp'])\n",
    "\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.fillna(0)\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.groupby(['Date', 'StateId', 'AvgLat', 'AvgLon']).sum()\n",
    "    curr_hr_cluster_hourly_df = curr_hr_cluster_hourly_df.reset_index(level=[0, 1, 2, 3])\n",
    "\n",
    "    cluster_hourly_df = curr_hr_cluster_hourly_df\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(drop=True)\n",
    "\n",
    "    # normalize the hourly weights\n",
    "    for i in range(0, 24):\n",
    "        col = \"Sum\" + str(i)\n",
    "        cluster_hourly_df[col] = cluster_hourly_df.groupby('Date')[i].transform(np.sum)\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        col = \"Sum\" + str(i)\n",
    "        cluster_hourly_df[i] = cluster_hourly_df[i] / cluster_hourly_df[col]\n",
    "        cluster_hourly_df = cluster_hourly_df.drop([col], axis=1)\n",
    "    cluster_hourly_df = cluster_hourly_df.fillna(0)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "def visualize_hourly_state_weight():\n",
    "    global cluster_hourly_df\n",
    "\n",
    "    dicts = {}\n",
    "    pos_dicts = {}\n",
    "    clu_list = cluster_hourly_df['StateId'].unique()\n",
    "    r = lambda: random.randint(0, 255)\n",
    "\n",
    "    for i in range(0, len(clu_list)):\n",
    "        dicts[clu_list[i]] = ('#%02X%02X%02X' % (r(), r(), r()))\n",
    "\n",
    "    # create a new graph where we will later add rectangles for each hour:cluster\n",
    "    fig2 = plt.figure(figsize=(18, 18))\n",
    "    ax1 = fig2.add_subplot(111, aspect='equal')\n",
    "\n",
    "    # get all the dates for y axis\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    days_diff = (max(date_list) - min(date_list)).days\n",
    "    date_range = [min(date_list) + dt.timedelta(days=x) for x in range(0, days_diff)]\n",
    "    y = range(0, len(date_range))\n",
    "    def_yticks = date_range\n",
    "    plt.yticks(y, def_yticks)\n",
    "\n",
    "    # set the x axis limit from 0-24 hours of a day, y axis with dates\n",
    "    limsx = (0, 24)\n",
    "    limsy = (0, len(date_range))\n",
    "\n",
    "    date_counter = 0\n",
    "    last_date = cluster_hourly_df['Date'][0]\n",
    "    j = 0\n",
    "\n",
    "    # drawing vertical lines for each hour\n",
    "    for i in range(0, 24):\n",
    "        ax1.axvline(x=i, linewidth=1, color='r')\n",
    "    # horizontal lines for each day\n",
    "    for i in range(0, len(date_range)):\n",
    "        ax1.axhline(y=i, linewidth=1, color='r')\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        pos_dicts[i] = i\n",
    "\n",
    "    for i in range(0, len(cluster_hourly_df)):\n",
    "\n",
    "        if cluster_hourly_df.loc[i, 'Date'] != last_date:\n",
    "            for k in range(0, 24):\n",
    "                pos_dicts[k] = k\n",
    "            add_days = (cluster_hourly_df['Date'][i] - last_date).days\n",
    "            date_counter = date_counter + add_days\n",
    "            last_date = cluster_hourly_df['Date'][i]\n",
    "\n",
    "        for j in range(0, 24):\n",
    "\n",
    "            a = float(pos_dicts.get(j))\n",
    "            b = a + cluster_hourly_df.loc[i, j]\n",
    "            pos_dicts[j] = b\n",
    "            width = b - a\n",
    "            height = 1\n",
    "            if width != 0:\n",
    "                col_id = dicts.get(cluster_hourly_df['StateId'][i])\n",
    "                # draw rectangle from a to b with height 1\n",
    "                ax1.add_patch(patches.Rectangle((a, date_counter), width, height, color=col_id,\n",
    "                                                label=cluster_hourly_df['StateId'][i]))\n",
    "                # annotate the rectangle with stateid\n",
    "                ax1.annotate(int(cluster_hourly_df['StateId'][i]), (a + width / 2, height / 2 + date_counter),\n",
    "                             color='w', weight='bold', fontsize=15, ha='center', va='center')\n",
    "    plt.xlim(limsx)\n",
    "    plt.ylim(limsy)\n",
    "    plt.title(user)\n",
    "    destpng = usr_directory + \"/online.png\"\n",
    "    plt.savefig(destpng)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "def update_staypts_csv():\n",
    "    staypts_df.to_csv(dest_file_staypoints, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "def update_hourly_weights_csv():\n",
    "    cluster_hourly_df.to_csv(dest_file_hourly_weights, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "def combine_states():\n",
    "    global cluster_hourly_df\n",
    "\n",
    "    # combine the states if the are close to each other geographically\n",
    "    for i in range(0, len(cluster_hourly_df)):\n",
    "        for j in range(i + 1, len(cluster_hourly_df)):\n",
    "            if (cluster_hourly_df.loc[i, 'StateId'] != cluster_hourly_df.loc[j, 'StateId']) and (\n",
    "                    meters(cluster_hourly_df.loc[i, 'AvgLat'], cluster_hourly_df.loc[i, 'AvgLon'],\n",
    "                           cluster_hourly_df.loc[j, 'AvgLat'], cluster_hourly_df.loc[j, 'AvgLon']) <= staypts_d_thrhld):\n",
    "                state_id = cluster_hourly_df.loc[i, 'StateId']\n",
    "                mean_lat = (cluster_hourly_df.loc[i, 'AvgLat'] + cluster_hourly_df.loc[j, 'AvgLat']) / 2\n",
    "                mean_lon = (cluster_hourly_df.loc[i, 'AvgLon'] + cluster_hourly_df.loc[j, 'AvgLon']) / 2\n",
    "                cluster_hourly_df.loc[\n",
    "                    (cluster_hourly_df['StateId'] == cluster_hourly_df.loc[j, 'StateId']), 'StateId'] = state_id\n",
    "                cluster_hourly_df.loc[(cluster_hourly_df['StateId'] == state_id), 'AvgLat'] = mean_lat\n",
    "                cluster_hourly_df.loc[(cluster_hourly_df['StateId'] == state_id), 'AvgLon'] = mean_lon\n",
    "\n",
    "    cluster_hourly_df = cluster_hourly_df.groupby(['Date', 'StateId', 'AvgLat', 'AvgLon']).sum()\n",
    "    cluster_hourly_df = cluster_hourly_df.reset_index(level=[0, 1, 2, 3])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "def change_ids():\n",
    "    global cluster_hourly_df\n",
    "    state_dict = {}\n",
    "    \n",
    "    state_list = cluster_hourly_df['StateId'].unique()\n",
    "    \n",
    "    # assign new ids as from 1 to t states in incremental number\n",
    "    for i in range(0, len(state_list)):\n",
    "        state_dict[state_list[i]] = i + 1\n",
    "\n",
    "    for i in range(0, len(cluster_hourly_df)):\n",
    "        new_id = state_dict.get(cluster_hourly_df.loc[i, 'StateId'])\n",
    "        cluster_hourly_df.loc[i, 'StateId'] = new_id\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "def create_save_seperate_trasition_matrices():\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "\n",
    "    # create a temp dataframe for each data, and calculate transition matrices from hour t to t+1\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "\n",
    "        # create a temp dataframe for previous date\n",
    "        matrices_df = pd.DataFrame()\n",
    "        temp_df = cluster_hourly_df.loc[cluster_hourly_df['Date'] == date_list[p]]\n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "\n",
    "        for i in range(0, 24):\n",
    "            matrices_df['Date'] = 0\n",
    "            matrices_df['StateId'] = 0\n",
    "            for j in range(0, len(temp_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i + 1) + ')-' + str(temp_df['StateId'][j])\n",
    "                matrices_df[colname] = 0\n",
    "\n",
    "        matrices_df['Date'] = temp_df['Date']\n",
    "        matrices_df['StateId'] = temp_df['StateId']\n",
    "\n",
    "        for i in range(0, 23):\n",
    "            for j in range(0, len(temp_df)):\n",
    "                for k in range(0, len(temp_df)):\n",
    "                    prob = temp_df[i][j] * temp_df[i + 1][k]\n",
    "                    colname = '(' + str(i + 1) + '-' + str(i + 2) + ')-' + str(temp_df['StateId'][k])\n",
    "                    matrices_df[colname][j] = prob\n",
    "\n",
    "        for j in range(0, len(temp_df)):\n",
    "            for k in range(0, len(temp_df)):\n",
    "                prob = temp_df[23][j] * temp_df[0][k]\n",
    "                colname = '(' + str(0) + '-' + str(1) + ')-' + str(temp_df['StateId'][k])\n",
    "                matrices_df[colname][j] = prob\n",
    "\n",
    "        file_name = dest_path_each_day_trsn_mat + str(date_list[p]) + \".csv\"\n",
    "        matrices_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "def create_save_markov_chains():\n",
    "    global final_transition_df\n",
    "    global co_loc\n",
    "\n",
    "    final_transition_df = pd.DataFrame()\n",
    "\n",
    "    # create an empty markov chain frame for each state, and transition for each hour of the day\n",
    "    cluster_list = cluster_hourly_df['StateId'].unique()\n",
    "    temp = cluster_hourly_df.drop_duplicates(subset=['AvgLat','AvgLon'])\n",
    "    AvgLat_list = temp['AvgLat'].values\n",
    "    AvgLon_list = temp['AvgLon'].values\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        final_transition_df['Address'] = 0\n",
    "        final_transition_df['AvgLat'] = 0\n",
    "        final_transition_df['AvgLon'] = 0\n",
    "        final_transition_df['StateId'] = 0\n",
    "        for j in range(0, cluster_hourly_df['StateId'].nunique()):\n",
    "            colname = '(' + str(i) + '-' + str(i + 1) + ')-' + str(cluster_list[j])\n",
    "            final_transition_df[colname] = 0\n",
    "\n",
    "    final_transition_df['StateId'] = cluster_list\n",
    "    final_transition_df['AvgLat'] = AvgLat_list\n",
    "    final_transition_df['AvgLon'] = AvgLon_list\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df.index = final_transition_df.StateId\n",
    "\n",
    "    # read each day file and sum the matching rows:cols combinations\n",
    "    date_list = cluster_hourly_df['Date'].unique()\n",
    "    path_dir = dest_path_each_day_trsn_mat\n",
    "\n",
    "    for p in range(0, cluster_hourly_df['Date'].nunique()):\n",
    "        filename = path_dir + str(date_list[p]) + '.csv'\n",
    "        temp_df = pd.read_csv(filename, header=0, sep='\\t')\n",
    "\n",
    "        for i in range(0, len(temp_df)):\n",
    "            rowname = temp_df['StateId'][i]\n",
    "            for src_column in temp_df:\n",
    "                for dest_column in final_transition_df:\n",
    "                    if src_column == dest_column and src_column != 'StateId':\n",
    "                        # import pdb; pdb.set_trace()\n",
    "                        final_transition_df[dest_column][rowname] = (final_transition_df[dest_column][rowname] +\n",
    "                                                                     temp_df[src_column][i])\n",
    "\n",
    "    # replace zero to a small value\n",
    "    final_transition_df = final_transition_df.fillna(0)\n",
    "    final_transition_df = final_transition_df.replace(0, 0.00001)\n",
    "\n",
    "    # calculate probability from cluster x to cluster y from time t to t+1\n",
    "    final_transition_df = final_transition_df.reset_index(drop=True)\n",
    "    for clus in range(0, len(final_transition_df)):\n",
    "        for i in range(0, 24):\n",
    "            temp_sum = 0\n",
    "            for j in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i + 1) + ')-' + str(final_transition_df['StateId'][j])\n",
    "                temp_sum += (final_transition_df[colname][clus])\n",
    "            for k in range(0, len(final_transition_df)):\n",
    "                colname = '(' + str(i) + '-' + str(i + 1) + ')-' + str(final_transition_df['StateId'][k])\n",
    "                if temp_sum != 0:\n",
    "                    final_transition_df[colname][clus] = final_transition_df[colname][clus] / temp_sum\n",
    "\n",
    "    # create dictionary for coordinate : address\n",
    "    points = tuple(zip(final_transition_df.AvgLat, final_transition_df.AvgLon))\n",
    "    geocoder = Nominatim(timeout=10)\n",
    "    coordinate_location = {}\n",
    "\n",
    "    for coordinate in points:\n",
    "        try:\n",
    "            location = geocoder.reverse(coordinate)\n",
    "        except:\n",
    "            location = 'unknown'\n",
    "        coordinate_location[coordinate] = location\n",
    "\n",
    "    co_loc = {k: v for k, v in coordinate_location.items()}\n",
    "\n",
    "    for i in range(0, len(final_transition_df)):\n",
    "        address = co_loc.get((final_transition_df['AvgLat'][i], final_transition_df['AvgLon'][i]))\n",
    "        if address == 'unknown':\n",
    "            final_transition_df['Address'][i] = 'unknown'\n",
    "        else:\n",
    "            final_transition_df['Address'][i] = address[0]\n",
    "\n",
    "    # save the file\n",
    "    final_transition_df.to_csv(dest_file_final_markov_chain)\n",
    "\n",
    "    for i in range(0, 24):\n",
    "        k = cluster_hourly_df['StateId'].nunique() * i + 4\n",
    "        final_transition_temp_df = final_transition_df.iloc[:, k:k + cluster_hourly_df['StateId'].nunique()]\n",
    "        final_transition_temp_df.index = cluster_list\n",
    "        file_name = usr_markov_chains_directory + \"/\" + str(i) + \" hour.csv\"\n",
    "        final_transition_temp_df.to_csv(file_name, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "# ------------------------------------------ MAIN ---------------------------------------------------------------------\n",
    "def main():\n",
    "    global usr_trejec_df\n",
    "    global trained_model_df\n",
    "    global curr_hr_df\n",
    "    global curr_hr_staypts_df\n",
    "    global staypts_df\n",
    "    global cluster_hourly_df\n",
    "    global final_transition_df\n",
    "\n",
    "    # read test user trajectory file. In real scenario, this will be the GPS read data\n",
    "    read_usr_file()\n",
    "    \n",
    "    if usr_trejec_df.empty:\n",
    "        return\n",
    "    \n",
    "    # prepare dataframes\n",
    "    prepare_dfs()\n",
    "\n",
    "    # Save first date and time as prev date and time for the start\n",
    "    prev_date = usr_trejec_df['Date'][0]\n",
    "    prev_hour = usr_trejec_df['Hour'][0]\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------------------------\n",
    "    #                                            ALGORITHM STEPS\n",
    "    #\n",
    "    # I. Read the new locations in an online gps location input mode\n",
    "    #  1. Every time the hour changes,\n",
    "    #                  A. Find staypoints for the last hour and assign staypointIDs\n",
    "    #                  B. Add time between staypoints based on speed of travel and distance between them\n",
    "    #                  C. Snap staypoints to the states and assign stateIDs\n",
    "    #  2. Every time the date changes,\n",
    "    #                  D. Calculate time slotted data for the entire day\n",
    "    #  3. If the hour and the time has not been changed, add the data to current hour data\n",
    "    # -----------------------------------------------------------------------------------------------------------------\n",
    "    # I\n",
    "    for i in range(0, len(usr_trejec_df)):\n",
    "\n",
    "        # store the read hour and date as new hour and new date\n",
    "        new_hour = usr_trejec_df['Hour'][i]\n",
    "        new_date = usr_trejec_df['Date'][i]\n",
    "\n",
    "        # 1.\n",
    "        # if the hour has changed\n",
    "        if new_hour != prev_hour:\n",
    "            # process the last hour data if available\n",
    "            if not curr_hr_df.empty:\n",
    "                # A.\n",
    "                create_last_hr_staypts()\n",
    "                # B.\n",
    "                add_start_end_times()\n",
    "                if not curr_hr_staypts_df.empty:\n",
    "                    # C.\n",
    "                    form_states()\n",
    "\n",
    "            prev_hour = new_hour\n",
    "            curr_hr_df = curr_hr_df.iloc[0:0]\n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])\n",
    "            # 2.\n",
    "            # if the date has changed\n",
    "            if new_date != prev_date:\n",
    "                if not staypts_df.empty:\n",
    "                    # D.\n",
    "                    cal_hourly_state_weight()\n",
    "                    update_staypts_csv()\n",
    "                prev_date = new_date\n",
    "\n",
    "        # 3.\n",
    "        # if the date and the hour has not changed, just add it to current hour dataframe.\n",
    "        # this dataframe is used once the hour is changed.\n",
    "        else:\n",
    "            curr_hr_df = curr_hr_df.append(usr_trejec_df.iloc[[i]])\n",
    "\n",
    "    combine_states()\n",
    "    change_ids()\n",
    "    update_hourly_weights_csv()\n",
    "    create_save_seperate_trasition_matrices()\n",
    "    create_save_markov_chains()\n",
    "    if not cluster_hourly_df.empty:\n",
    "        visualize_hourly_state_weight()\n",
    "\n",
    "def get_user_month():\n",
    "    global usern_mnth_df\n",
    "\n",
    "    # reusing old folder to fetch users and their corresponding month year with highest data\n",
    "    usern_mnth_df['user'] = \"\"\n",
    "    usern_mnth_df['year'] = \"\"\n",
    "    usern_mnth_df['month'] = \"\"\n",
    "\n",
    "    file_list = glob.glob(\n",
    "        \"C:/Users/12sha/Documents/thesislocation/Data/User stay points(month-maxdata, 50m 30min)/*.png\")\n",
    "    for i in range(0, len(file_list)):\n",
    "        file_name = file_list[i]\n",
    "        user = file_name[88:91]\n",
    "        year = file_name[102:106]\n",
    "        month = file_name[106:108]\n",
    "        month = int(month)\n",
    "        usern_mnth_df.loc[i, 'user'] = user\n",
    "        usern_mnth_df.loc[i, 'year'] = year\n",
    "        usern_mnth_df.loc[i, 'month'] = \"{0:0=2d}\".format(month)\n",
    "    usern_mnth_df = usern_mnth_df.drop_duplicates()\n",
    "    usern_mnth_df = usern_mnth_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------- S T A R T ---------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "#                                 Change Thresholds, User(From and To) and Destination File Paths Here\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "state_d_thrhld = 200            # State distance threshold in meters\n",
    "staypts_d_thrhld = 200          # Staypoint distance threshold in meters\n",
    "staypts_t_thrhld = 20           # Staypoint time threshold in minutes\n",
    "track_t_thrhld = 30             # tracking time threshold in minutes\n",
    "# From user number and to user number\n",
    "from_user = 0                   \n",
    "to_user = 182\n",
    "# destination paths\n",
    "base_path = \"C:/Users/12sha/Documents/thesislocation/Data/Final Example Results\"\n",
    "geolife_data_source_path = \"C:/Users/12sha/Documents/Geolife Trajectories 1.3/Data/\"\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "#                                         Select User and Corresponding Month\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "usern_mnth_df = pd.DataFrame()\n",
    "get_user_month() # Populates dataframe usern_mnth_df\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "#                                          Start the Process for Each User\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "for i in range(from_user, to_user):\n",
    "    user = usern_mnth_df.loc[i, 'user']\n",
    "    month = usern_mnth_df.loc[i, 'year'] + usern_mnth_df.loc[i, 'month'] + \"01\"\n",
    "    \n",
    "    #prepare date for next month\n",
    "    date_s = usern_mnth_df.loc[i, 'year'] + usern_mnth_df.loc[i, 'month'] + \"01\"\n",
    "    date_d = datetime(year=int(month[0:4]), month=int(month[4:6]), day=int(date_s[6:8]))\n",
    "    next_mn = date_d + relativedelta(months=1)\n",
    "    next_mn = next_mn.strftime('%Y%m%d')\n",
    "    \n",
    "    month = month[:-2]\n",
    "    next_mn = next_mn[:-2]\n",
    "    \n",
    "    for j in range(0, 2):\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        #                                                Directory Setup\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        # source paths\n",
    "        file_source_raw = geolife_data_source_path + user + \"/Trajectory/\" + month + \"*.plt\"\n",
    "\n",
    "        #desination paths\n",
    "        usr_directory = base_path + \"/User \" + user + \"/\" + month\n",
    "        usr_hrly_wght_directory = base_path + \"/User \" + user + \"/\" + month + \"/hourlyweights\"\n",
    "        usr_sty_pts_directory = base_path + \"/User \" + user + \"/\" + month + \"/staypoints\"\n",
    "        usr_markov_chains_directory = base_path + \"/User \" + user + \"/\" + month + \"/markovchains\"\n",
    "        dest_predicted_dir = base_path + \"/User \" + user + \"/\" + month + \"/predict/\"\n",
    "\n",
    "        if not os.path.exists(usr_directory):\n",
    "            os.makedirs(usr_directory)\n",
    "        if not os.path.exists(usr_hrly_wght_directory):\n",
    "            os.makedirs(usr_hrly_wght_directory)\n",
    "        if not os.path.exists(usr_sty_pts_directory):\n",
    "            os.makedirs(usr_sty_pts_directory)\n",
    "        if not os.path.exists(usr_markov_chains_directory):\n",
    "            os.makedirs(usr_markov_chains_directory)\n",
    "        if not os.path.exists(dest_predicted_dir):\n",
    "            os.makedirs(dest_predicted_dir)\n",
    "\n",
    "        # destination file names\n",
    "        dest_file_staypoints = usr_sty_pts_directory + \"/staypoints.csv\"\n",
    "        dest_file_hourly_weights = usr_hrly_wght_directory + \"/hourlyweights.csv\"\n",
    "        dest_path_each_day_trsn_mat = usr_hrly_wght_directory + \"/\"\n",
    "        dest_file_final_markov_chain = usr_markov_chains_directory + \"/final.csv\"\n",
    "\n",
    "        # remove if the file already exists\n",
    "        try:\n",
    "            os.remove(dest_file_staypoints)\n",
    "        except OSError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(dest_file_hourly_weights)\n",
    "        except OSError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(dest_file_final_markov_chain)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        #                                           Global Dataframes, Arrays and Dictionaries\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        # user raw trajectory dataframe\n",
    "        usr_trejec_df = pd.DataFrame()\n",
    "        # user trained model\n",
    "        trained_model_df = pd.DataFrame()\n",
    "        # current hour points\n",
    "        curr_hr_df = pd.DataFrame()\n",
    "        # current hour staypoints\n",
    "        curr_hr_staypts_df = pd.DataFrame()\n",
    "        # last hour last point\n",
    "        prev_hour_last_point = pd.DataFrame()\n",
    "        # all staypoints\n",
    "        staypts_df = pd.DataFrame()\n",
    "        # hourly cluster\n",
    "        cluster_hourly_df = pd.DataFrame()\n",
    "        # final markov chains\n",
    "        final_transition_df = pd.DataFrame()\n",
    "\n",
    "        clus_dict = {}\n",
    "        co_loc = {}\n",
    "        pred_loc = {}\n",
    "        lat_array = []\n",
    "        lon_array = []\n",
    "        global_count = 0\n",
    "\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        #                                                         M A I N\n",
    "        # ---------------------------------------------------------------------------------------------------------------------\n",
    "        main()\n",
    "        \n",
    "        # run for next month for same user\n",
    "        month = next_mn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
